{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>mfpbench</code>","text":"<p>Welcome to the documentation of the Multi-Fidelity Priors Benchmark library. Check out the quickstart to get started or check out our examples.</p> <p>At a glance, this library wraps several benchmarks that give access to the multi-fidelity, learning curve information of configurations from a static configuration space, as well as allow you to set the default configuration of some search space as the prior. This mainly includes validation score/error as well as cost.</p> <p>This includes any necessary setup commands which you can find in the setup.</p> Terminology? Deep Learning Example <p>In the deep learning setting, we often train configurations of a Neural Network for several epochs, recording the performance as it goes. However, finding the best configurations requires searching over some possible set Every configuration is going to take a different amount of time to train but often you'd like to find the best configuration as fast as possible.</p> <p>You start with your intutition and choose <code>learning_rate=0.001</code> and apply some <code>augmentation=True</code>, however when searching you also allow different values. You split your data into <code>\"train\", \"val\", \"test\"</code> sets, and set up your pipeline so your network trains on <code>\"train\"</code>, you score how good the hyperparameters are on <code>\"val\"</code> to choose the best but you also evaluate these configurations on <code>\"test\"</code>. You do this last part to get a sense of how much you are overfitting.</p> <ul> <li>fidelity - Here this would be the epochs</li> <li>configuration - One particular Neural Network architecture and it's hyperparameters.</li> <li>configuration space - The space of possible values for all the hyperparameters, or     in some cases, a finit set of distinct configurations.</li> <li>prior - The default configuration of <code>learning_rate=0.001</code> and     <code>augmentation=True</code>.</li> <li>validation score - How well your model does on the <code>\"val\"</code> set.</li> <li>cost - The amount of time it took to train a model</li> <li>learning curve - The ordered sequence of results for one configuration for     each fidelity value.</li> </ul> Example<pre><code>import mfpbench\n\nbenchmark = mfpbench.get(\"mfh3\", bias=2.5, noise=0)\n\nconfig = benchmark.sample()\nresult = benchmark.query(config, at=34)\n\ntrajectory = benchmark.trajectory(config, frm=5, to=benchmark.end)\n\nprint(config)\nprint(result)\nprint(len(trajectory))\n</code></pre> <pre><code>MFHartmann3Config(X_0=0.7343471694568163, X_1=0.9433106688641351, X_2=0.821513935537763)\nMFHartmannResult(fidelity=34, value=-0.6829846406575995, fid_cost=0.15982000000000002)\n96\n</code></pre>"},{"location":"#benchmarks","title":"Benchmarks","text":"<ul> <li> <p>pd1: A set of optimization results for larger deep-learning models from the     HyperBO paper. We use this raw tabular data and     build surrogate xgboost models to provide a smooth continuious space.</p> </li> <li> <p>yahpo-gym A collection of surrogate models trained from evaluations     of various models and search spaces over a variety of models and tasks.</p> </li> <li> <p>MFHartmann: A synthetic benchmark for multifidelity optimization     that allows you to control the noise and the crossing of learning curves. Useful for     testing the capabilities of optimizers.</p> </li> <li> <p>LCBenchTabular: A set of tabular benchmarks from     LCBench, which train various MLP's with     AutoPyTorch on different     OpenML tasks.</p> </li> <li> <p>Your own: There are also options to easily integrate your own benchmarks,     whether from raw tables     or with a more sophisticated objective function.</p> </li> </ul>"},{"location":"priors/","title":"Priors","text":"<p>TODO</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Make sure you first followed the setup guide.</p> <p>We will be using the synthetic MFHartmann for this tutorial as this requires no downloads to run.</p> <p>In general, the only import you should need for generic use is just <code>import mfpbench</code> and using <code>mfpbench.get(...)</code> to get a benchmark.</p> <p>There are also some nuances when working with tabular data that should be mentioned, see Tabular Benchmarks for more information.</p> <p>Quick Reference</p> <p>Useful Properties</p> <ul> <li><code>.space</code> - The space of the benchmark</li> <li><code>.start</code> - The starting fidelity of the benchmark</li> <li><code>.end</code> - The end fidelity of the benchmark</li> <li><code>.fidelity_name</code> - The name of the fidelity</li> <li><code>.table</code> - The table backing     a <code>TabularBenchmark</code>.</li> <li><code>Config</code> - The type of config used by the benchmark will     be attached to the benchmark object.</li> <li><code>Result</code> - The type of result used by the benchmark will     be attached to the benchmark object.</li> </ul> <p>Main Methods</p> <ul> <li><code>sample(n)</code> - Sample one or many configs from a benchmark</li> <li><code>query(config, at)</code> - Query a benchmark for a given fidelity</li> <li><code>trajectory(config)</code> - Get the full trajectory curve of a config</li> </ul> <p>Other</p> <ul> <li><code>load()</code> - Load a benchmark into memory if not already</li> </ul>"},{"location":"quickstart/#getting-a-benchmark","title":"Getting a benchmark","text":"<p>We try to make it so the normal use case of a benchmark is as simple as possible. For this we use <code>get()</code>. Each benchmarks comes with it's own <code>**kwargs</code> but you can find them in the API documentation of <code>get()</code>.</p> Get a benchmark<pre><code>import mfpbench\n\nbenchmark = mfpbench.get(\"mfh3\")\nprint(benchmark.name)\n</code></pre> <pre><code>mfh3\n</code></pre> API <p>Get a benchmark.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the benchmark</p> <p> TYPE: <code>str</code> </p> <code>prior</code> <p>The prior to use for the benchmark. * str -     If it ends in {.json} or {.yaml, .yml}, it will convert it to a path and     use it as if it is a path to a config. Otherwise, it is treated as preset * Path - path to a file * Config - A Config object * None - Use the default if available</p> <p> TYPE: <code>str | Path | Config | None</code> DEFAULT: <code>None</code> </p> <code>preload</code> <p>Whether to preload the benchmark data in</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Extra arguments, optional or required for other benchmarks. Please look up the associated benchmarks.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>For the <code>**kwargs</code>, please see the benchmarks listed below by <code>name=</code></p> <code>name='lcbench'</code> (YAHPO-GYM) <p>Possible <code>task_id=</code>:</p> <pre><code>('3945', '7593', '34539', '126025', '126026', '126029', '146212', '167104', '167149', '167152', '167161', '167168', '167181', '167184', '167185', '167190', '167200', '167201', '168329', '168330', '168331', '168335', '168868', '168908', '168910', '189354', '189862', '189865', '189866', '189873', '189905', '189906', '189908', '189909')\n</code></pre> PARAMETER  DESCRIPTION <code>task_id</code> <p>The task id to choose.</p> <p> TYPE: <code>str</code> </p> <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>datadir</code> <p>The path to where mfpbench stores it data. If left to <code>None</code>, will use the <code>_default_download_dir = ./data/yahpo-gym-data</code>.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed for the benchmark instance</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a str, will check the local location first for a prior specific for this benchmark, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a Mapping, will be used directly.</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If given, will perturb the prior by this amount. Only used if <code>prior=</code> is given as a config.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>session</code> <p>The onnxruntime session to use. If None, will create a new one.</p> <p>Not for faint hearted</p> <p>This is only a backdoor for onnx compatibility issues with YahpoGym. You are advised not to use this unless you know what you are doing.</p> <p> TYPE: <code>InferenceSession | None</code> DEFAULT: <code>None</code> </p> <code>name='lm1b_transformer_2048'</code> (PD1) PARAMETER  DESCRIPTION <code>datadir</code> <p>Path to the data directory</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the space</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>Any prior to use for the benchmark</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>Whether to perturb the prior. If specified, this is interpreted as the std of a normal from which to perturb numerical hyperparameters of the prior, and the raw probability of swapping a categorical value.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='uniref50_transformer_128'</code> (PD1) PARAMETER  DESCRIPTION <code>datadir</code> <p>Path to the data directory</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the space</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>Any prior to use for the benchmark</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>Whether to perturb the prior. If specified, this is interpreted as the std of a normal from which to perturb numerical hyperparameters of the prior, and the raw probability of swapping a categorical value.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='cifar100_wideresnet_2048'</code> (PD1) PARAMETER  DESCRIPTION <code>datadir</code> <p>Path to the data directory</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the space</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>Any prior to use for the benchmark</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>Whether to perturb the prior. If specified, this is interpreted as the std of a normal from which to perturb numerical hyperparameters of the prior, and the raw probability of swapping a categorical value.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='imagenet_resnet_512'</code> (PD1) PARAMETER  DESCRIPTION <code>datadir</code> <p>Path to the data directory</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the space</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>Any prior to use for the benchmark</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>Whether to perturb the prior. If specified, this is interpreted as the std of a normal from which to perturb numerical hyperparameters of the prior, and the raw probability of swapping a categorical value.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='jahs'</code> <p>Possible <code>task_id=</code>:</p> <pre><code>('CIFAR10', 'ColorectalHistology', 'FashionMNIST')\n</code></pre> PARAMETER  DESCRIPTION <code>task_id</code> <p>The specific task to use.</p> <p> TYPE: <code>Literal['CIFAR10', 'ColorectalHistology', 'FashionMNIST']</code> </p> <code>datadir</code> <p>The path to where mfpbench stores it data. If left to <code>None</code>, will use <code>_default_download_dir = \"./data/jahs-bench-data\"</code>.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to give this benchmark instance</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark.</p> <ul> <li>if <code>str</code> - A preset</li> <li>if <code>Path</code> - path to a file</li> <li>if <code>dict</code>, Config, Configuration - A config</li> <li>if <code>None</code> - Use the default if available</li> </ul> <p> TYPE: <code>str | Path | JAHSConfig | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If given, will perturb the prior by this amount. Only used if <code>prior=</code> is given as a config.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='mfh3'</code> PARAMETER  DESCRIPTION <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>bias</code> <p>How much bias to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>How much noise to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark.</p> <ul> <li>if <code>Path</code> - path to a file</li> <li>if <code>Mapping</code> - Use directly</li> <li>if <code>None</code> - There is no prior</li> </ul> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='mfh6'</code> PARAMETER  DESCRIPTION <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>bias</code> <p>How much bias to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>How much noise to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark.</p> <ul> <li>if <code>Path</code> - path to a file</li> <li>if <code>Mapping</code> - Use directly</li> <li>if <code>None</code> - There is no prior</li> </ul> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='lcbench_tabular'</code> <p>Possible <code>task_id=</code>:</p> <pre><code>('adult', 'airlines', 'albert', 'Amazon_employee_access', 'APSFailure', 'Australian', 'bank-marketing', 'blood-transfusion-service-center', 'car', 'christine', 'cnae-9', 'connect-4', 'covertype', 'credit-g', 'dionis', 'fabert', 'Fashion-MNIST', 'helena', 'higgs', 'jannis', 'jasmine', 'jungle_chess_2pcs_raw_endgame_complete', 'kc1', 'KDDCup09_appetency', 'kr-vs-kp', 'mfeat-factors', 'MiniBooNE', 'nomao', 'numerai28.6', 'phoneme', 'segment', 'shuttle', 'sylvine', 'vehicle', 'volkert')\n</code></pre> PARAMETER  DESCRIPTION <code>task_id</code> <p>The task to benchmark on.</p> <p> TYPE: <code>str</code> </p> <code>datadir</code> <p>The directory to look for the data in. If <code>None</code>, uses the default download directory.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>remove_constants</code> <p>Whether to remove constant config columns from the data or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a str, will check the local location first for a prior specific for this benchmark, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a Mapping, will be used directly.</p> <p> TYPE: <code>str | Path | LCBenchTabularConfig | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, this is interpreted as the standard deviation of a normal distribution while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/get.py</code> <pre><code>def get(\n    name: str,\n    *,\n    prior: str | Path | Config | None = None,\n    preload: bool = False,\n    **kwargs: Any,\n) -&gt; Benchmark:\n    \"\"\"Get a benchmark.\n\n    Args:\n        name: The name of the benchmark\n        prior: The prior to use for the benchmark.\n            * str -\n                If it ends in {.json} or {.yaml, .yml}, it will convert it to a path and\n                use it as if it is a path to a config. Otherwise, it is treated as preset\n            * Path - path to a file\n            * Config - A Config object\n            * None - Use the default if available\n        preload: Whether to preload the benchmark data in\n        **kwargs: Extra arguments, optional or required for other benchmarks. Please\n            look up the associated benchmarks.\n\n    For the `#!python **kwargs`, please see the benchmarks listed below by `name=`\n\n    ??? note \"`#!python name='lcbench'` (YAHPO-GYM)\"\n\n        Possible `#!python task_id=`:\n\n        ::: mfpbench.LCBenchBenchmark.yahpo_instances\n\n        ::: mfpbench.LCBenchBenchmark.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='lm1b_transformer_2048'` (PD1)\"\n\n        ::: mfpbench.PD1lm1b_transformer_2048.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='uniref50_transformer_128'` (PD1)\"\n\n        ::: mfpbench.PD1uniref50_transformer_128.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='cifar100_wideresnet_2048'` (PD1)\"\n\n        ::: mfpbench.PD1cifar100_wideresnet_2048.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='imagenet_resnet_512'` (PD1)\"\n\n        ::: mfpbench.PD1imagenet_resnet_512.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='jahs'`\"\n\n        Possible `#!python task_id=`:\n\n        ::: mfpbench.JAHSBenchmark.task_ids\n\n        ::: mfpbench.JAHSBenchmark.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='mfh3'`\"\n\n        ::: mfpbench.MFHartmann3Benchmark.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='mfh6'`\"\n\n        ::: mfpbench.MFHartmann6Benchmark.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='lcbench_tabular'`\"\n\n        Possible `#!python task_id=`:\n\n        ::: mfpbench.LCBenchTabularBenchmark.task_ids\n\n        ::: mfpbench.LCBenchTabularBenchmark.__init__\n            options:\n                show_source: false\n\n\n    \"\"\"  # noqa: E501\n    b = _mapping.get(name, None)\n    bench: Benchmark\n\n    if b is None:\n        raise ValueError(f\"{name} is not a benchmark in {list(_mapping.keys())}\")\n\n    if isinstance(prior, str) and any(\n        prior.endswith(suffix) for suffix in [\".json\", \".yaml\", \".yml\"]\n    ):\n        prior = Path(prior)\n\n    bench = b(prior=prior, **kwargs)\n\n    if preload:\n        bench.load()\n\n    return bench\n</code></pre> Preloading benchmarks <p>By default, benchmarks will not load in required data or surrogate models. To have these ready and in memory, you can pass in <code>preload=True</code>.</p>"},{"location":"quickstart/#properties-of-benchmarks","title":"Properties of Benchmarks","text":"<p>All benchmarks inherit from <code>Benchmark</code> which has some useful properties we might want to know about:</p> Benchmark Properties<pre><code>print(f\"Benchmark fidelity starts at: {benchmark.start}\")\nprint(f\"Benchmark fidelity ends at: {benchmark.end}\")\nprint(f\"Benchmark fidelity is called: {benchmark.fidelity_name}\")\nprint(f\"Benchmark has conditionals: {benchmark.has_conditionals}\")\nprint(\"Benchmark has the following space\")\nprint(benchmark.space)\n</code></pre> <pre><code>Benchmark fidelity starts at: 3\nBenchmark fidelity ends at: 100\nBenchmark fidelity is called: z\nBenchmark has conditionals: False\nBenchmark has the following space\nConfiguration space object:\n  Hyperparameters:\nmfh3\n    X_0, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n    X_1, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n    X_2, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n</code></pre>"},{"location":"quickstart/#sampling-from-a-benchmark","title":"Sampling from a benchmark","text":"<p>To sample from a benchmark, we use the <code>sample()</code> method. This method takes in a number of samples to return and returns a list of configs.</p> Sample from a benchmark<pre><code>config = benchmark.sample()\nprint(config)\n\nconfigs = benchmark.sample(10, seed=2)\n</code></pre> <pre><code>MFHartmann3Config(X_0=0.21397264541817052, X_1=0.03910426573268033, X_2=0.45116976402423326)\n</code></pre>"},{"location":"quickstart/#querying-a-benchmark","title":"Querying a benchmark","text":"<p>To query a benchmark, we use the <code>query()</code> method. This method takes in a config and a fidelity to query at and returns the <code>Result</code> of the benchmark at that fidelity. By default, this will return at the maximum fidelity but you can pass <code>at=</code> to query at a different fidelity.</p> Query a benchmark<pre><code>value = benchmark.query(config)\nprint(value)\n\nvalue = benchmark.query(config, at=benchmark.start)\nprint(value)\n</code></pre> <pre><code>MFHartmannResult(fidelity=100, value=-0.3311697366757019, fid_cost=1.0)\nMFHartmannResult(fidelity=3, value=-0.05551820886263131, fid_cost=0.050855000000000004)\n</code></pre> <p>When querying a benchmark, you can get the entire trajctory curve of a config with <code>trajectory()</code>. This will be a <code>list[Result]</code>, one for each fidelity available.</p> Get the trajectory of a config<pre><code>trajectory = benchmark.trajectory(config)\nprint(len(trajectory))\n\nerrors = [r.error for r in trajectory]\n\ntrajectory = benchmark.trajectory(config, frm=benchmark.start, to=benchmark.end // 2)\nprint(len(trajectory))\n</code></pre> <pre><code>98\n48\n</code></pre> <p>Tip</p> <p>The query and trajectory function can take in a <code>Config</code> object or anything that looks like a mapping.</p>"},{"location":"quickstart/#working-with-config-objects","title":"Working with <code>Config</code> objects","text":"<p>When interacting with a <code>Benchmark</code>, you will always be returned <code>Config</code> objects. These contain some simple methods to make working with them easier.</p> <p>They behave like a non-mutable dictionary  so you can use them like a non-mutable dictionary.</p> <pre><code>config = benchmark.sample()\nprint(\"index\", config[\"X_1\"])\n\nprint(\"get\", config.get(\"X_1231\", 42))\n\nfor key, value in config.items():\n    print(key, value)\n\nprint(\"contains\", \"X_1\" in config)\n\nprint(\"len\", len(config))\n\nprint(\"dict\", dict(config))\n</code></pre> <pre><code>index 0.32085906581482715\nget 42\nX_0 0.3932037853293264\nX_1 0.32085906581482715\nX_2 0.2698739842726221\ncontains True\nlen 3\ndict {'X_0': 0.3932037853293264, 'X_1': 0.32085906581482715, 'X_2': 0.2698739842726221}\n</code></pre> How is that done? <p>This is done by inheriting from python's <code>Mapping</code> class and implementing it's methods, namely <code>__getitem__()</code> <code>__iter__()</code>, <code>__len__()</code>. You can also implement things to look like lists, containers and other pythonic things!</p> <code>dict()</code>/<code>from_dict()</code><code>copy()</code><code>mutate()</code><code>perturb()</code><code>save()</code>/<code>from_file()</code> <p><code>Config.dict()</code> returns a dictionary of the config. This is useful for working with the config in other libraries.</p> <p><pre><code>config = benchmark.sample()\nprint(config)\n\nconfig_dict = config.dict()\nprint(config_dict)\n\nnew_config = benchmark.Config.from_dict(config_dict)\nprint(new_config)\n</code></pre> <pre><code>MFHartmann3Config(X_0=0.5813966650388789, X_1=0.4193927552821569, X_2=0.9450414913392458)\n{'X_0': 0.5813966650388789, 'X_1': 0.4193927552821569, 'X_2': 0.9450414913392458}\nMFHartmann3Config(X_0=0.5813966650388789, X_1=0.4193927552821569, X_2=0.9450414913392458)\n</code></pre> </p> <p><code>Config.copy()</code> returns a new config with the same values.</p> <p><pre><code>config = benchmark.sample()\nprint(config)\n\nnew_config = config.copy()\nprint(new_config)\nprint(new_config == config)\n</code></pre> <pre><code>MFHartmann3Config(X_0=0.10318422593399368, X_1=0.9347920100442383, X_2=0.6802034397555954)\nMFHartmann3Config(X_0=0.10318422593399368, X_1=0.9347920100442383, X_2=0.6802034397555954)\nTrue\n</code></pre> </p> <p><code>Config.mutate()</code> takes in a dictionary of keys to values and returns a new config with those values changed.</p> <p><pre><code>config = benchmark.sample()\nprint(config)\n\nnew_config = config.mutate(X_1=0.5)\nprint(new_config)\n</code></pre> <pre><code>MFHartmann3Config(X_0=0.10020405283458711, X_1=0.3847430385366982, X_2=0.3763848226810871)\nMFHartmann3Config(X_0=0.10020405283458711, X_1=0.5, X_2=0.3763848226810871)\n</code></pre> </p> <p><code>Config.perturb()</code> takes in the space the config is from, a standard deviation and/or a categorical swap change and returns a new config with the values perturbed by a normal distribution with the given standard deviation and/or the categorical swap change.</p> <p><pre><code>config = benchmark.sample()\nprint(config)\n\nperturbed_config = config.perturb(space=benchmark.space, std=0.2)\nprint(perturbed_config)\n</code></pre> <pre><code>MFHartmann3Config(X_0=0.1357023740863451, X_1=0.23077038986742926, X_2=0.8823950508970834)\nMFHartmann3Config(X_0=0.11168565072935235, X_1=0.3034037030248159, X_2=0.8836056310127075)\n</code></pre> </p> <p><code>Config.save()</code> and <code>Config.from_file()</code> are used to save and load configs to and from disk.</p> <p><pre><code>config = benchmark.sample()\nprint(config)\n\nconfig.save(\"example_config.yaml\")\nloaded_config = benchmark.Config.from_file(\"example_config.yaml\")\n\nconfig.save(\"example_config.json\")\nloaded_config = benchmark.Config.from_file(\"example_config.json\")\n\nprint(loaded_config == config)\n</code></pre> <pre><code>MFHartmann3Config(X_0=0.7075448956004533, X_1=0.3485179542630552, X_2=0.6901479678873498)\nTrue\n</code></pre> </p>"},{"location":"quickstart/#working-with-result-objects","title":"Working with <code>Result</code> objects","text":"<p>When interacting with a <code>Benchmark</code>, all results will be communicated back with <code>Result</code> objects. These contain some simple methods to make working with them easier. Every benchmark will have a different set of results available but in general we try to make at least an <code>error</code> and <code>score</code> available. We also make a <code>cost</code> available for benchmarks, which is often something like the time taken to train the specific config. These <code>error</code> and <code>score</code> attributes are usually validation errors and scores. Some benchmarks also provide a <code>test_error</code> and <code>test_score</code> which are the test errors and scores, but not all.</p> <pre><code>config = benchmark.sample()\nresult = benchmark.query(config)\n\nprint(\"error\", result.error)\nprint(\"cost\", result.cost)\n\nprint(result)\n</code></pre> <pre><code>error -0.3956291475964744\ncost 1.0\nMFHartmannResult(fidelity=100, value=-0.3956291475964744, fid_cost=1.0)\n</code></pre> <p>These share the <code>dict()</code> and <code>from_dict()</code> methods as <code>Config</code> objects but do not behave like dictionaries.</p> <p>The most notable property of <code>Result</code> objects is that also have the <code>fidelity</code> at which they were evaluated at and also the <code>config</code> that was evaluated to generate the results.</p>"},{"location":"quickstart/#tabular-benchmarks","title":"Tabular Benchmarks","text":"<p>Some benchmarks are tabular in nature, meaning they have a table of results that can be queried. These benchmarks inherit from <code>TabularBenchmark</code> and have a <code>table</code> property that is the ground source of truth for the benchmark. This table is a <code>pandas.DataFrame</code> and can be queried as such.</p> <p>In general, tabular benchmarks will have to construct themselves using the base <code>TabularBenchmark</code> This requires the follow arguments which can be used to normalize the table for efficient indexing and usage. Predefined tabular benchmarks will fill these in easily for you, e.g. <code>LCBenchTabularBenchmark</code>.</p> Required arguments for a <code>TabularBenchmark</code> <p>The main required arguments are <code>.config_name</code>, <code>.fidelity_name</code>, <code>.config_keys</code>, <code>.result_keys</code></p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of this benchmark.</p> <p> TYPE: <code>str</code> </p> <code>table</code> <p>The table to use for the benchmark.</p> <p> TYPE: <code>DataFrame</code> </p> <code>config_name</code> <p>The column in the table that contains the config id</p> <p> TYPE: <code>str</code> </p> <code>fidelity_name</code> <p>The column in the table that contains the fidelity</p> <p> TYPE: <code>str</code> </p> <code>result_keys</code> <p>The columns in the table that contain the results</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>config_keys</code> <p>The columns in the table that contain the config values</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>remove_constants</code> <p>Remove constant config columns from the data or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>space</code> <p>The configuration space to use for the benchmark. If None, will just be an empty space.</p> <p> TYPE: <code>ConfigurationSpace | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a string, will be treated as a prior specific for this benchmark if it can be found, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a dict or Configuration, will be used directly.</p> <p> TYPE: <code>str | Path | CTabular | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the benchmark.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/tabular.py</code> <pre><code>def __init__(  # noqa: PLR0913, C901\n    self,\n    name: str,\n    table: pd.DataFrame,\n    *,\n    config_name: str,\n    fidelity_name: str,\n    result_keys: Sequence[str],\n    config_keys: Sequence[str],\n    remove_constants: bool = False,\n    space: ConfigurationSpace | None = None,\n    seed: int | None = None,\n    prior: str | Path | CTabular | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n):\n    \"\"\"Initialize the benchmark.\n\n    Args:\n        name: The name of this benchmark.\n        table: The table to use for the benchmark.\n        config_name: The column in the table that contains the config id\n        fidelity_name: The column in the table that contains the fidelity\n        result_keys: The columns in the table that contain the results\n        config_keys: The columns in the table that contain the config values\n        remove_constants: Remove constant config columns from the data or not.\n        space: The configuration space to use for the benchmark. If None, will\n            just be an empty space.\n        prior: The prior to use for the benchmark. If None, no prior is used.\n            If a string, will be treated as a prior specific for this benchmark\n            if it can be found, otherwise assumes it to be a Path.\n            If a Path, will load the prior from the path.\n            If a dict or Configuration, will be used directly.\n        perturb_prior: If not None, will perturb the prior by this amount.\n            For numericals, while for categoricals, this is interpreted as the\n            probability of swapping the value for a random one.\n        seed: The seed to use for the benchmark.\n    \"\"\"\n    cls = self.__class__\n    if remove_constants:\n\n        def is_constant(_s: pd.Series) -&gt; bool:\n            _arr = _s.to_numpy()\n            return bool((_arr == _arr[0]).all())\n\n        constant_cols = [\n            col for col in table.columns if is_constant(table[col])  # type: ignore\n        ]\n        table = table.drop(columns=constant_cols)  # type: ignore\n        config_keys = [k for k in config_keys if k not in constant_cols]\n\n    # If the table isn't indexed, index it\n    index_cols = [config_name, fidelity_name]\n    if table.index.names != index_cols:\n        # Only drop the index if it's not relevant.\n        relevant_cols: list[str] = [  # type: ignore\n            *list(index_cols),  # type: ignore\n            *list(result_keys),\n            *list(config_keys),\n        ]\n        relevant = any(name in relevant_cols for name in table.index.names)\n        table = table.reset_index(drop=not relevant)\n\n        if config_name not in table.columns:\n            raise ValueError(f\"{config_name=} not in columns {table.columns}\")\n        if fidelity_name not in table.columns:\n            raise ValueError(f\"{fidelity_name=} not in columns {table.columns}\")\n\n        table = table.set_index(index_cols)\n        table = table.sort_index()\n\n    # Make sure all keys are in the table\n    for key in chain(result_keys, config_keys):\n        if key not in table.columns:\n            raise ValueError(f\"{key=} not in columns {table.columns}\")\n\n    # Make sure the keyword \"id\" is not in the columns as we use it to\n    # identify configs\n    if \"id\" in table.columns:\n        raise ValueError(f\"{table.columns=} contains 'id'. Please rename it\")\n\n    # Make sure we have equidistance fidelities for all configs\n    fidelity_values = table.index.get_level_values(fidelity_name)\n    fidelity_counts = fidelity_values.value_counts()\n    if not (fidelity_counts == fidelity_counts.iloc[0]).all():\n        raise ValueError(f\"{fidelity_name=} not  uniform. \\n{fidelity_counts}\")\n\n    # We now have the following table\n    #\n    # config_id fidelity | **metric, **config_values\n    #     0         0    |\n    #               1    |\n    #               2    |\n    #     1         0    |\n    #               1    |\n    #               2    |\n    #   ...\n\n    # Here we get all the unique configs\n    # config_id fidelity | **metric, **config_values\n    #     0         0    |\n    #     1         0    |\n    #   ...\n    config_id_table = table.groupby(level=config_name).agg(\"first\")\n    configs = {\n        str(config_id): cls.Config.from_dict(\n            {\n                **row[config_keys].to_dict(),  # type: ignore\n                \"id\": str(config_id),\n            },\n        )\n        for config_id, row in config_id_table.iterrows()\n    }\n\n    fidelity_values = table.index.get_level_values(fidelity_name).unique()\n\n    # We just assume equidistant fidelities\n    sorted_fids = sorted(fidelity_values)\n    start = sorted_fids[0]\n    end = sorted_fids[-1]\n    step = sorted_fids[1] - sorted_fids[0]\n\n    # Create the configuration space\n    if space is None:\n        space = ConfigurationSpace(name, seed=seed)\n\n    self.table = table\n    self.configs = configs\n    self.fidelity_name = fidelity_name\n    self.config_name = config_name\n    self.config_keys = sorted(config_keys)\n    self.result_keys = sorted(result_keys)\n    self.fidelity_range = (start, end, step)  # type: ignore\n\n    super().__init__(\n        name=name,\n        seed=seed,\n        space=space,\n        prior=prior,\n        perturb_prior=perturb_prior,\n    )\n</code></pre>"},{"location":"quickstart/#difference-for-config","title":"Difference for <code>Config</code>","text":"<p>When working with tabular benchmarks, the config type that is used is a <code>TabularConfig</code>. The one difference is that it includes an <code>.id</code> property that is used to identify the config in the table. This is what's used to retrieve results from the table. If this is missing when doing a <code>query()</code>, we'll do our best to match the config to the table and get the correct id, but this is not guaranteed.</p> <p>When using <code>dict()</code>, this <code>id</code> is not included in the dictionary. In general you should either store the <code>config</code> object itself or at least <code>config.id</code>, that you can include back in before calling <code>query()</code>.</p>"},{"location":"quickstart/#using-your-own-tabular-data","title":"Using your own Tabular Data","text":"<p>To facilitate each of use for you own usage of tabular data, we provide a <code>GenericTabularBenchmark</code> that can be used to load in and use your own tabular data.</p> <pre><code>import pandas as pd\nfrom mfpbench import GenericTabularBenchmark\n\n# Create some fake data\ndf = pd.DataFrame(\n    {\n        \"config\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\"],\n        \"fidelity\": [1, 2, 3, 1, 2, 3, 1, 2, 3],\n        \"balanced_accuracy\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n        \"color\": [\"red\", \"red\", \"red\", \"blue\", \"blue\", \"blue\", \"green\", \"green\", \"green\"],\n        \"shape\": [\"circle\", \"circle\", \"circle\", \"square\", \"square\", \"square\", \"triangle\", \"triangle\", \"triangle\"],\n        \"kind\": [\"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\"],\n    }\n)\nprint(df)\nprint()\nprint()\n\nbenchmark = GenericTabularBenchmark(\n    df,\n    name=\"mydata\",\n    config_name=\"config\",\n    fidelity_name=\"fidelity\",\n    config_keys=[\"color\", \"shape\"],\n    result_keys=[\"balanced_accuracy\"],\n    result_mapping={\n        \"error\": lambda df: 1 - df[\"balanced_accuracy\"],\n        \"score\": lambda df: df[\"balanced_accuracy\"],\n    },\n    remove_constants=True,\n)\n\nprint(benchmark.table)\n</code></pre> <pre><code>  config  fidelity  balanced_accuracy  color     shape kind\n0      a         1                0.1    red    circle  mlp\n1      a         2                0.2    red    circle  mlp\n2      a         3                0.3    red    circle  mlp\n3      b         1                0.4   blue    square  mlp\n4      b         2                0.5   blue    square  mlp\n5      b         3                0.6   blue    square  mlp\n6      c         1                0.7  green  triangle  mlp\n7      c         2                0.8  green  triangle  mlp\n8      c         3                0.9  green  triangle  mlp\n\n\n                 balanced_accuracy  color     shape  error  score\nconfig fidelity                                                  \na      1                       0.1    red    circle    0.9    0.1\n       2                       0.2    red    circle    0.8    0.2\n       3                       0.3    red    circle    0.7    0.3\nb      1                       0.4   blue    square    0.6    0.4\n       2                       0.5   blue    square    0.5    0.5\n       3                       0.6   blue    square    0.4    0.6\nc      1                       0.7  green  triangle    0.3    0.7\n       2                       0.8  green  triangle    0.2    0.8\n       3                       0.9  green  triangle    0.1    0.9\n</code></pre> <p>You can then operate on this benchmark as expected.</p> <pre><code>config = benchmark.sample()\nprint(config)\n\nresult = benchmark.query(config, at=2)\n\nprint(result)\n</code></pre> <pre><code>GenericTabularConfig(id='c', _values={'color': 'green', 'shape': 'triangle'})\nGenericTabularResult(fidelity=2, _values={'balanced_accuracy': 0.8, 'error': 0.19999999999999996, 'score': 0.8})\n</code></pre> API for <code>GenericTabularBenchmark</code> PARAMETER  DESCRIPTION <code>table</code> <p>The table to use for the benchmark</p> <p> TYPE: <code>DataFrame</code> </p> <code>name</code> <p>The name of the benchmark. If None, will be set to <code>unknown-{datetime.now().isoformat()}</code></p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>fidelity_name</code> <p>The column in the table that contains the fidelity</p> <p> TYPE: <code>str</code> </p> <code>config_name</code> <p>The column in the table that contains the config id</p> <p> TYPE: <code>str</code> </p> <code>result_keys</code> <p>The columns in the table that contain the results</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>config_keys</code> <p>The columns in the table that contain the config values</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>result_mapping</code> <p>A mapping from the result keys to the table keys. If a string, will be used as the key in the table. If a callable, will be called with the table and the result will be used as the value.</p> <p> TYPE: <code>dict[str, str | Callable[[DataFrame], Any]] | None</code> DEFAULT: <code>None</code> </p> <code>remove_constants</code> <p>Remove constant config columns from the data or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>space</code> <p>The configuration space to use for the benchmark. If None, will just be an empty space.</p> <p> TYPE: <code>ConfigurationSpace | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a str, will check the local location first for a prior specific for this benchmark, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a Mapping, will be used directly.</p> <p> TYPE: <code>str | Path | GenericTabularConfig | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, this is interpreted as the standard deviation of a normal distribution while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/tabular.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    table: pd.DataFrame,\n    *,\n    name: str | None = None,\n    fidelity_name: str,\n    config_name: str,\n    result_keys: Sequence[str],\n    config_keys: Sequence[str],\n    result_mapping: (dict[str, str | Callable[[pd.DataFrame], Any]] | None) = None,\n    remove_constants: bool = False,\n    space: ConfigurationSpace | None = None,\n    seed: int | None = None,\n    prior: str | Path | GenericTabularConfig | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n):\n    \"\"\"Initialize the benchmark.\n\n    Args:\n        table: The table to use for the benchmark\n        name: The name of the benchmark. If None, will be set to\n            `unknown-{datetime.now().isoformat()}`\n\n        fidelity_name: The column in the table that contains the fidelity\n        config_name: The column in the table that contains the config id\n        result_keys: The columns in the table that contain the results\n        config_keys: The columns in the table that contain the config values\n        result_mapping: A mapping from the result keys to the table keys.\n            If a string, will be used as the key in the table. If a callable,\n            will be called with the table and the result will be used as the value.\n        remove_constants: Remove constant config columns from the data or not.\n        space: The configuration space to use for the benchmark. If None, will\n            just be an empty space.\n        seed: The seed to use.\n        prior: The prior to use for the benchmark. If None, no prior is used.\n            If a str, will check the local location first for a prior\n            specific for this benchmark, otherwise assumes it to be a Path.\n            If a Path, will load the prior from the path.\n            If a Mapping, will be used directly.\n        perturb_prior: If not None, will perturb the prior by this amount.\n            For numericals, this is interpreted as the standard deviation of a\n            normal distribution while for categoricals, this is interpreted\n            as the probability of swapping the value for a random one.\n    \"\"\"\n    if name is None:\n        name = f\"unknown-{datetime.now().isoformat()}\"\n\n    _result_mapping: dict = result_mapping if result_mapping is not None else {}\n\n    # Remap the result keys so it works with the generic result types\n    if _result_mapping is not None:\n        for k, v in _result_mapping.items():\n            if isinstance(v, str):\n                if v not in table.columns:\n                    raise ValueError(f\"{v} not in columns\\n{table.columns}\")\n\n                table[k] = table[v]\n            elif callable(v):\n                table[k] = v(table)\n            else:\n                raise ValueError(f\"Unknown result mapping {v} for {k}\")\n\n    super().__init__(\n        name=name,\n        table=table,\n        config_name=config_name,\n        fidelity_name=fidelity_name,\n        result_keys=[*result_keys, *_result_mapping.keys()],\n        config_keys=config_keys,\n        remove_constants=remove_constants,\n        space=space,\n        seed=seed,\n        prior=prior,\n        perturb_prior=perturb_prior,\n    )\n</code></pre>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#installation","title":"Installation","text":""},{"location":"setup/#using-pip","title":"Using <code>pip</code>","text":"<pre><code>pip install mf-prior-bench\n</code></pre> <p>To install specific benchmark dependancies, you can specify any or all of the following:</p> <pre><code>pip install \"mf-prior-bench[yahpo, jahs-bench, pd1]\"\n</code></pre>"},{"location":"setup/#from-source","title":"From source","text":"<pre><code>git clone git@github.com:automl/mf-prior-bench\n\npip install \"mf-prior-bench\"\n</code></pre> <p>To install specific benchmark dependancies, you can specify any or all of the following:</p> <pre><code>git clone git@github.com:automl/mf-prior-bench\n\npip install \"mf-prior-bench[yahpo, jahs-bench, pd1]\"\n</code></pre>"},{"location":"setup/#cli","title":"CLI","text":"<p><code>mfpbench</code> provides some helpful commands for downloading data as well as generating priors for benchmarks.</p> CLI Help<pre><code>python -m mfpbench --help\n</code></pre> <pre><code>usage: python -m mfpbench [-h] {install,download,priors} ...\n\npositional arguments:\n  {install,download,priors}\n    install             python -m mfpbench install --help\n\n                        Install requirements for use with mfpbench.\n                        -------------------------------------------\n    download            python -m mfpbench download --help\n\n                        Download datasets for use with mfpbench.\n                        ----------------------------------------\n    priors              python -m mfpbench priors --help\n\n                        Generate priors for use with mfpbench.\n                        --------------------------------------\n\noptions:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"setup/#download","title":"Download","text":"<p>Some benchmark require raw files to work, you can download these using the <code>python -m mfpbench download</code> command.</p> <p>You can specify the location of the root data directory with <code>--data-dir</code>. We recommend leaving this to the default to make working with benchmarks easier.</p> <code>--benchmark</code><code>--list</code><code>--status</code><code>--force</code><code>--help</code> <pre><code># Disabled in docs\npython -m mfpbench download --benchmark \"pd1\"\n</code></pre> <p>Download Status<pre><code>python -m mfpbench download --list\n</code></pre> <pre><code>yahpo\njahs\npd1\nlcbench-tabular\n</code></pre> </p> <p>Setup Status<pre><code>python -m mfpbench download --status\n</code></pre> <pre><code>root: /home/runner/work/mf-prior-bench/mf-prior-bench/data\n----------------------------------------------------------\n[x] yahpo                python -m mfpbench download --benchmark yahpo\n[x] jahs                 python -m mfpbench download --benchmark jahs\n[x] pd1                  python -m mfpbench download --benchmark pd1\n[x] lcbench-tabular      python -m mfpbench download --benchmark lcbench-tabular\n</code></pre> </p> <pre><code># Execution disabled for docs\npython -m mfpbench download --benchmark \"pd1\" --force\n</code></pre> <p>Download Help<pre><code>python -m mfpbench download --help\n</code></pre> <pre><code>usage: python -m mfpbench download [-h] [--force] [--status] [--list]\n                                   [--benchmark {yahpo,jahs,pd1,lcbench-tabular}]\n                                   [--data-dir DATA_DIR]\n\noptions:\n  -h, --help            show this help message and exit\n  --force               Force download and remove existing data\n  --status              Print out the status of benchmarks\n  --list                Print out the available benchmarks data sources\n  --benchmark {yahpo,jahs,pd1,lcbench-tabular}\n                        The benchmark to download.\n  --data-dir DATA_DIR   Where to save the data, defaults to './data'\n</code></pre> </p>"},{"location":"setup/#install","title":"Install","text":"<p>As there will be conflicting dependancies between different benchmarks, we added some requirements files to specifically run these benchmarks. You will still have to setup an environment however you would do so but you can quickly install the required dependancies using the <code>python -m mfpbench install</code> subcommand.</p> <code>--benchmark</code><code>--list</code><code>--view</code><code>--requirements</code><code>-help</code> <pre><code># Disabled in docs\npython -m mfpbench install --benchmark \"lcbench-tabular\"\n</code></pre> <p>Install list<pre><code>python -m mfpbench install --list\n</code></pre> <pre><code>yahpo\njahs\npd1\nlcbench-tabular\n</code></pre> </p> <p>Install view<pre><code>python -m mfpbench install --view --benchmark \"yahpo\"\n</code></pre> <pre><code>=====\nyahpo\n=====\npath: /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages/requirements/yahpo.txt\nexec: /opt/hostedtoolcache/Python/3.11.6/x64/bin/python\ncmd: python -m pip install -r /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages/requirements/yahpo.txt\n------------------------------------------------------------------------------------------------------------------------\n# /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages/requirements/yahpo.txt\ntqdm\n</code></pre> </p> <pre><code># Disabled for docs\npython -m mfpbench install --requirements \"/path/to/myreqs\"\n</code></pre> <p>Install help<pre><code>python -m mfpbench install --help\n</code></pre> <pre><code>usage: python -m mfpbench install [-h] [--list] [--view]\n                                  [--benchmark {yahpo,jahs,pd1,lcbench-tabular}]\n                                  [--requirements REQUIREMENTS]\n\noptions:\n  -h, --help            show this help message and exit\n  --list                Print out the available benchmarks data sources\n  --view                View the default requirements for a benchmark\n  --benchmark {yahpo,jahs,pd1,lcbench-tabular}\n                        The benchmark to setup.\n  --requirements REQUIREMENTS\n                        The requirements into the currently active\n                        environment. If not specified, will use the default\n                        requirements. If string, the string will be used as\n                        the path to the requirements file.\n</code></pre> </p>"},{"location":"setup/#generating-priors","title":"Generating Priors","text":"<p>TODO</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>mfpbench<ul> <li>benchmark</li> <li>config</li> <li>correlations</li> <li>get</li> <li>jahs<ul> <li>benchmark</li> </ul> </li> <li>lcbench_tabular<ul> <li>benchmark</li> </ul> </li> <li>pd1<ul> <li>benchmark</li> <li>benchmarks<ul> <li>cifar100</li> <li>imagenet</li> <li>lm1b</li> <li>translate_wmt</li> <li>uniref50</li> </ul> </li> <li>processing<ul> <li>columns</li> <li>process_script</li> </ul> </li> <li>surrogate<ul> <li>train_xgboost</li> <li>training</li> <li>xgboost_space</li> </ul> </li> </ul> </li> <li>priors</li> <li>result</li> <li>resultframe</li> <li>setup_benchmark</li> <li>stats</li> <li>synthetic<ul> <li>hartmann<ul> <li>benchmark</li> <li>generators</li> </ul> </li> </ul> </li> <li>tabular</li> <li>util</li> <li>yahpo<ul> <li>benchmark</li> <li>benchmarks<ul> <li>iaml<ul> <li>iaml</li> <li>iaml_glmnet</li> <li>iaml_ranger</li> <li>iaml_rpart</li> <li>iaml_super</li> <li>iaml_xgboost</li> </ul> </li> <li>lcbench</li> <li>nb301</li> <li>rbv2<ul> <li>rbv2</li> <li>rbv2_aknn</li> <li>rbv2_glmnet</li> <li>rbv2_ranger</li> <li>rbv2_rpart</li> <li>rbv2_super</li> <li>rbv2_svm</li> <li>rbv2_xgboost</li> </ul> </li> </ul> </li> <li>config</li> <li>result</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/mfpbench/benchmark/","title":"benchmark","text":""},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark","title":"<code>class Benchmark(name, space, *, seed=None, prior=None, perturb_prior=None)</code>","text":"<p>         Bases: <code>Generic[C, R, F]</code>, <code>ABC</code></p> <p>Base class for a Benchmark.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of this benchmark</p> <p> TYPE: <code>str</code> </p> <code>space</code> <p>The configuration space to use for the benchmark.</p> <p> TYPE: <code>ConfigurationSpace</code> </p> <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a str, will check the local location first for a prior specific for this benchmark, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a Mapping, will be used directly.</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, this is interpreted as the standard deviation of a normal distribution while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/benchmark.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    space: ConfigurationSpace,\n    *,\n    seed: int | None = None,\n    prior: str | Path | C | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n):\n    \"\"\"Initialize the benchmark.\n\n    Args:\n        name: The name of this benchmark\n        space: The configuration space to use for the benchmark.\n        seed: The seed to use.\n        prior: The prior to use for the benchmark. If None, no prior is used.\n            If a str, will check the local location first for a prior\n            specific for this benchmark, otherwise assumes it to be a Path.\n            If a Path, will load the prior from the path.\n            If a Mapping, will be used directly.\n        perturb_prior: If not None, will perturb the prior by this amount.\n            For numericals, this is interpreted as the standard deviation of a\n            normal distribution while for categoricals, this is interpreted\n            as the probability of swapping the value for a random one.\n    \"\"\"\n    self.name = name\n    self.seed = seed\n    self.space = space\n    self.start: F = self.fidelity_range[0]\n    self.end: F = self.fidelity_range[1]\n    self.step: F = self.fidelity_range[2]\n\n    self._prior_arg = prior\n\n    # NOTE: This is handled entirely by subclasses as it requires knowledge\n    # of the overall space the prior comes from, which only the subclasses now\n    # at construction time. There's probably a better way to handle this but\n    # for now this is fine.\n    if perturb_prior is not None and not (0 &lt;= perturb_prior &lt;= 1):\n        raise NotImplementedError(\n            \"If perturbing prior, `perturb_prior` must be in [0, 1]\",\n        )\n\n    self.perturb_prior = perturb_prior\n    self.prior: C | None = None\n\n    if prior is not None:\n        self.prior = self._load_prior(prior, benchname=self.name)\n        self.prior.validate()\n    else:\n        self.prior = None\n\n    if self.prior is not None and self.perturb_prior is not None:\n        self.prior = self.prior.perturb(\n            space,\n            seed=self.seed,\n            std=self.perturb_prior,\n            categorical_swap_chance=self.perturb_prior,\n        )\n\n    if self.prior is not None:\n        self.prior.set_as_default_prior(space)\n</code></pre>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.fidelity_range","title":"<code>fidelity_range: tuple[F, F, F]</code>   <code>attr</code>","text":"<p>The fidelity range of this benchmark, (start, end, step)</p>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.fidelity_name","title":"<code>fidelity_name: str</code>   <code>attr</code>","text":"<p>The name of the fidelity used in this benchmark</p>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.Config","title":"<code>Config: type[C]</code>   <code>attr</code>","text":"<p>The config type of this benchmark</p>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.Result","title":"<code>Result: type[R]</code>   <code>attr</code>","text":"<p>The result type of this benchmark</p>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.has_conditionals","title":"<code>has_conditionals: bool</code>   <code>classvar</code> <code>attr</code>","text":"<p>Whether this benchmark has conditionals in it or not</p>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.space","title":"<code>space: ConfigurationSpace</code>   <code>attr</code>","text":"<p>The configuration space used in this benchmark</p>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.start","title":"<code>start: F</code>   <code>attr</code>","text":"<p>The start of the fidelity range</p>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.end","title":"<code>end: F</code>   <code>attr</code>","text":"<p>The end of the fidelity range</p>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.step","title":"<code>step: F</code>   <code>attr</code>","text":"<p>The step of the fidelity range</p>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.iter_fidelities","title":"<code>def iter_fidelities(frm=None, to=None, step=None)</code>","text":"<p>Iterate through the advertised fidelity space of the benchmark.</p> PARAMETER  DESCRIPTION <code>frm</code> <p>Start of the curve, defaults to the minimum fidelity</p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> <code>to</code> <p>End of the curve, defaults to the maximum fidelity</p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> <code>step</code> <p>Step size, defaults to benchmark standard (1 for epoch)</p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Iterator[F]</code> <p>An iterator over the fidelities</p> Source code in <code>src/mfpbench/benchmark.py</code> <pre><code>def iter_fidelities(\n    self,\n    frm: F | None = None,\n    to: F | None = None,\n    step: F | None = None,\n) -&gt; Iterator[F]:\n    \"\"\"Iterate through the advertised fidelity space of the benchmark.\n\n    Args:\n        frm: Start of the curve, defaults to the minimum fidelity\n        to: End of the curve, defaults to the maximum fidelity\n        step: Step size, defaults to benchmark standard (1 for epoch)\n\n    Returns:\n        An iterator over the fidelities\n    \"\"\"\n    frm = frm if frm is not None else self.start\n    to = to if to is not None else self.end\n    step = step if step is not None else self.step\n    assert self.start &lt;= frm &lt;= to &lt;= self.end\n\n    dtype = int if isinstance(frm, int) else float\n    fidelities: list[F] = list(\n        np.arange(start=frm, stop=(to + step), step=step, dtype=dtype),\n    )\n\n    # Note: Clamping floats on arange\n    #\n    #   There's an annoying detail about floats here, essentially we could over\n    #   (frm=0.03, to + step = 1+ .05, step=0.5) -&gt; [0.03, 0.08, ..., 1.03]\n    #   We just clamp this to the last fidelity\n    #\n    #   This does not effect ints\n    if isinstance(step, float) and fidelities[-1] &gt;= self.end:\n        fidelities[-1] = self.end\n\n    yield from fidelities\n</code></pre>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.load","title":"<code>def load()</code>","text":"<p>Explicitly load the benchmark before querying, optional.</p> Source code in <code>src/mfpbench/benchmark.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Explicitly load the benchmark before querying, optional.\"\"\"\n</code></pre>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.query","title":"<code>def query(config, at=None, *, argmax=None, argmin=None)</code>","text":"<p>Submit a query and get a result.</p> PARAMETER  DESCRIPTION <code>config</code> <p>The query to use</p> <p> TYPE: <code>C | Mapping[str, Any]</code> </p> <code>at</code> <p>The fidelity at which to query, defaults to None which means maximum</p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> <code>argmax</code> <p>Whether to return the argmax up to the point <code>at</code>. Will be slower as it has to get the entire trajectory. Uses the key from the Results.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>argmin</code> <p>Whether to return the argmin up to the point <code>at</code>. Will be slower as it has to get the entire trajectory. Uses the key from the Results.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>R</code> <p>The result of the query</p> Source code in <code>src/mfpbench/benchmark.py</code> <pre><code>def query(\n    self,\n    config: C | Mapping[str, Any],\n    at: F | None = None,\n    *,\n    argmax: str | None = None,\n    argmin: str | None = None,\n) -&gt; R:\n    \"\"\"Submit a query and get a result.\n\n    Args:\n        config: The query to use\n        at: The fidelity at which to query, defaults to None which means *maximum*\n        argmax: Whether to return the argmax up to the point `at`. Will be slower as\n            it has to get the entire trajectory. Uses the key from the Results.\n        argmin: Whether to return the argmin up to the point `at`. Will be slower as\n            it has to get the entire trajectory. Uses the key from the Results.\n\n    Returns:\n        The result of the query\n    \"\"\"\n    at = at if at is not None else self.end\n    assert self.start &lt;= at &lt;= self.end\n\n    if argmax is not None and argmin is not None:\n        raise ValueError(\"Can't have both argmax and argmin\")\n\n    if argmax is not None:\n        _argmax = argmax\n        return max(\n            self.trajectory(config, frm=self.start, to=at),\n            key=lambda r: getattr(r, _argmax),\n        )\n\n    if argmin is not None:\n        _argmin = argmin\n        return min(\n            self.trajectory(config, frm=self.start, to=at),\n            key=lambda r: getattr(r, _argmin),\n        )\n\n    if not isinstance(config, self.Config):\n        _config = self.Config.from_dict(config)\n    else:\n        _config = config\n\n    return self._objective_function(_config, at=at)\n</code></pre>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.trajectory","title":"<code>def trajectory(config, *, frm=None, to=None, step=None)</code>","text":"<p>Get the full trajectory of a configuration.</p> PARAMETER  DESCRIPTION <code>config</code> <p>The config to query</p> <p> TYPE: <code>C | Mapping[str, Any]</code> </p> <code>frm</code> <p>Start of the curve, should default to the start</p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> <code>to</code> <p>End of the curve, should default to the total</p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> <code>step</code> <p>Step size, defaults to <code>cls.default_step</code></p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[R]</code> <p>A list of the results for this config</p> Source code in <code>src/mfpbench/benchmark.py</code> <pre><code>def trajectory(\n    self,\n    config: C | Mapping[str, Any],\n    *,\n    frm: F | None = None,\n    to: F | None = None,\n    step: F | None = None,\n) -&gt; list[R]:\n    \"\"\"Get the full trajectory of a configuration.\n\n    Args:\n        config: The config to query\n        frm: Start of the curve, should default to the start\n        to: End of the curve, should default to the total\n        step: Step size, defaults to ``cls.default_step``\n\n    Returns:\n        A list of the results for this config\n    \"\"\"\n    to = to if to is not None else self.end\n    frm = frm if frm is not None else self.start\n    step = step if step is not None else self.step\n\n    if not isinstance(config, self.Config):\n        _config = self.Config.from_dict(config)\n    else:\n        _config = config\n\n    return self._trajectory(_config, frm=frm, to=to, step=step)\n</code></pre>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.sample","title":"<code>def sample(n=None, *, seed=None)</code>","text":"<p>Sample a random possible config.</p> PARAMETER  DESCRIPTION <code>n</code> <p>How many samples to take, None means jsut a single one, not in a list</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for sampling</p> <p>Seeding</p> <p>This is different than any seed passed to the construction of the benchmark.</p> <p> TYPE: <code>int | RandomState | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>C | list[C]</code> <p>Get back a possible Config to use</p> Source code in <code>src/mfpbench/benchmark.py</code> <pre><code>def sample(\n    self,\n    n: int | None = None,\n    *,\n    seed: int | np.random.RandomState | None = None,\n) -&gt; C | list[C]:\n    \"\"\"Sample a random possible config.\n\n    Args:\n        n: How many samples to take, None means jsut a single one, not in a list\n        seed: The seed to use for sampling\n\n            !!! note \"Seeding\"\n\n                This is different than any seed passed to the construction\n                of the benchmark.\n\n    Returns:\n        Get back a possible Config to use\n    \"\"\"\n    space = copy.deepcopy(self.space)\n    if isinstance(seed, np.random.RandomState):\n        rng = seed.randint(0, 2**32 - 1)\n    else:\n        rng = (\n            seed\n            if seed is not None\n            else np.random.default_rng().integers(0, 2**32 - 1)\n        )\n\n    space.seed(rng)\n    if n is None:\n        return self.Config.from_dict(space.sample_configuration())\n\n    # Just because of how configspace works\n    if n == 1:\n        return [self.Config.from_dict(space.sample_configuration())]\n\n    return [self.Config.from_dict(c) for c in space.sample_configuration(n)]\n</code></pre>"},{"location":"api/mfpbench/benchmark/#mfpbench.benchmark.Benchmark.frame","title":"<code>def frame()</code>","text":"<p>Get an empty frame to record with.</p> Source code in <code>src/mfpbench/benchmark.py</code> <pre><code>def frame(self) -&gt; ResultFrame[C, F, R]:\n    \"\"\"Get an empty frame to record with.\"\"\"\n    return ResultFrame[C, F, R]()\n</code></pre>"},{"location":"api/mfpbench/config/","title":"config","text":""},{"location":"api/mfpbench/config/#mfpbench.config.Config","title":"<code>class Config</code>   <code>dataclass</code>","text":"<p>         Bases: <code>ABC</code>, <code>Mapping[str, Any]</code></p> <p>A Config used to query a benchmark.</p> <ul> <li>Include all hyperparams</li> <li>Includes the fidelity</li> <li>Configuration and generation agnostic</li> <li>Immutable to prevent accidental changes during running, mutate with copy and     provide arguments as required.</li> <li>Easy equality between configs</li> </ul>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create from a dict or mapping object.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Self:\n    \"\"\"Create from a dict or mapping object.\"\"\"\n    field_names = {f.name for f in fields(cls)}\n    if not field_names.issuperset(d.keys()):\n        raise ValueError(f\"Dict keys {d.keys()} must be a subset of {field_names}\")\n\n    return cls(**{f.name: d[f.name] for f in fields(cls) if f.name in d})\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.from_row","title":"<code>def from_row(row)</code>   <code>classmethod</code>","text":"<p>Create from a row of a dataframe.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@classmethod\ndef from_row(cls, row: pd.Series) -&gt; Self:\n    \"\"\"Create from a row of a dataframe.\"\"\"\n    return cls.from_dict(row.to_dict())\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.dict","title":"<code>def dict()</code>","text":"<p>As a raw dictionary.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>def dict(self) -&gt; dict[str, Any]:\n    \"\"\"As a raw dictionary.\"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.mutate","title":"<code>def mutate(**kwargs)</code>","text":"<p>Copy a config and mutate it if needed.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Copy a config and mutate it if needed.\"\"\"\n    return replace(self, **kwargs)\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.copy","title":"<code>def copy()</code>","text":"<p>Copy this config and mutate it if needed.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this config and mutate it if needed.\"\"\"\n    return replace(self)\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.perturb","title":"<code>def perturb(space, *, seed=None, std=None, categorical_swap_chance=None)</code>","text":"<p>Perturb this config.</p> <p>Add gaussian noise to each hyperparameter. The mean is centered at the current config.</p> PARAMETER  DESCRIPTION <code>space</code> <p>The space to perturb in</p> <p> TYPE: <code>ConfigurationSpace</code> </p> <code>seed</code> <p>The seed to use for the perturbation</p> <p> TYPE: <code>int | RandomState | None</code> DEFAULT: <code>None</code> </p> <code>std</code> <p>A value in [0, 1] representing the fraction of the hyperparameter range to use as the std. If None, will use keep the current value</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>categorical_swap_chance</code> <p>The probability that a categorical hyperparameter will be changed If None, will use keep the current value</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The perturbed config</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>def perturb(\n    self,\n    space: ConfigurationSpace,\n    *,\n    seed: int | np.random.RandomState | None = None,\n    std: float | None = None,\n    categorical_swap_chance: float | None = None,\n) -&gt; Self:\n    \"\"\"Perturb this config.\n\n    Add gaussian noise to each hyperparameter. The mean is centered at\n    the current config.\n\n    Args:\n        space: The space to perturb in\n        seed: The seed to use for the perturbation\n        std: A value in [0, 1] representing the fraction of the hyperparameter range\n            to use as the std. If None, will use keep the current value\n        categorical_swap_chance:\n            The probability that a categorical hyperparameter will be changed\n            If None, will use keep the current value\n\n    Returns:\n        The perturbed config\n    \"\"\"\n    new_values: dict = {}\n    for name, value in self.items():\n        hp = space[name]\n        if isinstance(hp, CategoricalHyperparameter) and categorical_swap_chance:\n            new_value = perturb(value, hp, seed=seed, std=categorical_swap_chance)\n        elif not isinstance(hp, CategoricalHyperparameter) and std:\n            new_value = perturb(value, hp, seed=seed, std=std)\n        else:\n            new_value = value\n\n        new_values[name] = new_value\n\n    return self.mutate(**new_values)\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.validate","title":"<code>def validate()</code>   <code>abstractmethod</code>","text":"<p>Validate the config, just useful early on while testing.</p> RAISES DESCRIPTION <code>AssertionError</code> <p>If the config is not valid</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@abstractmethod\ndef validate(self) -&gt; None:\n    \"\"\"Validate the config, just useful early on while testing.\n\n    Raises:\n        AssertionError: If the config is not valid\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.__eq__","title":"<code>def __eq__(that)</code>","text":"<p>Equality is defined in terms of their dictionary repr.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>def __eq__(self, that: Any) -&gt; bool:\n    \"\"\"Equality is defined in terms of their dictionary repr.\"\"\"\n    this = self.dict()\n    if isinstance(that, dict):\n        that = that.copy()\n    elif isinstance(that, Configuration):\n        that = dict(that)\n    elif isinstance(that, self.__class__):\n        that = that.dict()\n    else:\n        return False\n\n    this = {\n        k: np.round(v, 10) if isinstance(v, float) else v for k, v in this.items()\n    }\n    _that = {\n        k: np.round(v, 10) if isinstance(v, float) else v for k, v in that.items()\n    }\n    return this == _that\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.set_as_default_prior","title":"<code>def set_as_default_prior(configspace)</code>","text":"<p>Apply this configuration as a prior on a configspace.</p> PARAMETER  DESCRIPTION <code>configspace</code> <p>The space to apply this config to</p> <p> TYPE: <code>ConfigurationSpace</code> </p> Source code in <code>src/mfpbench/config.py</code> <pre><code>def set_as_default_prior(self, configspace: ConfigurationSpace) -&gt; None:\n    \"\"\"Apply this configuration as a prior on a configspace.\n\n    Args:\n        configspace: The space to apply this config to\n    \"\"\"\n    # We convert to dict incase there's any special transformation that happen\n    d = self.dict()\n    for k, v in d.items():\n        hp = configspace[k]\n        # https://github.com/automl/ConfigSpace/issues/270\n        if isinstance(hp, Constant):\n            if hp.default_value != v:\n                raise ValueError(\n                    f\"Constant {k} must be set to the fixed value\"\n                    f\" {hp.default_value}, not {v}\",\n                )\n            # No need to do anything here\n        else:\n            hp.default_value = hp.check_default(v)\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.from_file","title":"<code>def from_file(path)</code>   <code>classmethod</code>","text":"<p>Load a config from a supported file type.</p>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.from_file--note","title":"Note:","text":"<p>Only supports yaml and json for now</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str | Path) -&gt; Self:\n    \"\"\"Load a config from a supported file type.\n\n    Note:\n    ----\n    Only supports yaml and json for now\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"File {path} does not exist\")\n\n    if path.suffix == \"json\":\n        return cls.from_json(path)\n    if path.suffix in [\"yaml\", \"yml\"]:\n        return cls.from_yaml(path)\n\n    # It has no file suffix, just try both\n    try:\n        return cls.from_yaml(path)\n    except yaml.error.YAMLError:\n        pass\n\n    try:\n        return cls.from_json(path)\n    except json.JSONDecodeError:\n        pass\n\n    raise ValueError(f\"Path {path} is not valid yaml or json\")\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.from_yaml","title":"<code>def from_yaml(path)</code>   <code>classmethod</code>","text":"<p>Load a config from a yaml file.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, path: str | Path) -&gt; Self:\n    \"\"\"Load a config from a yaml file.\"\"\"\n    path = Path(path)\n    with path.open(\"r\") as f:\n        d = yaml.safe_load(f)\n        return cls.from_dict(d)\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.from_json","title":"<code>def from_json(path)</code>   <code>classmethod</code>","text":"<p>Load a config from a json file.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@classmethod\ndef from_json(cls, path: str | Path) -&gt; Self:\n    \"\"\"Load a config from a json file.\"\"\"\n    path = Path(path)\n    with path.open(\"r\") as f:\n        d = json.load(f)\n        return cls.from_dict(d)\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.Config.save","title":"<code>def save(path, format=None)</code>","text":"<p>Save the config.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Where to save to. Will infer json or yaml based on filename</p> <p> TYPE: <code>str | Path</code> </p> <code>format</code> <p>The format to save as. Will use file suffix if not provided</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/config.py</code> <pre><code>def save(self, path: str | Path, format: str | None = None) -&gt; None:\n    \"\"\"Save the config.\n\n    Args:\n        path: Where to save to. Will infer json or yaml based on filename\n        format: The format to save as. Will use file suffix if not provided\n    \"\"\"\n    d = self.dict()\n    path = Path(path)\n    if format is None:\n        if path.suffix == \"json\":\n            format = \"json\"\n        elif path.suffix in [\"yaml\", \"yml\"]:\n            format = \"yaml\"\n        else:\n            format = \"yaml\"\n\n    if format == \"yaml\":\n        with path.open(\"w\") as f:\n            yaml.dump(d, f)\n    elif format == \"json\":\n        with path.open(\"w\") as f:\n            json.dump(d, f)\n    else:\n        raise ValueError(f\"unkown format `format={format}`\")\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.TabularConfig","title":"<code>class TabularConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Config</code></p>"},{"location":"api/mfpbench/config/#mfpbench.config.TabularConfig.id","title":"<code>id: str | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The id of this config.</p> <p>Warning</p> <p>While this is not required for a config, it is likely required to query into a tabular benchmark.</p> <p>Reasons for this not existing is when you have created this <code>from_dict</code> with a dict that does not have an id key.</p>"},{"location":"api/mfpbench/config/#mfpbench.config.TabularConfig.from_row","title":"<code>def from_row(row)</code>   <code>classmethod</code>","text":"<p>Create from a row of a dataframe.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@classmethod\ndef from_row(cls, row: pd.Series) -&gt; Self:\n    \"\"\"Create from a row of a dataframe.\"\"\"\n    return cls.from_dict({\"id\": row.name, **row.to_dict()})\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.TabularConfig.dict","title":"<code>def dict(*, with_id=False)</code>","text":"<p>As a raw dictionary.</p> PARAMETER  DESCRIPTION <code>with_id</code> <p>Whether to include the id key</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@override\ndef dict(self, *, with_id: bool = False) -&gt; Any:\n    \"\"\"As a raw dictionary.\n\n\n    Args:\n        with_id: Whether to include the id key\n    \"\"\"\n    d = {**super().dict()}\n    if not with_id:\n        d.pop(\"id\")\n    return d\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.TabularConfig.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create from a dict or mapping object.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@classmethod\n@override\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Self:\n    \"\"\"Create from a dict or mapping object.\"\"\"\n    d = dict(d)\n    d.setdefault(\"id\", None)\n    return cls(**d)\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.TabularConfig.names","title":"<code>def names()</code>   <code>classmethod</code>","text":"<p>The names of entries in this config.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@classmethod\ndef names(cls) -&gt; list[str]:\n    \"\"\"The names of entries in this config.\"\"\"\n    return [f.name for f in fields(cls) if f.name not in (\"id\",)]\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.TabularConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate the config, just useful early on while testing.</p> <p>Not implemented</p> <p>Does not do anything for Tabular Benchmarks</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate the config, just useful early on while testing.\n\n    !!! note \"Not implemented\"\n\n        Does not do anything for Tabular Benchmarks\n    \"\"\"\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.GenericTabularConfig","title":"<code>class GenericTabularConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>TabularConfig</code></p> <p>A generic tabular config.</p> <p>This is useful for adhoc tabular benchmarks and is what they will return, i.e. directly creating a benchmark from TabularBenchmark.</p>"},{"location":"api/mfpbench/config/#mfpbench.config.GenericTabularConfig.__hash__","title":"<code>def __hash__()</code>","text":"<p>Hash based on the dictionary repr.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Hash based on the dictionary repr.\"\"\"\n    return hash(self.id) ^ hash(tuple(self._values.items()))\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.GenericTabularConfig.dict","title":"<code>def dict(*, with_id=False)</code>","text":"<p>As a raw dictionary.</p> PARAMETER  DESCRIPTION <code>with_id</code> <p>Whether to include the id key</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@override\ndef dict(self, *, with_id: bool = False) -&gt; Any:\n    \"\"\"As a raw dictionary.\n\n    Args:\n        with_id: Whether to include the id key\n    \"\"\"\n    d = {**self._values}\n    if with_id:\n        d[\"id\"] = self.id\n    return d\n</code></pre>"},{"location":"api/mfpbench/config/#mfpbench.config.GenericTabularConfig.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create from a dict or mapping object.</p> Source code in <code>src/mfpbench/config.py</code> <pre><code>@classmethod\n@override\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Self:\n    \"\"\"Create from a dict or mapping object.\"\"\"\n    d = dict(d)\n    id = d.pop(\"id\")\n    return cls(id=id, _values=d)\n</code></pre>"},{"location":"api/mfpbench/correlations/","title":"correlations","text":""},{"location":"api/mfpbench/correlations/#mfpbench.correlations.RunningStats","title":"<code>class RunningStats()</code>","text":"Source code in <code>src/mfpbench/correlations.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    self.n = 0\n    self.old_m = np.array(0)\n    self.new_m = np.array(0)\n    self.old_s = np.array(0)\n    self.new_s = np.array(0)\n    self.previous_m = np.array(0)\n    self.previous_s = np.array(0)\n</code></pre>"},{"location":"api/mfpbench/correlations/#mfpbench.correlations.RunningStats.clear","title":"<code>def clear()</code>","text":"<p>Clear the running stats.</p> Source code in <code>src/mfpbench/correlations.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear the running stats.\"\"\"\n    self.n = 0\n</code></pre>"},{"location":"api/mfpbench/correlations/#mfpbench.correlations.RunningStats.push","title":"<code>def push(x)</code>","text":"<p>Push a new value into the running stats.</p> Source code in <code>src/mfpbench/correlations.py</code> <pre><code>def push(self, x: np.ndarray) -&gt; None:\n    \"\"\"Push a new value into the running stats.\"\"\"\n    self.n += 1\n    self.previous_m = self.old_m\n    self.previous_s = self.old_s\n\n    if self.n == 1:\n        self.old_m = self.new_m = x\n        self.old_s = np.array(0)\n    else:\n        self.new_m = self.old_m + (x - self.old_m) / self.n\n        self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)\n\n        self.old_m = self.new_m\n        self.old_s = self.new_s\n</code></pre>"},{"location":"api/mfpbench/correlations/#mfpbench.correlations.RunningStats.mean","title":"<code>def mean()</code>","text":"<p>Return the mean of the running stats.</p> Source code in <code>src/mfpbench/correlations.py</code> <pre><code>def mean(self) -&gt; np.ndarray:\n    \"\"\"Return the mean of the running stats.\"\"\"\n    return self.new_m if self.n &gt; 1 else 0.0  # type: ignore\n</code></pre>"},{"location":"api/mfpbench/correlations/#mfpbench.correlations.RunningStats.variance","title":"<code>def variance()</code>","text":"<p>Return the variance of the running stats.</p> Source code in <code>src/mfpbench/correlations.py</code> <pre><code>def variance(self) -&gt; np.ndarray:\n    \"\"\"Return the variance of the running stats.\"\"\"\n    return self.new_s / (self.n - 1) if self.n &gt; 1 else np.array(0.0)\n</code></pre>"},{"location":"api/mfpbench/correlations/#mfpbench.correlations.RunningStats.std","title":"<code>def std()</code>","text":"<p>Return the standard deviation of the running stats.</p> Source code in <code>src/mfpbench/correlations.py</code> <pre><code>def std(self) -&gt; np.ndarray:\n    \"\"\"Return the standard deviation of the running stats.\"\"\"\n    return np.asarray(np.sqrt(self.variance()))\n</code></pre>"},{"location":"api/mfpbench/correlations/#mfpbench.correlations.correlation_curve","title":"<code>def correlation_curve(b, *, n_samples=25, method='spearman')</code>","text":"<p>Compute the correlation curve for a benchmark.</p> PARAMETER  DESCRIPTION <code>b</code> <p>The benchmark to compute the correlation curve for</p> <p> TYPE: <code>Benchmark</code> </p> <code>n_samples</code> <p>The number of samples to take from the benchmark</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>method</code> <p>The method to use for computing the correlation curve</p> <p> TYPE: <code>Literal['spearman', 'kendalltau', 'cosine']</code> DEFAULT: <code>'spearman'</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The mean correlation curve</p> Source code in <code>src/mfpbench/correlations.py</code> <pre><code>def correlation_curve(\n    b: Benchmark,\n    *,\n    n_samples: int = 25,\n    method: Literal[\"spearman\", \"kendalltau\", \"cosine\"] = \"spearman\",\n) -&gt; np.ndarray:\n    \"\"\"Compute the correlation curve for a benchmark.\n\n    Args:\n        b: The benchmark to compute the correlation curve for\n        n_samples: The number of samples to take from the benchmark\n        method: The method to use for computing the correlation curve\n\n    Returns:\n        The mean correlation curve\n    \"\"\"\n    configs = b.sample(n_samples)\n    frame = b.frame()\n    for config in configs:\n        trajectory = b.trajectory(config)\n        for r in trajectory:\n            frame.add(r)\n\n    correlations = frame.correlations(method=method)\n    return correlations[-1, :]\n</code></pre>"},{"location":"api/mfpbench/correlations/#mfpbench.correlations.monte_carlo","title":"<code>def monte_carlo(benchmark, n_samples=25, epsilon=0.001, iterations_max=5000)</code>","text":"<p>Compute the correlation curve use a mc method for convergence.</p> PARAMETER  DESCRIPTION <code>benchmark</code> <p>The benchmark to compute the correlation curve for</p> <p> TYPE: <code>Benchmark</code> </p> <code>n_samples</code> <p>The number of samples to take from the benchmark per iteration</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>epsilon</code> <p>The convergence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>iterations_max</code> <p>The maximum number of iterations to run</p> <p> TYPE: <code>int</code> DEFAULT: <code>5000</code> </p> RETURNS DESCRIPTION <code>RunningStats</code> <p>RunningStats</p> Source code in <code>src/mfpbench/correlations.py</code> <pre><code>def monte_carlo(\n    benchmark: Benchmark,\n    n_samples: int = 25,\n    epsilon: float = 1e-3,\n    iterations_max: int = 5000,\n) -&gt; RunningStats:\n    \"\"\"Compute the correlation curve use a mc method for convergence.\n\n    Args:\n        benchmark: The benchmark to compute the correlation curve for\n        n_samples: The number of samples to take from the benchmark per iteration\n        epsilon: The convergence threshold\n        iterations_max: The maximum number of iterations to run\n\n    Returns:\n        RunningStats\n    \"\"\"\n    stats = RunningStats()\n    converged = False\n    itrs = 0\n    diff: float = np.inf\n    while not converged and itrs &lt; iterations_max:\n        curve = correlation_curve(benchmark, n_samples=n_samples)\n        stats.push(curve)\n\n        if stats.n &gt; 2:\n            diff = float(np.linalg.norm(stats.new_m - stats.previous_m, ord=2))\n            if diff &lt;= epsilon:\n                converged = True\n\n        else:\n            diff = np.inf\n        itrs += 1\n\n    return stats\n</code></pre>"},{"location":"api/mfpbench/get/","title":"get","text":""},{"location":"api/mfpbench/get/#mfpbench.get.get","title":"<code>def get(name, *, prior=None, preload=False, **kwargs)</code>","text":"<p>Get a benchmark.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the benchmark</p> <p> TYPE: <code>str</code> </p> <code>prior</code> <p>The prior to use for the benchmark. * str -     If it ends in {.json} or {.yaml, .yml}, it will convert it to a path and     use it as if it is a path to a config. Otherwise, it is treated as preset * Path - path to a file * Config - A Config object * None - Use the default if available</p> <p> TYPE: <code>str | Path | Config | None</code> DEFAULT: <code>None</code> </p> <code>preload</code> <p>Whether to preload the benchmark data in</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Extra arguments, optional or required for other benchmarks. Please look up the associated benchmarks.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>For the <code>**kwargs</code>, please see the benchmarks listed below by <code>name=</code></p> <code>name='lcbench'</code> (YAHPO-GYM) <p>Possible <code>task_id=</code>:</p> <pre><code>('3945', '7593', '34539', '126025', '126026', '126029', '146212', '167104', '167149', '167152', '167161', '167168', '167181', '167184', '167185', '167190', '167200', '167201', '168329', '168330', '168331', '168335', '168868', '168908', '168910', '189354', '189862', '189865', '189866', '189873', '189905', '189906', '189908', '189909')\n</code></pre> PARAMETER  DESCRIPTION <code>task_id</code> <p>The task id to choose.</p> <p> TYPE: <code>str</code> </p> <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>datadir</code> <p>The path to where mfpbench stores it data. If left to <code>None</code>, will use the <code>_default_download_dir = ./data/yahpo-gym-data</code>.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed for the benchmark instance</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a str, will check the local location first for a prior specific for this benchmark, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a Mapping, will be used directly.</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If given, will perturb the prior by this amount. Only used if <code>prior=</code> is given as a config.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>session</code> <p>The onnxruntime session to use. If None, will create a new one.</p> <p>Not for faint hearted</p> <p>This is only a backdoor for onnx compatibility issues with YahpoGym. You are advised not to use this unless you know what you are doing.</p> <p> TYPE: <code>InferenceSession | None</code> DEFAULT: <code>None</code> </p> <code>name='lm1b_transformer_2048'</code> (PD1) PARAMETER  DESCRIPTION <code>datadir</code> <p>Path to the data directory</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the space</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>Any prior to use for the benchmark</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>Whether to perturb the prior. If specified, this is interpreted as the std of a normal from which to perturb numerical hyperparameters of the prior, and the raw probability of swapping a categorical value.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='uniref50_transformer_128'</code> (PD1) PARAMETER  DESCRIPTION <code>datadir</code> <p>Path to the data directory</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the space</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>Any prior to use for the benchmark</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>Whether to perturb the prior. If specified, this is interpreted as the std of a normal from which to perturb numerical hyperparameters of the prior, and the raw probability of swapping a categorical value.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='cifar100_wideresnet_2048'</code> (PD1) PARAMETER  DESCRIPTION <code>datadir</code> <p>Path to the data directory</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the space</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>Any prior to use for the benchmark</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>Whether to perturb the prior. If specified, this is interpreted as the std of a normal from which to perturb numerical hyperparameters of the prior, and the raw probability of swapping a categorical value.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='imagenet_resnet_512'</code> (PD1) PARAMETER  DESCRIPTION <code>datadir</code> <p>Path to the data directory</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the space</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>Any prior to use for the benchmark</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>Whether to perturb the prior. If specified, this is interpreted as the std of a normal from which to perturb numerical hyperparameters of the prior, and the raw probability of swapping a categorical value.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='jahs'</code> <p>Possible <code>task_id=</code>:</p> <pre><code>('CIFAR10', 'ColorectalHistology', 'FashionMNIST')\n</code></pre> PARAMETER  DESCRIPTION <code>task_id</code> <p>The specific task to use.</p> <p> TYPE: <code>Literal['CIFAR10', 'ColorectalHistology', 'FashionMNIST']</code> </p> <code>datadir</code> <p>The path to where mfpbench stores it data. If left to <code>None</code>, will use <code>_default_download_dir = \"./data/jahs-bench-data\"</code>.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to give this benchmark instance</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark.</p> <ul> <li>if <code>str</code> - A preset</li> <li>if <code>Path</code> - path to a file</li> <li>if <code>dict</code>, Config, Configuration - A config</li> <li>if <code>None</code> - Use the default if available</li> </ul> <p> TYPE: <code>str | Path | JAHSConfig | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If given, will perturb the prior by this amount. Only used if <code>prior=</code> is given as a config.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='mfh3'</code> PARAMETER  DESCRIPTION <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>bias</code> <p>How much bias to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>How much noise to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark.</p> <ul> <li>if <code>Path</code> - path to a file</li> <li>if <code>Mapping</code> - Use directly</li> <li>if <code>None</code> - There is no prior</li> </ul> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='mfh6'</code> PARAMETER  DESCRIPTION <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>bias</code> <p>How much bias to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>How much noise to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark.</p> <ul> <li>if <code>Path</code> - path to a file</li> <li>if <code>Mapping</code> - Use directly</li> <li>if <code>None</code> - There is no prior</li> </ul> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>name='lcbench_tabular'</code> <p>Possible <code>task_id=</code>:</p> <pre><code>('adult', 'airlines', 'albert', 'Amazon_employee_access', 'APSFailure', 'Australian', 'bank-marketing', 'blood-transfusion-service-center', 'car', 'christine', 'cnae-9', 'connect-4', 'covertype', 'credit-g', 'dionis', 'fabert', 'Fashion-MNIST', 'helena', 'higgs', 'jannis', 'jasmine', 'jungle_chess_2pcs_raw_endgame_complete', 'kc1', 'KDDCup09_appetency', 'kr-vs-kp', 'mfeat-factors', 'MiniBooNE', 'nomao', 'numerai28.6', 'phoneme', 'segment', 'shuttle', 'sylvine', 'vehicle', 'volkert')\n</code></pre> PARAMETER  DESCRIPTION <code>task_id</code> <p>The task to benchmark on.</p> <p> TYPE: <code>str</code> </p> <code>datadir</code> <p>The directory to look for the data in. If <code>None</code>, uses the default download directory.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>remove_constants</code> <p>Whether to remove constant config columns from the data or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a str, will check the local location first for a prior specific for this benchmark, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a Mapping, will be used directly.</p> <p> TYPE: <code>str | Path | LCBenchTabularConfig | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, this is interpreted as the standard deviation of a normal distribution while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/get.py</code> <pre><code>def get(\n    name: str,\n    *,\n    prior: str | Path | Config | None = None,\n    preload: bool = False,\n    **kwargs: Any,\n) -&gt; Benchmark:\n    \"\"\"Get a benchmark.\n\n    Args:\n        name: The name of the benchmark\n        prior: The prior to use for the benchmark.\n            * str -\n                If it ends in {.json} or {.yaml, .yml}, it will convert it to a path and\n                use it as if it is a path to a config. Otherwise, it is treated as preset\n            * Path - path to a file\n            * Config - A Config object\n            * None - Use the default if available\n        preload: Whether to preload the benchmark data in\n        **kwargs: Extra arguments, optional or required for other benchmarks. Please\n            look up the associated benchmarks.\n\n    For the `#!python **kwargs`, please see the benchmarks listed below by `name=`\n\n    ??? note \"`#!python name='lcbench'` (YAHPO-GYM)\"\n\n        Possible `#!python task_id=`:\n\n        ::: mfpbench.LCBenchBenchmark.yahpo_instances\n\n        ::: mfpbench.LCBenchBenchmark.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='lm1b_transformer_2048'` (PD1)\"\n\n        ::: mfpbench.PD1lm1b_transformer_2048.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='uniref50_transformer_128'` (PD1)\"\n\n        ::: mfpbench.PD1uniref50_transformer_128.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='cifar100_wideresnet_2048'` (PD1)\"\n\n        ::: mfpbench.PD1cifar100_wideresnet_2048.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='imagenet_resnet_512'` (PD1)\"\n\n        ::: mfpbench.PD1imagenet_resnet_512.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='jahs'`\"\n\n        Possible `#!python task_id=`:\n\n        ::: mfpbench.JAHSBenchmark.task_ids\n\n        ::: mfpbench.JAHSBenchmark.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='mfh3'`\"\n\n        ::: mfpbench.MFHartmann3Benchmark.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='mfh6'`\"\n\n        ::: mfpbench.MFHartmann6Benchmark.__init__\n            options:\n                show_source: false\n\n    ??? note \"`#!python name='lcbench_tabular'`\"\n\n        Possible `#!python task_id=`:\n\n        ::: mfpbench.LCBenchTabularBenchmark.task_ids\n\n        ::: mfpbench.LCBenchTabularBenchmark.__init__\n            options:\n                show_source: false\n\n\n    \"\"\"  # noqa: E501\n    b = _mapping.get(name, None)\n    bench: Benchmark\n\n    if b is None:\n        raise ValueError(f\"{name} is not a benchmark in {list(_mapping.keys())}\")\n\n    if isinstance(prior, str) and any(\n        prior.endswith(suffix) for suffix in [\".json\", \".yaml\", \".yml\"]\n    ):\n        prior = Path(prior)\n\n    bench = b(prior=prior, **kwargs)\n\n    if preload:\n        bench.load()\n\n    return bench\n</code></pre>"},{"location":"api/mfpbench/get/#mfpbench.get.get--mfpbench.get.get","title":"get","text":""},{"location":"api/mfpbench/priors/","title":"priors","text":""},{"location":"api/mfpbench/priors/#mfpbench.priors.benchmarks","title":"<code>def benchmarks(*, seed, only=None, exclude=None, conditional_spaces=False)</code>","text":"<p>Generate benchmarks.</p> PARAMETER  DESCRIPTION <code>seed</code> <p>The seed to use for the benchmarks</p> <p> TYPE: <code>int</code> </p> <code>only</code> <p>Only generate benchmarks that start with one of the strings in this list</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>exclude</code> <p>Exclude benchmarks that start with one of the strings in this list</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>conditional_spaces</code> <p>Whether to include benchmarks with conditional spaces</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Benchmark</code> <p>The benchmarks</p> Source code in <code>src/mfpbench/priors.py</code> <pre><code>def benchmarks(\n    *,\n    seed: int,\n    only: list[str] | None = None,\n    exclude: list[str] | None = None,\n    conditional_spaces: bool = False,  # Not supported due to `remove_hyperparamter`\n) -&gt; Iterator[Benchmark]:\n    \"\"\"Generate benchmarks.\n\n    Args:\n        seed: The seed to use for the benchmarks\n        only: Only generate benchmarks that start with one of the strings in this list\n        exclude: Exclude benchmarks that start with one of the strings in this list\n        conditional_spaces: Whether to include benchmarks with conditional spaces\n\n    Yields:\n        The benchmarks\n    \"\"\"\n    # A mapping from the indexable name to the argument name and cls\n    benches: dict[str, tuple[str, type[Benchmark], str | None]] = {}\n\n    for name, cls in mfpbench._mapping.items():\n        if issubclass(cls, YAHPOBenchmark) and cls.yahpo_instances is not None:\n            benches.update(\n                {\n                    f\"{name}-{task_id}\": (name, cls, task_id)\n                    for task_id in cls.yahpo_instances\n                },\n            )\n        else:\n            benches[name] = (name, cls, None)\n\n    for index_name, (benchmark_name, cls, task_id) in benches.items():\n        if only is not None and not any(index_name.startswith(o) for o in only):\n            continue\n\n        if exclude is not None and any(index_name.startswith(e) for e in exclude):\n            continue\n\n        if cls.has_conditionals and not conditional_spaces:\n            continue\n\n        kwargs = {\n            \"name\": benchmark_name,\n            \"seed\": seed,\n        }\n        if task_id is not None:\n            kwargs[\"task_id\"] = task_id\n\n        yield mfpbench.get(**kwargs)  # type: ignore\n</code></pre>"},{"location":"api/mfpbench/priors/#mfpbench.priors.generate_priors","title":"<code>def generate_priors(*, seed, nsamples, to, prior_spec, prefix=None, fidelity=None, only=None, exclude=None, use_hartmann_optimum=None, clean=False)</code>","text":"<p>Generate priors for a benchmark.</p> Source code in <code>src/mfpbench/priors.py</code> <pre><code>def generate_priors(  # noqa: C901\n    *,\n    seed: int,\n    nsamples: int,\n    to: Path,\n    prior_spec: Iterable[tuple[str, int, float | None, float | None]],\n    prefix: str | None = None,\n    fidelity: int | float | None = None,\n    only: list[str] | None = None,\n    exclude: list[str] | None = None,\n    use_hartmann_optimum: list[str] | None = None,\n    clean: bool = False,\n) -&gt; None:\n    \"\"\"Generate priors for a benchmark.\"\"\"\n    if to.exists() and clean:\n        for child in filter(lambda path: path.is_file(), to.iterdir()):\n            child.unlink()\n\n    to.mkdir(exist_ok=True)\n    prior_spec = list(prior_spec)\n\n    for bench in benchmarks(seed=seed, only=only, exclude=exclude):\n        print(f\" - Benchmark: {bench.name}\")  # noqa: T201\n\n        max_fidelity = bench.fidelity_range[1]\n\n        # If a fidelity was specfied, then we need to make sure we can use it\n        # as an int in a benchmark with an int fidelity, no accidental rounding.\n        if fidelity is not None:\n            if (\n                isinstance(max_fidelity, int)\n                and isinstance(fidelity, float)\n                and fidelity.is_integer()\n            ):\n                fidelity = int(fidelity)\n\n            if type(fidelity) != type(max_fidelity):\n                raise ValueError(\n                    f\"Cannot use fidelity {fidelity} (type={type(fidelity)}) with\"\n                    f\" benchmark {bench.name}\",\n                )\n            at = fidelity\n        else:\n            at = max_fidelity\n\n        results: list[Result] = []\n        configs = bench.sample(n=nsamples)\n        results = [bench.query(config, at=at) for config in configs]\n\n        print(\" - Finished results\")  # noqa: T201\n        results = sorted(results, key=lambda r: r.error)\n        print(\" - Finished sorting\")  # noqa: T201\n\n        # Take out the results as specified by the prior and store the perturbations\n        # to make, if any.\n        prior_configs = {\n            name: (results[index].config, std, categorical_swap_chance)\n            for name, index, std, categorical_swap_chance in prior_spec\n        }\n\n        # Inject hartmann optimum in if specified\n        if use_hartmann_optimum is not None and isinstance(bench, MFHartmannBenchmark):\n            for optimum_replace in use_hartmann_optimum:\n                if optimum_replace not in prior_configs:\n                    raise ValueError(f\"Prior '{optimum_replace}' not found in priors.\")\n\n                opt = bench.optimum\n                _, std, categorical_swap_chance = prior_configs[optimum_replace]\n\n                prior_configs[optimum_replace] = (opt, std, categorical_swap_chance)\n\n        print(\" - Priors: \", prior_configs)  # noqa: T201\n\n        # Perturb each of the configs as specified to make the offset priors\n        space = bench.space\n        priors = {\n            name: config.perturb(\n                space,\n                seed=seed,\n                std=std,\n                categorical_swap_chance=categorical_swap_chance,\n            )\n            for name, (config, std, categorical_swap_chance) in prior_configs.items()\n        }\n        print(\" - Perturbed priors: \", priors)  # noqa: T201\n\n        name_components = []\n        if prefix is not None:\n            name_components.append(prefix)\n\n        name_components.append(bench.name)\n\n        basename = \"-\".join(name_components)\n\n        path_priors = [\n            (to / f\"{basename}-{prior_name}.yaml\", prior_config)\n            for prior_name, prior_config in priors.items()\n        ]\n        for path, prior in path_priors:\n            prior.save(path)\n</code></pre>"},{"location":"api/mfpbench/result/","title":"result","text":""},{"location":"api/mfpbench/result/#mfpbench.result.Result","title":"<code>class Result</code>   <code>dataclass</code>","text":"<p>         Bases: <code>ABC</code>, <code>Generic[C, F]</code></p> <p>Collect all results in a class for clarity.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.fidelity","title":"<code>fidelity: F</code>   <code>attr</code>","text":"<p>The fidelity of this result.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.config","title":"<code>config: C</code>   <code>classvar</code> <code>attr</code>","text":"<p>The config used to generate this result.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.score","title":"<code>score: float</code>   <code>abstractmethod</code> <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.error","title":"<code>error: float</code>   <code>abstractmethod</code> <code>prop</code>","text":"<p>The error of interest.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.test_score","title":"<code>test_score: float</code>   <code>abstractmethod</code> <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.test_error","title":"<code>test_error: float</code>   <code>abstractmethod</code> <code>prop</code>","text":"<p>The error on the test set.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.val_score","title":"<code>val_score: float</code>   <code>abstractmethod</code> <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.val_error","title":"<code>val_error: float</code>   <code>abstractmethod</code> <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.cost","title":"<code>cost: float</code>   <code>abstractmethod</code> <code>prop</code>","text":"<p>The time cost for evaluting this config.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.from_dict","title":"<code>def from_dict(config, result, fidelity)</code>   <code>classmethod</code>","text":"<p>Create from a dict or mapping object.</p> Source code in <code>src/mfpbench/result.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls,\n    config: C,\n    result: Mapping[str, Any],\n    fidelity: F,\n) -&gt; Self:\n    \"\"\"Create from a dict or mapping object.\"\"\"\n    fieldnames = set(cls.names())\n    if not fieldnames.issubset(result.keys()):\n        raise ValueError(\n            f\"Result dict is missing fields: {fieldnames - result.keys()}\",\n        )\n    # To help with serialization, we need to convert floats to... ehh floats\n    # This is due to some things returning an np.float -_-\n    result = {\n        k: float(v) if isinstance(v, float) else v\n        for k, v in result.items()\n        if k in fieldnames\n    }\n    return cls(config=config, fidelity=fidelity, **result)\n</code></pre>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.names","title":"<code>def names()</code>   <code>classmethod</code>","text":"<p>The names of the fields in this result.</p> Source code in <code>src/mfpbench/result.py</code> <pre><code>@classmethod\ndef names(cls) -&gt; tuple[str, ...]:\n    \"\"\"The names of the fields in this result.\"\"\"\n    return tuple(\n        f.name for f in fields(cls) if f.name not in (\"config\", \"fidelity\")\n    )\n</code></pre>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.from_row","title":"<code>def from_row(config, row, fidelity)</code>   <code>classmethod</code>","text":"<p>Create from a row of a dataframe.</p> Source code in <code>src/mfpbench/result.py</code> <pre><code>@classmethod\ndef from_row(\n    cls,\n    config: C,\n    row: Mapping[str, Any],\n    fidelity: F,\n) -&gt; Self:\n    \"\"\"Create from a row of a dataframe.\"\"\"\n    return cls.from_dict(config, dict(row), fidelity)\n</code></pre>"},{"location":"api/mfpbench/result/#mfpbench.result.Result.dict","title":"<code>def dict()</code>","text":"<p>Create a dict from this result.</p> Source code in <code>src/mfpbench/result.py</code> <pre><code>def dict(self) -&gt; dict[str, Any]:\n    \"\"\"Create a dict from this result.\"\"\"\n    d = asdict(self)\n    del d[\"config\"]\n    del d[\"fidelity\"]\n    return d\n</code></pre>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult","title":"<code>class GenericTabularResult</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Result[C, F]</code>, <code>Generic[C, F]</code></p> <p>A generic tabular result.</p> <p>This is useful for adhoc tabular benchmarks.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.error","title":"<code>error: float</code>   <code>prop</code>","text":"<p>The error of interest.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>The error on the test set.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.val_score","title":"<code>val_score: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.val_error","title":"<code>val_error: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.cost","title":"<code>cost: float</code>   <code>prop</code>","text":"<p>The time cost for evaluting this config.</p>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.__hash__","title":"<code>def __hash__()</code>","text":"<p>Hash based on the dictionary repr.</p> Source code in <code>src/mfpbench/result.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Hash based on the dictionary repr.\"\"\"\n    return (\n        hash(self.config) ^ hash(self.fidelity) ^ hash(tuple(self._values.items()))\n    )\n</code></pre>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.dict","title":"<code>def dict()</code>","text":"<p>As a raw dictionary.</p> Source code in <code>src/mfpbench/result.py</code> <pre><code>def dict(self) -&gt; Any:\n    \"\"\"As a raw dictionary.\"\"\"\n    return dict(self._values)\n</code></pre>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.from_dict","title":"<code>def from_dict(config, result, fidelity)</code>   <code>classmethod</code>","text":"<p>Create from a dict or mapping object.</p> Source code in <code>src/mfpbench/result.py</code> <pre><code>@override\n@classmethod\ndef from_dict(cls, config: C, result: Mapping[str, Any], fidelity: F) -&gt; Self:\n    \"\"\"Create from a dict or mapping object.\"\"\"\n    return cls(config=config, _values=dict(result), fidelity=fidelity)\n</code></pre>"},{"location":"api/mfpbench/result/#mfpbench.result.GenericTabularResult.names","title":"<code>def names()</code>   <code>classmethod</code>","text":"<p>The names of the fields in this result.</p> Source code in <code>src/mfpbench/result.py</code> <pre><code>@classmethod\ndef names(cls) -&gt; tuple[str, ...]:\n    \"\"\"The names of the fields in this result.\"\"\"\n    return tuple(\n        f.name\n        for f in fields(cls)\n        if f.name not in (\"config\", \"fidelity\", \"__values\")\n    )\n</code></pre>"},{"location":"api/mfpbench/resultframe/","title":"resultframe","text":"<p>A ResultFrame is a mapping from a config to all results for that config.</p>"},{"location":"api/mfpbench/resultframe/#mfpbench.resultframe.ResultFrame","title":"<code>class ResultFrame()</code>","text":"<p>         Bases: <code>Mapping[Union[C, F], List[R]]</code></p> Source code in <code>src/mfpbench/resultframe.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    # This lets us quickly index from a config to its registered results\n    # We take this to be the rows and __len__\n    self._ctor: dict[C, list[R]] = {}\n\n    # This lets us quickly go from a fidelity to its registered results\n    # This is akin to the columns\n    self._ftor: dict[F, list[R]] = {}\n\n    # This is an ordering for when a config result was added\n    self._result_order: list[R] = []\n</code></pre>"},{"location":"api/mfpbench/resultframe/#mfpbench.resultframe.ResultFrame.fidelities","title":"<code>fidelities: Iterator[F]</code>   <code>prop</code>","text":"<p>Get the fidelities that have been evaluated.</p>"},{"location":"api/mfpbench/resultframe/#mfpbench.resultframe.ResultFrame.configs","title":"<code>configs: Iterator[C]</code>   <code>prop</code>","text":"<p>Get the configs that have been evaluated.</p>"},{"location":"api/mfpbench/resultframe/#mfpbench.resultframe.ResultFrame.results","title":"<code>results: Iterator[R]</code>   <code>prop</code>","text":"<p>Get the results that have been evaluated.</p>"},{"location":"api/mfpbench/resultframe/#mfpbench.resultframe.ResultFrame.add","title":"<code>def add(result)</code>","text":"<p>Add a result to the frame.</p> Source code in <code>src/mfpbench/resultframe.py</code> <pre><code>def add(self, result: R) -&gt; None:\n    \"\"\"Add a result to the frame.\"\"\"\n    f = result.fidelity\n    c = result.config\n\n    if c in self._ctor:\n        self._ctor[c].append(result)\n    else:\n        self._ctor[c] = [result]\n\n    if f in self._ftor:\n        self._ftor[f].append(result)\n    else:\n        self._ftor[f] = [result]\n\n    self._result_order.append(result)\n</code></pre>"},{"location":"api/mfpbench/resultframe/#mfpbench.resultframe.ResultFrame.correlations","title":"<code>def correlations(at=None, *, method='spearman')</code>","text":"<p>The correlation ranksing between stored results.</p> <p>To calculate the correlations, we select all configs that are present in each selected fidelity.</p> PARAMETER  DESCRIPTION <code>at</code> <p>The fidelities to get correlations between, defaults to all of them</p> <p> TYPE: <code>Sequence[F] | None</code> DEFAULT: <code>None</code> </p> <code>method</code> <p>The method to calculate correlations with</p> <p> TYPE: <code>Literal['spearman', 'kendalltau', 'cosine']</code> DEFAULT: <code>'spearman'</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The correlation matrix with one row/column per fidelity</p> Source code in <code>src/mfpbench/resultframe.py</code> <pre><code>def correlations(\n    self,\n    at: Sequence[F] | None = None,\n    *,\n    method: Literal[\"spearman\", \"kendalltau\", \"cosine\"] = \"spearman\",\n) -&gt; np.ndarray:\n    \"\"\"The correlation ranksing between stored results.\n\n    To calculate the correlations, we select all configs that are present in each\n    selected fidelity.\n\n    Args:\n        at: The fidelities to get correlations between, defaults to all of them\n        method: The method to calculate correlations with\n\n    Returns:\n        The correlation matrix with one row/column per fidelity\n    \"\"\"\n    if len(self) == 0:\n        raise RuntimeError(\"Must evaluate at two fidelities at least\")\n    if len(self._ftor) &lt;= 1:\n        raise ValueError(f\"Only one fidelity {list(self._ftor)} evaluated\")\n\n    if at is None:\n        at = list(self._ftor.keys())\n\n    # Get the selected_fidelities\n    # { 1: [a, b, c, ...], 2: [b, c, d, ...], ..., 100: [d, c, e, b, ...] }\n    selected = {f: self._ftor[f] for f in at}\n\n    # We get the intersection of configs that are found at all fidelity values\n    # {b, c}\n    common = {result.config for result in chain.from_iterable(selected.values())}\n\n    # Next we prune out the selected fidelities results\n    # {1: [b, c], 2: [b, c], ..., 100: [c, b]}\n    # .. ensuring they're in some sorted order\n    # {1: [b, c], 2: [b, c], ..., 100: [b, c]}\n    selected = {\n        f: sorted(\n            [r for r in results if r.config in common],\n            key=lambda r: repr(r.config),\n        )\n        for f, results in selected.items()\n    }\n\n    # Lastly, we pull out the results\n    results = [\n        [r.error for r in fidelity_results]\n        for fidelity_results in selected.values()\n    ]\n\n    x = np.asarray(results)\n    return rank_correlation(x, method=method)\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/","title":"setup_benchmark","text":""},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.BenchmarkSetup","title":"<code>class BenchmarkSetup</code>   <code>dataclass</code>","text":"<p>         Bases: <code>ABC</code></p>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.BenchmarkSetup.name","title":"<code>name: str</code>   <code>classvar</code>","text":"<p>The name of the benchmark group.</p>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.BenchmarkSetup.download","title":"<code>def download(path)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Download the data from the source.</p> PARAMETER  DESCRIPTION <code>path</code> <p>The root path to download to. Will install to path/name</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>@classmethod\n@abstractmethod\ndef download(cls, path: Path) -&gt; None:\n    \"\"\"Download the data from the source.\n\n    Args:\n        path: The root path to download to.\n            Will install to\n            path/[name][mfpbench.setup_benchmark.BenchmarkSetup.name]\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.BenchmarkSetup.default_location","title":"<code>def default_location()</code>   <code>classmethod</code>","text":"<p>Get the default location for the data.</p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>@classmethod\ndef default_location(cls) -&gt; Path:\n    \"\"\"Get the default location for the data.\"\"\"\n    return DATAROOT / cls.name\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.BenchmarkSetup.default_requirements_path","title":"<code>def default_requirements_path()</code>   <code>classmethod</code>","text":"<p>Get the default location for the data.</p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>@classmethod\ndef default_requirements_path(cls) -&gt; Path:\n    \"\"\"Get the default location for the data.\"\"\"\n    return REQ_DIR / f\"{cls.name}.txt\"\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.BenchmarkSetup.install_cmd","title":"<code>def install_cmd(requirements_path)</code>   <code>classmethod</code>","text":"<p>Get the command to install the requirements.</p> PARAMETER  DESCRIPTION <code>requirements_path</code> <p>The path to the requirements.txt file.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>@classmethod\ndef install_cmd(cls, requirements_path: Path) -&gt; str:\n    \"\"\"Get the command to install the requirements.\n\n    Args:\n        requirements_path: The path to the requirements.txt file.\n    \"\"\"\n    return f\"python -m pip install -r {requirements_path.absolute()}\"\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.BenchmarkSetup.install","title":"<code>def install(requirements_path)</code>   <code>classmethod</code>","text":"<p>Install the requirements to download the data.</p> PARAMETER  DESCRIPTION <code>requirements_path</code> <p>The path to the requirements.txt file.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>@classmethod\ndef install(cls, requirements_path: Path) -&gt; None:\n    \"\"\"Install the requirements to download the data.\n\n    Args:\n        requirements_path: The path to the requirements.txt file.\n    \"\"\"\n    cmd = cls.install_cmd(requirements_path)\n    print(f\"Running: {cmd}\")\n    subprocess.run(cmd, shell=True, check=True)  # noqa: S602\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.BenchmarkSetup.source","title":"<code>def source(name)</code>   <code>classmethod</code>","text":"<p>Get all the sources.</p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>@classmethod\ndef source(cls, name: str) -&gt; type[BenchmarkSetup]:\n    \"\"\"Get all the sources.\"\"\"\n    subclasses = cls.__subclasses__()\n\n    found = first_true(subclasses, pred=lambda x: x.name == name, default=None)\n    if found is None:\n        names = [subclass.name for subclass in subclasses]\n        raise ValueError(f\"No source with {name=}\\nPlease choose from {names}\")\n\n    return found\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.BenchmarkSetup.sources","title":"<code>def sources()</code>   <code>classmethod</code>","text":"<p>Get all the sources.</p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>@classmethod\ndef sources(cls) -&gt; list[type[BenchmarkSetup]]:\n    \"\"\"Get all the sources.\"\"\"\n    return cls.__subclasses__()\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.download_status","title":"<code>def download_status(source, datadir=None)</code>","text":"<p>Check whether the data is downloaded for some source.</p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>def download_status(source: str, datadir: Path | None = None) -&gt; bool:\n    \"\"\"Check whether the data is downloaded for some source.\"\"\"\n    datadir = datadir if datadir is not None else DATAROOT\n    _source = BenchmarkSetup.source(source)\n    source_path = datadir / _source.name\n    return source_path.exists() and bool(\n        next(source_path.iterdir(), False),  # noqa: FBT003\n    )\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.print_download_status","title":"<code>def print_download_status(sources=None, datadir=None)</code>","text":"<p>Print the status of the data.</p> PARAMETER  DESCRIPTION <code>sources</code> <p>The benchmarks to check the status of. <code>None</code> for all.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>datadir</code> <p>Where the root data directory is</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>def print_download_status(\n    sources: list[str] | None = None,\n    datadir: Path | None = None,\n) -&gt; None:\n    \"\"\"Print the status of the data.\n\n    Args:\n        sources: The benchmarks to check the status of. `None` for all.\n        datadir: Where the root data directory is\n    \"\"\"\n    datadir = datadir if datadir is not None else DATAROOT\n    s = f\"root: {datadir.absolute()}\"\n    print(s)\n    print(\"-\" * len(s))\n\n    if (sources is not None and \"all\" in sources) or sources is None:\n        names = [source.name for source in BenchmarkSetup.sources()]\n    else:\n        names = sources\n\n    for name in names:\n        if download_status(name, datadir=datadir):\n            print(f\"[\u2713] {name}\")\n        else:\n            print(f\"[x] {name: &lt;20} python -m mfpbench download --benchmark {name}\")\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.print_requirements","title":"<code>def print_requirements(benchmarks)</code>","text":"<p>Print the status of the data.</p> PARAMETER  DESCRIPTION <code>benchmarks</code> <p>The benchmarks to check the status of. <code>None</code> for all.</p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>def print_requirements(benchmarks: list[str]) -&gt; None:\n    \"\"\"Print the status of the data.\n\n    Args:\n        benchmarks: The benchmarks to check the status of. `None` for all.\n    \"\"\"\n    sources = BenchmarkSetup.sources()\n    if benchmarks is not None and \"all\" not in benchmarks:\n        sources = [source for source in sources if source.name in benchmarks]\n\n    for source in sources:\n        print(\"=\" * len(source.name))\n        print(f\"{source.name}\")\n        print(\"=\" * len(source.name))\n\n        path = source.default_requirements_path()\n        pathstr = f\"path: {path}\"\n        cmd = source.install_cmd(path)\n        cmdstr = f\"cmd: {cmd}\"\n        execpath = sys.executable\n        execstr = f\"exec: {execpath}\"\n        n = max(len(pathstr), len(cmdstr), len(execstr))\n\n        print(pathstr)\n        print(execstr)\n        print(cmdstr)\n        print(\"-\" * n)\n        if not path.exists():\n            print(\"Not found!\")\n        else:\n            print(f\"# {path}\")\n            with path.open(\"r\") as f:\n                print(f.read())\n        print()\n</code></pre>"},{"location":"api/mfpbench/setup_benchmark/#mfpbench.setup_benchmark.setup","title":"<code>def setup(benchmark, *, datadir=None, download=True, install=False, force=False)</code>","text":"<p>Download data for a benchmark.</p> PARAMETER  DESCRIPTION <code>benchmark</code> <p>The benchmark to download the data for.</p> <p> TYPE: <code>str</code> </p> <code>datadir</code> <p>Where the root data directory is</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>download</code> <p>Whether to download the data</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>install</code> <p>Whether to install the requirements for the benchmark. If True, will install the default. If a str, tries to interpret it as a full path.</p> <p> TYPE: <code>str | bool</code> DEFAULT: <code>False</code> </p> <code>force</code> <p>Whether to force redownload of the data</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/mfpbench/setup_benchmark.py</code> <pre><code>def setup(\n    benchmark: str,\n    *,\n    datadir: Path | None = None,\n    download: bool = True,\n    install: str | bool = False,\n    force: bool = False,\n) -&gt; None:\n    \"\"\"Download data for a benchmark.\n\n    Args:\n        benchmark: The benchmark to download the data for.\n        datadir: Where the root data directory is\n        download: Whether to download the data\n        install: Whether to install the requirements for the benchmark.\n            If True, will install the default. If a str, tries to interpret\n            it as a full path.\n        force: Whether to force redownload of the data\n    \"\"\"\n    datadir = datadir if datadir is not None else DATAROOT\n\n    source = BenchmarkSetup.source(benchmark)\n    source_path = datadir / source.name\n\n    if download:\n        if source_path.exists() and force:\n            print(f\"Removing {source_path}\")\n            shutil.rmtree(source_path)\n\n        if not source_path.exists() or next(source_path.iterdir(), None) is None:\n            print(f\"Downloading to {source_path}\")\n            source_path.mkdir(exist_ok=True, parents=True)\n            source.download(source_path)\n            print(f\"Finished downloading to {source_path}\")\n        else:\n            print(f\"Already found something at {source_path}\")\n            pass\n\n    if install is not False:\n        if install is True:\n            req_path = source.default_requirements_path()\n        else:\n            req_path = Path(install)\n            if not req_path.exists():\n                raise FileNotFoundError(f\"Could not find requirements at {req_path}\")\n\n        print(f\"Installing requirements at {req_path}\")\n        source.install(req_path)\n</code></pre>"},{"location":"api/mfpbench/stats/","title":"stats","text":""},{"location":"api/mfpbench/stats/#mfpbench.stats.spearmanr","title":"<code>def spearmanr(X)</code>","text":"<p>Calculate the spearman rank correlation between observer rankings.</p> Source code in <code>src/mfpbench/stats.py</code> <pre><code>def spearmanr(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the spearman rank correlation between observer rankings.\"\"\"\n    corr, _ = scipy.stats.spearmanr(X, axis=1)\n    result: np.ndarray\n    if isinstance(corr, float):\n        result = np.eye(N=2, dtype=float)\n        result[0, 1] = corr\n        result[1, 0] = corr\n    else:\n        result = corr\n\n    return result\n</code></pre>"},{"location":"api/mfpbench/stats/#mfpbench.stats.kendalltau","title":"<code>def kendalltau(X)</code>","text":"<p>Calculate the kendall tau rank correlation between observer rankings.</p> Source code in <code>src/mfpbench/stats.py</code> <pre><code>def kendalltau(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the kendall tau rank correlation between observer rankings.\"\"\"\n    results: np.ndarray\n    results = np.eye(N=len(X), dtype=float)\n\n    idxs = range(len(X))\n    for i, j in combinations(idxs, 2):\n        x = X[i, :]\n        y = X[j, :]\n\n        corr, _ = scipy.stats.kendalltau(x, y)\n\n        results[i, j] = corr\n        results[j, i] = corr\n\n    return results\n</code></pre>"},{"location":"api/mfpbench/stats/#mfpbench.stats.cosine","title":"<code>def cosine(X)</code>","text":"<p>Calculate the cosine rank correlation between observer rankings.</p> Source code in <code>src/mfpbench/stats.py</code> <pre><code>def cosine(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the cosine rank correlation between observer rankings.\"\"\"\n    X_norm = np.zeros(shape=X.shape, like=X)\n    for i, row in enumerate(X):\n        mi = row.min()\n        ma = row.max()\n        X_norm[i] = 2 * ((row - mi) / (ma - mi)) - 1\n\n    results: np.ndarray\n    results = np.eye(N=len(X), dtype=float)\n    idxs = range(len(X))\n    for i, j in combinations(idxs, 2):\n        x = X_norm[i, :10]\n        y = X_norm[j, :10]\n\n        corr = np.dot(x, y) / (norm(x) * norm(y))\n\n        results[i, j] = corr\n        results[j, i] = corr\n\n    return results\n</code></pre>"},{"location":"api/mfpbench/stats/#mfpbench.stats.rank_correlation","title":"<code>def rank_correlation(x, *, method='cosine')</code>","text":"<p>Calculate rank correlation between observer rankings.</p> <p>Will return the correlation between the two rankings, otherwise it will return a correlation matrix where each row represents a random variable.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The rankings to calculate correlations for where each row</p> <p> TYPE: <code>ndarray</code> </p> <code>method</code> <p>The method to use</p> <p> TYPE: <code>Literal['spearman', 'kendalltau', 'cosine']</code> DEFAULT: <code>'cosine'</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The correlation matrix</p> Source code in <code>src/mfpbench/stats.py</code> <pre><code>def rank_correlation(\n    x: np.ndarray,\n    *,\n    method: Literal[\"spearman\", \"kendalltau\", \"cosine\"] = \"cosine\",\n) -&gt; np.ndarray:\n    \"\"\"Calculate rank correlation between observer rankings.\n\n    Will return the correlation between the two rankings,\n    otherwise it will return a correlation matrix where each row represents a random\n    variable.\n\n    Args:\n        x: The rankings to calculate correlations for where each row\n        method: The method to use\n\n    Returns:\n        The correlation matrix\n    \"\"\"\n    if not isinstance(x, np.ndarray):\n        x = np.asarray(x)\n\n    assert x.ndim &gt;= 2\n\n    measures = {\"spearman\": spearmanr, \"kendalltau\": kendalltau, \"cosine\": cosine}\n    f = measures.get(method)\n    if f is None:\n        raise NotImplementedError(method)\n\n    return f(x)\n</code></pre>"},{"location":"api/mfpbench/tabular/","title":"tabular","text":""},{"location":"api/mfpbench/tabular/#mfpbench.tabular.TabularBenchmark","title":"<code>class TabularBenchmark(name, table, *, config_name, fidelity_name, result_keys, config_keys, remove_constants=False, space=None, seed=None, prior=None, perturb_prior=None)</code>","text":"<p>         Bases: <code>Benchmark[CTabular, R, F]</code></p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of this benchmark.</p> <p> TYPE: <code>str</code> </p> <code>table</code> <p>The table to use for the benchmark.</p> <p> TYPE: <code>DataFrame</code> </p> <code>config_name</code> <p>The column in the table that contains the config id</p> <p> TYPE: <code>str</code> </p> <code>fidelity_name</code> <p>The column in the table that contains the fidelity</p> <p> TYPE: <code>str</code> </p> <code>result_keys</code> <p>The columns in the table that contain the results</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>config_keys</code> <p>The columns in the table that contain the config values</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>remove_constants</code> <p>Remove constant config columns from the data or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>space</code> <p>The configuration space to use for the benchmark. If None, will just be an empty space.</p> <p> TYPE: <code>ConfigurationSpace | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a string, will be treated as a prior specific for this benchmark if it can be found, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a dict or Configuration, will be used directly.</p> <p> TYPE: <code>str | Path | CTabular | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the benchmark.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/tabular.py</code> <pre><code>def __init__(  # noqa: PLR0913, C901\n    self,\n    name: str,\n    table: pd.DataFrame,\n    *,\n    config_name: str,\n    fidelity_name: str,\n    result_keys: Sequence[str],\n    config_keys: Sequence[str],\n    remove_constants: bool = False,\n    space: ConfigurationSpace | None = None,\n    seed: int | None = None,\n    prior: str | Path | CTabular | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n):\n    \"\"\"Initialize the benchmark.\n\n    Args:\n        name: The name of this benchmark.\n        table: The table to use for the benchmark.\n        config_name: The column in the table that contains the config id\n        fidelity_name: The column in the table that contains the fidelity\n        result_keys: The columns in the table that contain the results\n        config_keys: The columns in the table that contain the config values\n        remove_constants: Remove constant config columns from the data or not.\n        space: The configuration space to use for the benchmark. If None, will\n            just be an empty space.\n        prior: The prior to use for the benchmark. If None, no prior is used.\n            If a string, will be treated as a prior specific for this benchmark\n            if it can be found, otherwise assumes it to be a Path.\n            If a Path, will load the prior from the path.\n            If a dict or Configuration, will be used directly.\n        perturb_prior: If not None, will perturb the prior by this amount.\n            For numericals, while for categoricals, this is interpreted as the\n            probability of swapping the value for a random one.\n        seed: The seed to use for the benchmark.\n    \"\"\"\n    cls = self.__class__\n    if remove_constants:\n\n        def is_constant(_s: pd.Series) -&gt; bool:\n            _arr = _s.to_numpy()\n            return bool((_arr == _arr[0]).all())\n\n        constant_cols = [\n            col for col in table.columns if is_constant(table[col])  # type: ignore\n        ]\n        table = table.drop(columns=constant_cols)  # type: ignore\n        config_keys = [k for k in config_keys if k not in constant_cols]\n\n    # If the table isn't indexed, index it\n    index_cols = [config_name, fidelity_name]\n    if table.index.names != index_cols:\n        # Only drop the index if it's not relevant.\n        relevant_cols: list[str] = [  # type: ignore\n            *list(index_cols),  # type: ignore\n            *list(result_keys),\n            *list(config_keys),\n        ]\n        relevant = any(name in relevant_cols for name in table.index.names)\n        table = table.reset_index(drop=not relevant)\n\n        if config_name not in table.columns:\n            raise ValueError(f\"{config_name=} not in columns {table.columns}\")\n        if fidelity_name not in table.columns:\n            raise ValueError(f\"{fidelity_name=} not in columns {table.columns}\")\n\n        table = table.set_index(index_cols)\n        table = table.sort_index()\n\n    # Make sure all keys are in the table\n    for key in chain(result_keys, config_keys):\n        if key not in table.columns:\n            raise ValueError(f\"{key=} not in columns {table.columns}\")\n\n    # Make sure the keyword \"id\" is not in the columns as we use it to\n    # identify configs\n    if \"id\" in table.columns:\n        raise ValueError(f\"{table.columns=} contains 'id'. Please rename it\")\n\n    # Make sure we have equidistance fidelities for all configs\n    fidelity_values = table.index.get_level_values(fidelity_name)\n    fidelity_counts = fidelity_values.value_counts()\n    if not (fidelity_counts == fidelity_counts.iloc[0]).all():\n        raise ValueError(f\"{fidelity_name=} not  uniform. \\n{fidelity_counts}\")\n\n    # We now have the following table\n    #\n    # config_id fidelity | **metric, **config_values\n    #     0         0    |\n    #               1    |\n    #               2    |\n    #     1         0    |\n    #               1    |\n    #               2    |\n    #   ...\n\n    # Here we get all the unique configs\n    # config_id fidelity | **metric, **config_values\n    #     0         0    |\n    #     1         0    |\n    #   ...\n    config_id_table = table.groupby(level=config_name).agg(\"first\")\n    configs = {\n        str(config_id): cls.Config.from_dict(\n            {\n                **row[config_keys].to_dict(),  # type: ignore\n                \"id\": str(config_id),\n            },\n        )\n        for config_id, row in config_id_table.iterrows()\n    }\n\n    fidelity_values = table.index.get_level_values(fidelity_name).unique()\n\n    # We just assume equidistant fidelities\n    sorted_fids = sorted(fidelity_values)\n    start = sorted_fids[0]\n    end = sorted_fids[-1]\n    step = sorted_fids[1] - sorted_fids[0]\n\n    # Create the configuration space\n    if space is None:\n        space = ConfigurationSpace(name, seed=seed)\n\n    self.table = table\n    self.configs = configs\n    self.fidelity_name = fidelity_name\n    self.config_name = config_name\n    self.config_keys = sorted(config_keys)\n    self.result_keys = sorted(result_keys)\n    self.fidelity_range = (start, end, step)  # type: ignore\n\n    super().__init__(\n        name=name,\n        seed=seed,\n        space=space,\n        prior=prior,\n        perturb_prior=perturb_prior,\n    )\n</code></pre>"},{"location":"api/mfpbench/tabular/#mfpbench.tabular.TabularBenchmark.table","title":"<code>table: pd.DataFrame</code>   <code>attr</code>","text":"<p>The table of results used for this benchmark</p>"},{"location":"api/mfpbench/tabular/#mfpbench.tabular.TabularBenchmark.fidelity_name","title":"<code>fidelity_name: str</code>   <code>attr</code>","text":"<p>The name of the fidelity used in this benchmark</p>"},{"location":"api/mfpbench/tabular/#mfpbench.tabular.TabularBenchmark.config_name","title":"<code>config_name: str</code>   <code>attr</code>","text":"<p>The column in the table that contains the config id. Will be set to the index</p>"},{"location":"api/mfpbench/tabular/#mfpbench.tabular.TabularBenchmark.config_keys","title":"<code>config_keys: Sequence[str]</code>   <code>attr</code>","text":"<p>The keys in the table that contain the config</p>"},{"location":"api/mfpbench/tabular/#mfpbench.tabular.TabularBenchmark.result_keys","title":"<code>result_keys: Sequence[str]</code>   <code>attr</code>","text":"<p>The keys in the table that contain the results</p>"},{"location":"api/mfpbench/tabular/#mfpbench.tabular.TabularBenchmark.query","title":"<code>def query(config, at=None, *, argmax=None, argmin=None)</code>","text":"<p>Submit a query and get a result.</p> <p>Passing a raw config</p> <p>If a mapping is passed (and not a <code>Config</code> object), we will attempt to look for <code>id</code> in the mapping, to know which config to lookup.</p> <p>If this fails, we will try match the config to one of the configs in the benchmark.</p> <p>Prefer to pass the <code>Config</code> object directly if possible.</p> Override <p>This function overrides the default <code>query()</code> to allow for this config matching</p> PARAMETER  DESCRIPTION <code>config</code> <p>The query to use</p> <p> TYPE: <code>CTabular | Mapping[str, Any] | str</code> </p> <code>at</code> <p>The fidelity at which to query, defaults to None which means maximum</p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> <code>argmax</code> <p>Whether to return the argmax up to the point <code>at</code>. Will be slower as it has to get the entire trajectory. Uses the key from the Results.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>argmin</code> <p>Whether to return the argmin up to the point <code>at</code>. Will be slower as it has to get the entire trajectory. Uses the key from the Results.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>R</code> <p>The result of the query</p> Source code in <code>src/mfpbench/tabular.py</code> <pre><code>def query(\n    self,\n    config: CTabular | Mapping[str, Any] | str,\n    at: F | None = None,\n    *,\n    argmax: str | None = None,\n    argmin: str | None = None,\n) -&gt; R:\n    \"\"\"Submit a query and get a result.\n\n    !!! warning \"Passing a raw config\"\n\n        If a mapping is passed (and **not** a [`Config`][mfpbench.Config] object),\n        we will attempt to look for `id` in the mapping, to know which config to\n        lookup.\n\n        If this fails, we will try match the config to one of the configs in\n        the benchmark.\n\n        Prefer to pass the [`Config`][mfpbench.Config] object directly if possible.\n\n    ??? note \"Override\"\n\n        This function overrides the default\n        [`query()`][mfpbench.Benchmark.query] to allow for this\n        config matching\n\n    Args:\n        config: The query to use\n        at: The fidelity at which to query, defaults to None which means *maximum*\n        argmax: Whether to return the argmax up to the point `at`. Will be slower as\n            it has to get the entire trajectory. Uses the key from the Results.\n        argmin: Whether to return the argmin up to the point `at`. Will be slower as\n            it has to get the entire trajectory. Uses the key from the Results.\n\n    Returns:\n        The result of the query\n    \"\"\"\n    _config = self._find_config(config)\n    return super().query(\n        _config,\n        at=at,  # type: ignore\n        argmax=argmax,\n        argmin=argmin,\n    )\n</code></pre>"},{"location":"api/mfpbench/tabular/#mfpbench.tabular.TabularBenchmark.trajectory","title":"<code>def trajectory(config, *, frm=None, to=None, step=None)</code>","text":"<p>Submit a query and get a result.</p> <p>Passing a raw config</p> <p>If a mapping is passed (and not a <code>Config</code> object), we will attempt to look for <code>id</code> in the mapping, to know which config to lookup.</p> <p>If this fails, we will try match the config to one of the configs in the benchmark.</p> <p>Prefer to pass the <code>Config</code> object directly if possible.</p> Override <p>This function overrides the default <code>trajectory()</code> to allow for this config matching</p> PARAMETER  DESCRIPTION <code>config</code> <p>The query to use</p> <p> TYPE: <code>CTabular | Mapping[str, Any] | str</code> </p> <code>frm</code> <p>Start of the curve, should default to the start</p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> <code>to</code> <p>End of the curve, should default to the total</p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> <code>step</code> <p>Step size, defaults to <code>cls.default_step</code></p> <p> TYPE: <code>F | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[R]</code> <p>The result of the query</p> Source code in <code>src/mfpbench/tabular.py</code> <pre><code>@override\ndef trajectory(\n    self,\n    config: CTabular | Mapping[str, Any] | str,\n    *,\n    frm: F | None = None,\n    to: F | None = None,\n    step: F | None = None,\n) -&gt; list[R]:\n    \"\"\"Submit a query and get a result.\n\n    !!! warning \"Passing a raw config\"\n\n        If a mapping is passed (and **not** a [`Config`][mfpbench.Config] object),\n        we will attempt to look for `id` in the mapping, to know which config to\n        lookup.\n\n        If this fails, we will try match the config to one of the configs in\n        the benchmark.\n\n        Prefer to pass the [`Config`][mfpbench.Config] object directly if possible.\n\n    ??? note \"Override\"\n\n        This function overrides the default\n        [`trajectory()`][mfpbench.Benchmark.trajectory] to allow for this\n        config matching\n\n    Args:\n        config: The query to use\n        frm: Start of the curve, should default to the start\n        to: End of the curve, should default to the total\n        step: Step size, defaults to ``cls.default_step``\n\n    Returns:\n        The result of the query\n    \"\"\"\n    _config = self._find_config(config)\n    return super().trajectory(_config, frm=frm, to=to, step=step)  # type: ignore\n</code></pre>"},{"location":"api/mfpbench/tabular/#mfpbench.tabular.TabularBenchmark.sample","title":"<code>def sample(n=None, *, seed=None)</code>","text":"<p>Sample a random possible config.</p> PARAMETER  DESCRIPTION <code>n</code> <p>How many samples to take, None means jsut a single one, not in a list</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the sampling.</p> <p>Seeding</p> <p>This is different than any seed passed to the construction of the benchmark.</p> <p> TYPE: <code>int | RandomState | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CTabular | list[CTabular]</code> <p>Get back a possible Config to use</p> Source code in <code>src/mfpbench/tabular.py</code> <pre><code>@override\ndef sample(\n    self,\n    n: int | None = None,\n    *,\n    seed: int | np.random.RandomState | None = None,\n) -&gt; CTabular | list[CTabular]:\n    \"\"\"Sample a random possible config.\n\n    Args:\n        n: How many samples to take, None means jsut a single one, not in a list\n        seed: The seed to use for the sampling.\n\n            !!! note \"Seeding\"\n\n                This is different than any seed passed to the construction\n                of the benchmark.\n\n    Returns:\n        Get back a possible Config to use\n    \"\"\"\n    _seed: int | None\n    if isinstance(seed, np.random.RandomState):\n        _seed = seed.random_integers(0, 2**32 - 1)\n    else:\n        _seed = seed\n\n    rng = np.random.default_rng(seed=_seed)\n\n    config_items: list[CTabular] = list(self.configs.values())\n    n_configs = len(config_items)\n    sample_amount = n if n is not None else 1\n\n    if sample_amount &gt; n_configs:\n        raise ValueError(\n            f\"Can't sample {sample_amount} configs from {n_configs} configs\",\n        )\n\n    indices = rng.choice(n_configs, size=sample_amount, replace=False)\n    if n is None:\n        first_index: int = indices[0]\n        return config_items[first_index]\n\n    return [config_items[i] for i in indices]\n</code></pre>"},{"location":"api/mfpbench/tabular/#mfpbench.tabular.GenericTabularBenchmark","title":"<code>class GenericTabularBenchmark(table, *, name=None, fidelity_name, config_name, result_keys, config_keys, result_mapping=None, remove_constants=False, space=None, seed=None, prior=None, perturb_prior=None)</code>","text":"<p>         Bases: <code>TabularBenchmark[GenericTabularConfig, GenericTabularResult[GenericTabularConfig, F], F]</code></p> PARAMETER  DESCRIPTION <code>table</code> <p>The table to use for the benchmark</p> <p> TYPE: <code>DataFrame</code> </p> <code>name</code> <p>The name of the benchmark. If None, will be set to <code>unknown-{datetime.now().isoformat()}</code></p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>fidelity_name</code> <p>The column in the table that contains the fidelity</p> <p> TYPE: <code>str</code> </p> <code>config_name</code> <p>The column in the table that contains the config id</p> <p> TYPE: <code>str</code> </p> <code>result_keys</code> <p>The columns in the table that contain the results</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>config_keys</code> <p>The columns in the table that contain the config values</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>result_mapping</code> <p>A mapping from the result keys to the table keys. If a string, will be used as the key in the table. If a callable, will be called with the table and the result will be used as the value.</p> <p> TYPE: <code>dict[str, str | Callable[[DataFrame], Any]] | None</code> DEFAULT: <code>None</code> </p> <code>remove_constants</code> <p>Remove constant config columns from the data or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>space</code> <p>The configuration space to use for the benchmark. If None, will just be an empty space.</p> <p> TYPE: <code>ConfigurationSpace | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a str, will check the local location first for a prior specific for this benchmark, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a Mapping, will be used directly.</p> <p> TYPE: <code>str | Path | GenericTabularConfig | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, this is interpreted as the standard deviation of a normal distribution while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/tabular.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    table: pd.DataFrame,\n    *,\n    name: str | None = None,\n    fidelity_name: str,\n    config_name: str,\n    result_keys: Sequence[str],\n    config_keys: Sequence[str],\n    result_mapping: (dict[str, str | Callable[[pd.DataFrame], Any]] | None) = None,\n    remove_constants: bool = False,\n    space: ConfigurationSpace | None = None,\n    seed: int | None = None,\n    prior: str | Path | GenericTabularConfig | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n):\n    \"\"\"Initialize the benchmark.\n\n    Args:\n        table: The table to use for the benchmark\n        name: The name of the benchmark. If None, will be set to\n            `unknown-{datetime.now().isoformat()}`\n\n        fidelity_name: The column in the table that contains the fidelity\n        config_name: The column in the table that contains the config id\n        result_keys: The columns in the table that contain the results\n        config_keys: The columns in the table that contain the config values\n        result_mapping: A mapping from the result keys to the table keys.\n            If a string, will be used as the key in the table. If a callable,\n            will be called with the table and the result will be used as the value.\n        remove_constants: Remove constant config columns from the data or not.\n        space: The configuration space to use for the benchmark. If None, will\n            just be an empty space.\n        seed: The seed to use.\n        prior: The prior to use for the benchmark. If None, no prior is used.\n            If a str, will check the local location first for a prior\n            specific for this benchmark, otherwise assumes it to be a Path.\n            If a Path, will load the prior from the path.\n            If a Mapping, will be used directly.\n        perturb_prior: If not None, will perturb the prior by this amount.\n            For numericals, this is interpreted as the standard deviation of a\n            normal distribution while for categoricals, this is interpreted\n            as the probability of swapping the value for a random one.\n    \"\"\"\n    if name is None:\n        name = f\"unknown-{datetime.now().isoformat()}\"\n\n    _result_mapping: dict = result_mapping if result_mapping is not None else {}\n\n    # Remap the result keys so it works with the generic result types\n    if _result_mapping is not None:\n        for k, v in _result_mapping.items():\n            if isinstance(v, str):\n                if v not in table.columns:\n                    raise ValueError(f\"{v} not in columns\\n{table.columns}\")\n\n                table[k] = table[v]\n            elif callable(v):\n                table[k] = v(table)\n            else:\n                raise ValueError(f\"Unknown result mapping {v} for {k}\")\n\n    super().__init__(\n        name=name,\n        table=table,\n        config_name=config_name,\n        fidelity_name=fidelity_name,\n        result_keys=[*result_keys, *_result_mapping.keys()],\n        config_keys=config_keys,\n        remove_constants=remove_constants,\n        space=space,\n        seed=seed,\n        prior=prior,\n        perturb_prior=perturb_prior,\n    )\n</code></pre>"},{"location":"api/mfpbench/util/","title":"util","text":""},{"location":"api/mfpbench/util/#mfpbench.util.findwhere","title":"<code>def findwhere(itr, func, *, default=-1)</code>","text":"<p>Find the index of the next occurence where func is True.</p> PARAMETER  DESCRIPTION <code>itr</code> <p>The iterable to search over</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>func</code> <p>The function to use</p> <p> TYPE: <code>Callable[[T], bool]</code> </p> <code>default</code> <p>The default value to give if no value was found where func was True</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The first index where func was True</p> Source code in <code>src/mfpbench/util.py</code> <pre><code>def findwhere(itr: Iterable[T], func: Callable[[T], bool], *, default: int = -1) -&gt; int:\n    \"\"\"Find the index of the next occurence where func is True.\n\n    Args:\n        itr: The iterable to search over\n        func: The function to use\n        default: The default value to give if no value was found where func was True\n\n    Returns:\n        The first index where func was True\n    \"\"\"\n    return next((i for i, t in enumerate(itr) if func(t)), default)\n</code></pre>"},{"location":"api/mfpbench/util/#mfpbench.util.remove_hyperparameter","title":"<code>def remove_hyperparameter(name, space)</code>","text":"<p>A new configuration space with the hyperparameter removed.</p> <p>Essentially copies hp over and fails if there is conditionals or forbiddens</p> <p>Warning</p> <p>This will not work with conditional search spaces and will raise an error</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the hyperparameter to remove</p> <p> TYPE: <code>str</code> </p> <code>space</code> <p>The configuration space to remove the hyperparameter from</p> <p> TYPE: <code>ConfigurationSpace</code> </p> RETURNS DESCRIPTION <code>ConfigurationSpace</code> <p>The space with the hyperparameter removed</p> Source code in <code>src/mfpbench/util.py</code> <pre><code>def remove_hyperparameter(name: str, space: ConfigurationSpace) -&gt; ConfigurationSpace:\n    \"\"\"A new configuration space with the hyperparameter removed.\n\n    Essentially copies hp over and fails if there is conditionals or forbiddens\n\n    !!! warning\n\n        This will not work with conditional search spaces and will raise an\n        error\n\n    Args:\n        name: The name of the hyperparameter to remove\n        space: The configuration space to remove the hyperparameter from\n\n    Returns:\n        The space with the hyperparameter removed\n    \"\"\"\n    if name not in space._hyperparameters:\n        raise ValueError(f\"{name} not in {space}\")\n\n    if any(space.get_conditions()):\n        raise NotImplementedError(\"We do not handle conditionals for now\")\n\n    if any(space.get_forbiddens()):\n        raise NotImplementedError(\"We do not handle forbiddems for now\")\n\n    # Copying conditionals only work on objects and not named entities\n    # Seeing as we copy objects and don't use the originals, transfering these\n    # to the new objects is a bit tedious, possible but not required at this time\n    # ... same goes for forbiddens\n    assert name not in space._conditionals, \"Can't handle conditionals\"\n    assert not any(\n        name != f.hyperparameter.name for f in space.get_forbiddens()\n    ), \"Can't handle forbiddens\"\n\n    hps = [copy(hp) for hp in space.get_hyperparameters() if hp.name != name]\n\n    if isinstance(space.random, np.random.RandomState):\n        new_seed = space.random.randint(2**32 - 1)\n    else:\n        new_seed = copy(space.random)\n\n    new_space = ConfigurationSpace(\n        # TODO: not sure if this will have implications, assuming not\n        seed=new_seed,\n        name=copy(space.name),\n        meta=copy(space.meta),\n    )\n    new_space.add_hyperparameters(hps)\n\n    return new_space\n</code></pre>"},{"location":"api/mfpbench/util/#mfpbench.util.pairs","title":"<code>def pairs(itr)</code>","text":"<p>An iterator over pairs of items in the iterator.</p> <pre><code># Check if sorted\nif all(a &lt; b for a, b in pairs(items)):\n    ...\n</code></pre> PARAMETER  DESCRIPTION <code>itr</code> <p>An itr of items</p> <p> TYPE: <code>Iterable[T]</code> </p> RETURNS DESCRIPTION <code>Iterator[tuple[T, T]]</code> <p>An itr of sequential pairs of the items</p> Source code in <code>src/mfpbench/util.py</code> <pre><code>def pairs(itr: Iterable[T]) -&gt; Iterator[tuple[T, T]]:\n    \"\"\"An iterator over pairs of items in the iterator.\n\n    ```python\n    # Check if sorted\n    if all(a &lt; b for a, b in pairs(items)):\n        ...\n    ```\n\n    Args:\n        itr: An itr of items\n\n    Returns:\n        An itr of sequential pairs of the items\n    \"\"\"\n    itr1, itr2 = tee(itr)\n\n    # Skip first item\n    _ = next(itr2)\n\n    # Check there is a second element\n    peek = next(itr2, None)\n    if peek is None:\n        raise ValueError(\"Can't create a pair from iterable with 1 item\")\n\n    # Put it back in\n    itr2 = chain([peek], itr2)\n\n    return iter((a, b) for a, b in zip(itr1, itr2))\n</code></pre>"},{"location":"api/mfpbench/util/#mfpbench.util.intersection","title":"<code>def intersection(*items)</code>","text":"<p>Does an intersection over all collection of items.</p> <pre><code>ans = intersection([\"a\", \"b\", \"c\"], \"ab\", (\"b\", \"c\"))\nitems = [(1, 2, 3), (2, 3), (4, 5)]\nans = intesection(*items)\n</code></pre> PARAMETER  DESCRIPTION <code>*items</code> <p>Iterable things</p> <p> TYPE: <code>Iterable[T]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>set[T]</code> <p>The intersection of all items</p> Source code in <code>src/mfpbench/util.py</code> <pre><code>def intersection(*items: Iterable[T]) -&gt; set[T]:\n    \"\"\"Does an intersection over all collection of items.\n\n    ```python\n    ans = intersection([\"a\", \"b\", \"c\"], \"ab\", (\"b\", \"c\"))\n    items = [(1, 2, 3), (2, 3), (4, 5)]\n    ans = intesection(*items)\n    ```\n\n    Args:\n        *items: Iterable things\n\n    Returns:\n        The intersection of all items\n    \"\"\"\n    if len(items) == 0:\n        return set()\n\n    return set(reduce(lambda s1, s2: set(s1) &amp; set(s2), items, items[0]))\n</code></pre>"},{"location":"api/mfpbench/util/#mfpbench.util.invert","title":"<code>def invert(d)</code>","text":"<p>Invert the key value pairs of a dictionary.</p> Source code in <code>src/mfpbench/util.py</code> <pre><code>def invert(d: Mapping[K, V]) -&gt; Mapping[V, K]:\n    \"\"\"Invert the key value pairs of a dictionary.\"\"\"\n    return {v: k for k, v in d.items()}\n</code></pre>"},{"location":"api/mfpbench/util/#mfpbench.util.rename","title":"<code>def rename(d, keys)</code>","text":"<p>Rename keys of a dictionary based on a set of keys to update.</p> Source code in <code>src/mfpbench/util.py</code> <pre><code>def rename(d: Mapping[K1, V], keys: Mapping[K1, K2]) -&gt; dict[K1 | K2, V]:\n    \"\"\"Rename keys of a dictionary based on a set of keys to update.\"\"\"\n    return {keys.get(k1, k1): v for k1, v in d.items()}\n</code></pre>"},{"location":"api/mfpbench/util/#mfpbench.util.perturb","title":"<code>def perturb(value, hp, std, seed=None)</code>","text":"<p>Perturb a value based on a hyperparameter.</p> PARAMETER  DESCRIPTION <code>value</code> <p>The value to perturb</p> <p> TYPE: <code>ValueT</code> </p> <code>hp</code> <p>The hyperparameter it comes from</p> <p> TYPE: <code>Constant | UniformIntegerHyperparameter | UniformFloatHyperparameter | NormalIntegerHyperparameter | NormalFloatHyperparameter | CategoricalHyperparameter | OrdinalHyperparameter</code> </p> <code>std</code> <p>The standard deviation of the noise to add</p> <p> TYPE: <code>float</code> </p> <code>seed</code> <p>The seed to use for the perturbation</p> <p> TYPE: <code>int | RandomState | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValueT</code> <p>The perturbed value</p> Source code in <code>src/mfpbench/util.py</code> <pre><code>def perturb(  # noqa: C901, PLR0912, PLR0911, PLR0915\n    value: ValueT,\n    hp: (\n        Constant\n        | UniformIntegerHyperparameter\n        | UniformFloatHyperparameter\n        | NormalIntegerHyperparameter\n        | NormalFloatHyperparameter\n        | CategoricalHyperparameter\n        | OrdinalHyperparameter\n    ),\n    std: float,\n    seed: int | np.random.RandomState | None = None,\n) -&gt; ValueT:\n    \"\"\"Perturb a value based on a hyperparameter.\n\n    Args:\n        value: The value to perturb\n        hp: The hyperparameter it comes from\n        std: The standard deviation of the noise to add\n        seed: The seed to use for the perturbation\n\n    Returns:\n        The perturbed value\n    \"\"\"\n    # TODO:\n    # * https://github.com/automl/ConfigSpace/issues/289\n    assert 0 &lt;= std &lt;= 1, \"Noise must be between 0 and 1\"\n    rng: np.random.RandomState\n    if seed is None:\n        rng = np.random.RandomState()\n    elif isinstance(seed, int):\n        rng = np.random.RandomState(seed)\n    else:\n        rng = seed\n\n    if isinstance(hp, Constant):\n        return value\n\n    if isinstance(\n        hp,\n        (\n            NormalIntegerHyperparameter,\n            NormalFloatHyperparameter,\n            UniformFloatHyperparameter,\n            UniformIntegerHyperparameter,\n        ),\n    ):\n        # TODO:\n        # * https://github.com/automl/ConfigSpace/issues/287\n        # * https://github.com/automl/ConfigSpace/issues/290\n        # * https://github.com/automl/ConfigSpace/issues/291\n        # Doesn't act as intended\n        assert hp.upper is not None\n        assert hp.lower is not None\n        assert hp.q is None\n        assert isinstance(value, (int, float))\n\n        if isinstance(hp, UniformIntegerHyperparameter):\n            if hp.log:\n                _lower = np.log(hp.lower)\n                _upper = np.log(hp.upper)\n            else:\n                _lower = hp.lower\n                _upper = hp.upper\n        elif isinstance(hp, NormalIntegerHyperparameter):\n            _lower = hp.nfhp._lower\n            _upper = hp.nfhp._upper\n        elif isinstance(hp, (UniformFloatHyperparameter, NormalFloatHyperparameter)):\n            _lower = hp._lower\n            _upper = hp._upper\n        else:\n            raise RuntimeError(\"Wut\")\n\n        space_length = std * (_upper - _lower)\n        rescaled_std = std * space_length\n\n        if not hp.log:\n            sample = np.clip(rng.normal(value, rescaled_std), _lower, _upper)\n        else:\n            logged_value = np.log(value)\n            sample = rng.normal(logged_value, rescaled_std)\n            sample = np.clip(np.exp(sample), hp.lower, hp.upper)\n\n        if isinstance(hp, (UniformIntegerHyperparameter, NormalIntegerHyperparameter)):\n            return int(np.rint(sample))  # type: ignore\n\n        if isinstance(hp, (UniformFloatHyperparameter, NormalFloatHyperparameter)):\n            return float(sample)  # type: ignore\n\n        raise RuntimeError(\"Please report to github, shouldn't get here\")\n\n        # if isinstance(hp, (BetaIntegerHyperparameter, BetaFloatHyperparameter)):\n        # TODO\n        # raise NotImplementedError(\n        # \"BetaIntegerHyperparameter, BetaFloatHyperparameter not implemented\"\n        # )\n\n    if isinstance(hp, CategoricalHyperparameter):\n        # We basically with (1 - std) choose the same value, otherwise uniformly select\n        # at random\n        if rng.uniform() &lt; 1 - std:\n            return value\n\n        choices = set(hp.choices) - {value}\n        return rng.choice(list(choices))  # type: ignore\n\n    if isinstance(hp, OrdinalHyperparameter):\n        # TODO:\n        # * https://github.com/automl/ConfigSpace/issues/288\n        # We build a normal centered at the value\n        if rng.uniform() &lt; 1 - std:\n            return value\n\n        # [0, 1,  2, 3]\n        #       ^  mean\n        index_value = hp.sequence.index(value)\n        index_std = std * len(hp.sequence)\n        normal_value = rng.normal(index_value, index_std)\n        index = int(np.rint(np.clip(normal_value, 0, len(hp.sequence))))\n        return hp.sequence[index]  # type: ignore\n\n    raise ValueError(f\"Can't perturb {hp}\")\n</code></pre>"},{"location":"api/mfpbench/jahs/benchmark/","title":"benchmark","text":""},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSConfig","title":"<code>class JAHSConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Config</code></p> <p>The config for JAHSBench, useful to have regardless of the configspace used.</p> <p>github.com/automl/jahs_bench_201/blob/main/jahs_bench/lib/core/configspace.py</p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config incase required.</p> Source code in <code>src/mfpbench/jahs/benchmark.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate this config incase required.\"\"\"\n    # Just being explicit to catch bugs easily, we can remove later\n    assert self.N in [1, 3, 5]\n    assert self.W in [4, 8, 16]\n    assert self.Op1 in [0, 1, 2, 3, 4, 5]\n    assert self.Op2 in [0, 1, 2, 3, 4, 5]\n    assert self.Op3 in [0, 1, 2, 3, 4, 5]\n    assert self.Op4 in [0, 1, 2, 3, 4, 5]\n    assert self.Op5 in [0, 1, 2, 3, 4, 5]\n    assert self.Op6 in [0, 1, 2, 3, 4, 5]\n    assert self.Resolution in [0.25, 0.5, 1.0]\n    assert isinstance(self.TrivialAugment, bool)\n    assert self.Activation in [\"ReLU\", \"Hardswish\", \"Mish\"]\n    assert self.Optimizer in [\"SGD\"]\n    assert 1e-3 &lt;= self.LearningRate &lt;= 1e0\n    assert 1e-5 &lt;= self.WeightDecay &lt;= 1e-2\n</code></pre>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSResult","title":"<code>class JAHSResult</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Result[JAHSConfig, int]</code></p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSResult.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSResult.error","title":"<code>error: float</code>   <code>prop</code>","text":"<p>The error of interest.</p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSResult.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSResult.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>The error on the test set.</p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSResult.val_score","title":"<code>val_score: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSResult.val_error","title":"<code>val_error: float</code>   <code>prop</code>","text":"<p>The error on the validation set.</p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSResult.cost","title":"<code>cost: float</code>   <code>prop</code>","text":"<p>The time taken (assumed to be seconds).</p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSBenchmark","title":"<code>class JAHSBenchmark(task_id, *, datadir=None, seed=None, prior=None, perturb_prior=None)</code>","text":"<p>         Bases: <code>Benchmark[JAHSConfig, JAHSResult, int]</code>, <code>ABC</code></p> PARAMETER  DESCRIPTION <code>task_id</code> <p>The specific task to use.</p> <p> TYPE: <code>Literal['CIFAR10', 'ColorectalHistology', 'FashionMNIST']</code> </p> <code>datadir</code> <p>The path to where mfpbench stores it data. If left to <code>None</code>, will use <code>_default_download_dir = \"./data/jahs-bench-data\"</code>.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to give this benchmark instance</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark.</p> <ul> <li>if <code>str</code> - A preset</li> <li>if <code>Path</code> - path to a file</li> <li>if <code>dict</code>, Config, Configuration - A config</li> <li>if <code>None</code> - Use the default if available</li> </ul> <p> TYPE: <code>str | Path | JAHSConfig | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If given, will perturb the prior by this amount. Only used if <code>prior=</code> is given as a config.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/jahs/benchmark.py</code> <pre><code>def __init__(\n    self,\n    task_id: Literal[\"CIFAR10\", \"ColorectalHistology\", \"FashionMNIST\"],\n    *,\n    datadir: str | Path | None = None,\n    seed: int | None = None,\n    prior: str | Path | JAHSConfig | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n):\n    \"\"\"Initialize the benchmark.\n\n    Args:\n        task_id: The specific task to use.\n        datadir: The path to where mfpbench stores it data. If left to `None`,\n            will use `#!python _default_download_dir = \"./data/jahs-bench-data\"`.\n        seed: The seed to give this benchmark instance\n        prior: The prior to use for the benchmark.\n\n            * if `str` - A preset\n            * if `Path` - path to a file\n            * if `dict`, Config, Configuration - A config\n            * if `None` - Use the default if available\n\n        perturb_prior: If given, will perturb the prior by this amount.\n            Only used if `prior=` is given as a config.\n    \"\"\"\n    cls = self.__class__\n    if datadir is None:\n        datadir = JAHSBenchSource.default_location()\n\n    datadir = Path(datadir)\n\n    if not datadir.exists():\n        raise FileNotFoundError(\n            f\"Can't find folder at {datadir}.\"\n            f\"\\n`python -m mfpbench download --status --data-dir {datadir}`\",\n        )\n\n    # Loaded on demand with `@property`\n    self._bench: jahs_bench.Benchmark | None = None\n    self.datadir = datadir\n    self.task_id = task_id\n\n    name = f\"jahs_{task_id}\"\n    super().__init__(\n        seed=seed,\n        name=name,\n        space=cls._jahs_configspace(name=name, seed=seed),\n        prior=prior,\n        perturb_prior=perturb_prior,\n    )\n</code></pre>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSBenchmark.task_ids","title":"<code>task_ids: tuple[str, ...]</code>   <code>classvar</code> <code>attr</code>","text":"<pre><code>('CIFAR10', 'ColorectalHistology', 'FashionMNIST')\n</code></pre>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSBenchmark.bench","title":"<code>bench: jahs_bench.Benchmark</code>   <code>prop</code>","text":"<p>The underlying benchmark used.</p>"},{"location":"api/mfpbench/jahs/benchmark/#mfpbench.jahs.benchmark.JAHSBenchmark.load","title":"<code>def load()</code>","text":"<p>Pre-load JAHS XGBoost model before querying the first time.</p> Source code in <code>src/mfpbench/jahs/benchmark.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Pre-load JAHS XGBoost model before querying the first time.\"\"\"\n    # Access the property\n    _ = self.bench\n</code></pre>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/","title":"benchmark","text":""},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularResult","title":"<code>class LCBenchTabularResult</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Result[LCBenchTabularConfig, int]</code></p>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularResult.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularResult.error","title":"<code>error: float</code>   <code>prop</code>","text":"<p>The error of interest.</p>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularResult.val_score","title":"<code>val_score: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularResult.val_error","title":"<code>val_error: float</code>   <code>prop</code>","text":"<p>The error on the validation set.</p>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularResult.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularResult.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>The error on the test set.</p>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularResult.cost","title":"<code>cost: float</code>   <code>prop</code>","text":"<p>The time to train the configuration (assumed to be seconds).</p>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularBenchmark","title":"<code>class LCBenchTabularBenchmark(task_id, datadir=None, *, remove_constants=False, seed=None, prior=None, perturb_prior=None)</code>","text":"<p>         Bases: <code>TabularBenchmark</code></p> PARAMETER  DESCRIPTION <code>task_id</code> <p>The task to benchmark on.</p> <p> TYPE: <code>str</code> </p> <code>datadir</code> <p>The directory to look for the data in. If <code>None</code>, uses the default download directory.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>remove_constants</code> <p>Whether to remove constant config columns from the data or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a str, will check the local location first for a prior specific for this benchmark, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a Mapping, will be used directly.</p> <p> TYPE: <code>str | Path | LCBenchTabularConfig | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, this is interpreted as the standard deviation of a normal distribution while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/lcbench_tabular/benchmark.py</code> <pre><code>def __init__(\n    self,\n    task_id: str,\n    datadir: str | Path | None = None,\n    *,\n    remove_constants: bool = False,\n    seed: int | None = None,\n    prior: str | Path | LCBenchTabularConfig | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n) -&gt; None:\n    \"\"\"Initialize the benchmark.\n\n    Args:\n        task_id: The task to benchmark on.\n        datadir: The directory to look for the data in. If `None`, uses the default\n            download directory.\n        remove_constants: Whether to remove constant config columns from the data or\n            not.\n        seed: The seed to use.\n        prior: The prior to use for the benchmark. If None, no prior is used.\n            If a str, will check the local location first for a prior\n            specific for this benchmark, otherwise assumes it to be a Path.\n            If a Path, will load the prior from the path.\n            If a Mapping, will be used directly.\n        perturb_prior: If not None, will perturb the prior by this amount.\n            For numericals, this is interpreted as the standard deviation of a\n            normal distribution while for categoricals, this is interpreted\n            as the probability of swapping the value for a random one.\n    \"\"\"\n    cls = self.__class__\n    if task_id not in cls.task_ids:\n        raise ValueError(f\"Unknown task {task_id}, must be one of {cls.task_ids}\")\n\n    if datadir is None:\n        datadir = LCBenchTabularSource.default_location()\n\n    table_path = Path(datadir) / f\"{task_id}.parquet\"\n    if not table_path.exists():\n        raise FileNotFoundError(\n            f\"Could not find table {table_path}.\"\n            f\"`python -m mfpbench download --status --data-dir {datadir}\",\n        )\n\n    self.task_id = task_id\n    self.datadir = Path(datadir) if isinstance(datadir, str) else datadir\n\n    table = pd.read_parquet(table_path)\n\n    # NOTE: Dropping of 0'th epoch\n    # As the 0'th epoch is a completely untrained model, this is different\n    # from 1st epoch where it is trained and it's score is somewhat representitive.\n    # This is a benchmarking library for HPO and we do not want to include untrained\n    # models nor have it be part of the fidelity range. For that reason, we drop\n    # the 0'th epoch.\n    drop_epoch = 0\n    table = table.drop(index=drop_epoch, level=\"epoch\")\n\n    benchmark_task_name = f\"lcbench_tabular-{task_id}\"\n    space = _get_raw_lcbench_space(name=f\"lcbench_tabular-{task_id}\", seed=seed)\n\n    super().__init__(\n        table=table,  # type: ignore\n        name=benchmark_task_name,\n        config_name=\"id\",\n        fidelity_name=cls.fidelity_name,\n        result_keys=LCBenchTabularResult.names(),\n        config_keys=LCBenchTabularConfig.names(),\n        remove_constants=remove_constants,\n        space=space,\n        seed=seed,\n        prior=prior,\n        perturb_prior=perturb_prior,\n    )\n</code></pre>"},{"location":"api/mfpbench/lcbench_tabular/benchmark/#mfpbench.lcbench_tabular.benchmark.LCBenchTabularBenchmark.task_ids","title":"<code>task_ids: tuple[str, ...]</code>   <code>classvar</code>","text":"<pre><code>('adult', 'airlines', 'albert', 'Amazon_employee_access', 'APSFailure', 'Australian', 'bank-marketing', 'blood-transfusion-service-center', 'car', 'christine', 'cnae-9', 'connect-4', 'covertype', 'credit-g', 'dionis', 'fabert', 'Fashion-MNIST', 'helena', 'higgs', 'jannis', 'jasmine', 'jungle_chess_2pcs_raw_endgame_complete', 'kc1', 'KDDCup09_appetency', 'kr-vs-kp', 'mfeat-factors', 'MiniBooNE', 'nomao', 'numerai28.6', 'phoneme', 'segment', 'shuttle', 'sylvine', 'vehicle', 'volkert')\n</code></pre>"},{"location":"api/mfpbench/pd1/benchmark/","title":"benchmark","text":""},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Config","title":"<code>class PD1Config</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Config</code></p> <p>The config for PD1.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Result","title":"<code>class PD1Result</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Result[PD1Config, int]</code></p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Result.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Result.error","title":"<code>error: float</code>   <code>prop</code>","text":"<p>The error of interest.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Result.val_score","title":"<code>val_score: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Result.val_error","title":"<code>val_error: float</code>   <code>prop</code>","text":"<p>The error on the validation set.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Result.cost","title":"<code>cost: float</code>   <code>prop</code>","text":"<p>The train cost of the model (asssumed to be seconds).</p> <p>Please double check with YAHPO.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1ResultSimple","title":"<code>class PD1ResultSimple</code>   <code>dataclass</code>","text":"<p>         Bases: <code>PD1Result</code></p> <p>Used for all PD1 benchmarks, except imagenet, lm1b, translate_wmt, uniref50.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1ResultSimple.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1ResultSimple.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>The error on the test set.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1ResultTransformer","title":"<code>class PD1ResultTransformer</code>   <code>dataclass</code>","text":"<p>         Bases: <code>PD1Result</code></p> <p>Imagenet, lm1b, translate_wmt, uniref50, cifar100 contains no test error.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1ResultTransformer.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1ResultTransformer.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>The error on the test set.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark","title":"<code>class PD1Benchmark(*, datadir=None, seed=None, prior=None, perturb_prior=None)</code>","text":"<p>         Bases: <code>Benchmark[C, R, int]</code></p> PARAMETER  DESCRIPTION <code>datadir</code> <p>Path to the data directory</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the space</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>Any prior to use for the benchmark</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>Whether to perturb the prior. If specified, this is interpreted as the std of a normal from which to perturb numerical hyperparameters of the prior, and the raw probability of swapping a categorical value.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/pd1/benchmark.py</code> <pre><code>def __init__(\n    self,\n    *,\n    datadir: str | Path | None = None,\n    seed: int | None = None,\n    prior: str | Path | C | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n):\n    \"\"\"Create a PD1 Benchmark.\n\n    Args:\n        datadir: Path to the data directory\n        seed: The seed to use for the space\n        prior: Any prior to use for the benchmark\n        perturb_prior: Whether to perturb the prior. If specified, this\n            is interpreted as the std of a normal from which to perturb\n            numerical hyperparameters of the prior, and the raw probability\n            of swapping a categorical value.\n    \"\"\"\n    cls = self.__class__\n    space = cls._create_space(seed=seed)\n    name = f\"{cls.pd1_dataset}-{cls.pd1_model}-{cls.pd1_batchsize}\"\n    if datadir is None:\n        datadir = PD1Source.default_location()\n\n    datadir = Path(datadir) if isinstance(datadir, str) else datadir\n    if not datadir.exists():\n        raise FileNotFoundError(\n            f\"Can't find folder at {datadir}.\"\n            f\"\\n`python -m mfpbench download --status --data-dir {datadir.parent}`\",\n        )\n    self._surrogates: dict[str, XGBRegressor] | None = None\n    self.datadir = datadir\n\n    super().__init__(\n        seed=seed,\n        name=name,\n        prior=prior,\n        perturb_prior=perturb_prior,\n        space=space,\n    )\n</code></pre>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.pd1_dataset","title":"<code>pd1_dataset: str</code>   <code>classvar</code>","text":"<p>The dataset that this benchmark uses.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.pd1_model","title":"<code>pd1_model: str</code>   <code>classvar</code>","text":"<p>The model that this benchmark uses.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.pd1_batchsize","title":"<code>pd1_batchsize: int</code>   <code>classvar</code>","text":"<p>The batchsize that this benchmark uses.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.pd1_metrics","title":"<code>pd1_metrics: tuple[str, ...]</code>   <code>classvar</code>","text":"<p>The metrics that are available for this benchmark.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.Config","title":"<code>Config: type[C]</code>   <code>attr</code>","text":"<p>The config type for this benchmark.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.Result","title":"<code>Result: type[R]</code>   <code>attr</code>","text":"<p>The result type for this benchmark.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.surrogates","title":"<code>surrogates: dict[str, XGBRegressor]</code>   <code>prop</code>","text":"<p>The surrogates for this benchmark, one per metric.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.surrogate_dir","title":"<code>surrogate_dir: Path</code>   <code>prop</code>","text":"<p>The directory where the surrogates are stored.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.surrogate_paths","title":"<code>surrogate_paths: dict[str, Path]</code>   <code>prop</code>","text":"<p>The paths to the surrogates.</p>"},{"location":"api/mfpbench/pd1/benchmark/#mfpbench.pd1.benchmark.PD1Benchmark.load","title":"<code>def load()</code>","text":"<p>Load the benchmark.</p> Source code in <code>src/mfpbench/pd1/benchmark.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the benchmark.\"\"\"\n    _ = self.surrogates  # Call up the surrogate into memory\n</code></pre>"},{"location":"api/mfpbench/pd1/benchmarks/cifar100/","title":"cifar100","text":""},{"location":"api/mfpbench/pd1/benchmarks/imagenet/","title":"imagenet","text":""},{"location":"api/mfpbench/pd1/benchmarks/lm1b/","title":"lm1b","text":""},{"location":"api/mfpbench/pd1/benchmarks/translate_wmt/","title":"translate_wmt","text":""},{"location":"api/mfpbench/pd1/benchmarks/uniref50/","title":"uniref50","text":""},{"location":"api/mfpbench/pd1/processing/columns/","title":"columns","text":""},{"location":"api/mfpbench/pd1/processing/process_script/","title":"process_script","text":""},{"location":"api/mfpbench/pd1/processing/process_script/#mfpbench.pd1.processing.process_script.Datapack","title":"<code>class Datapack</code>   <code>dataclass</code>","text":""},{"location":"api/mfpbench/pd1/processing/process_script/#mfpbench.pd1.processing.process_script.Datapack.unpacked_path","title":"<code>unpacked_path: Path</code>   <code>prop</code>","text":"<p>The path to the unpacked csv.</p>"},{"location":"api/mfpbench/pd1/processing/process_script/#mfpbench.pd1.processing.process_script.safe_accumulate","title":"<code>def safe_accumulate(x, fill=np.nan)</code>","text":"<p>Accumulate, but fill in missing values with a default value.</p> Source code in <code>src/mfpbench/pd1/processing/process_script.py</code> <pre><code>def safe_accumulate(\n    x: Iterator[float | None] | float,\n    fill: float = np.nan,\n) -&gt; Iterator[float]:\n    \"\"\"Accumulate, but fill in missing values with a default value.\"\"\"\n    if isinstance(x, float):\n        return iter([fill if np.isnan(x) else x])\n\n    itr = iter(f if f is not None else fill for f in x)\n    return accumulate(itr)\n</code></pre>"},{"location":"api/mfpbench/pd1/processing/process_script/#mfpbench.pd1.processing.process_script.uniref50_epoch_convert","title":"<code>def uniref50_epoch_convert(x)</code>","text":"<p>Converts the epochs of uniref50 to some usable form.</p> Converts <p>0             NaN 1    [0, 0, 0, 1] 2           [nan] 3     [0, 0, nan].</p> to <p>0             NaN 1    [0, 1, 2, 3] 2           [nan] 3     [0, 1, nan]</p> Source code in <code>src/mfpbench/pd1/processing/process_script.py</code> <pre><code>def uniref50_epoch_convert(x: float | list[float]) -&gt; float | list[float]:\n    \"\"\"Converts the epochs of uniref50 to some usable form.\n\n    Converts:\n        0             NaN\n        1    [0, 0, 0, 1]\n        2           [nan]\n        3     [0, 0, nan].\n\n    to:\n        0             NaN\n        1    [0, 1, 2, 3]\n        2           [nan]\n        3     [0, 1, nan]\n    \"\"\"\n    if isinstance(x, list):\n        return [i if not pd.isna(e) else e for i, e in enumerate(x, start=1)]\n\n    return x\n</code></pre>"},{"location":"api/mfpbench/pd1/processing/process_script/#mfpbench.pd1.processing.process_script.process_pd1","title":"<code>def process_pd1(tarball)</code>","text":"<p>Process the pd1 dataset.</p> <p>Note</p> <p>Will write to the same directory as the tarball</p> PARAMETER  DESCRIPTION <code>tarball</code> <p>The path to the tarball containing the raw data.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/mfpbench/pd1/processing/process_script.py</code> <pre><code>def process_pd1(tarball: Path) -&gt; None:  # noqa: PLR0912, PLR0915, C901\n    \"\"\"Process the pd1 dataset.\n\n    !!! note\n\n        Will write to the same directory as the tarball\n\n    Args:\n        tarball: The path to the tarball containing the raw data.\n    \"\"\"\n    datadir = tarball.parent\n    rawdir = tarball.parent / \"raw\"\n\n    fulltable_name = \"full.csv\"\n    fulltable_path = datadir / fulltable_name\n\n    # If we have the full table we can skip the tarball extraction\n    if not fulltable_path.exists():\n        if not tarball.exists():\n            raise FileNotFoundError(f\"No tarball found at {tarball}\")\n\n        rawdir.mkdir(exist_ok=True)\n\n        # Unpack it to the rawdir\n        readme_path = rawdir / \"README.txt\"\n        if not readme_path.exists():\n            shutil.unpack_archive(tarball, rawdir)\n\n            unpacked_folder_name = \"pd1\"  # This is what the tarball will unpack into\n            unpacked_folder = rawdir / unpacked_folder_name\n\n            # Move everything from the uncpack folder to the \"raw\" folder\n            for filepath in unpacked_folder.iterdir():\n                to = rawdir / filepath.name\n                shutil.move(str(filepath), str(to))\n\n            # Remove the archive folder, its all been moved to \"raw\"\n            shutil.rmtree(str(unpacked_folder))\n\n    # For processing the df\n    drop_columns = [c.name for c in COLUMNS if not c.keep]\n    renames = {c.name: c.rename for c in COLUMNS if c.rename is not None}\n    hps = [c.rename for c in COLUMNS if c.hp]\n    # metrics = [c.rename if c.rename else c.name for c in COLUMNS if c.metric]\n\n    dfs: list[pd.DataFrame] = []\n    for matched, phase in product([True, False], [0, 1]):\n        # Unpack the jsonl.gz archive if needed\n        datapack = Datapack(matched=matched, phase=phase, dir=rawdir)\n        df = datapack._unpack()\n\n        # Tag them from the dataset they came from\n        df[\"matched\"] = matched\n        df[\"phase\"] = phase\n\n        df = df.drop(columns=drop_columns)\n        df = df.rename(columns=renames)\n\n        dfs.append(df)\n\n    # We now merge them all into one super table for convenience\n    full_df = pd.concat(dfs, ignore_index=True)\n\n    # Since some columns values are essentially lists, we need to explode them out\n    # However, we need to make sure to distuinguish between transformer and not as\n    # transformers do not have test error available\n    # We've already renamed the columns them at this point\n    list_columns = [c.rename if c.rename else c.name for c in COLUMNS if c.type == list]\n    transformer_datasets = [\"uniref50\", \"translate_wmt\", \"imagenet\", \"lm1b\"]\n    dataset_columns = [\"dataset\", \"model\", \"batch_size\"]\n\n    groups = full_df.groupby(dataset_columns)\n    for (name, model, batchsize), _dataset in groups:  # type: ignore\n        fname = f\"{name}-{model}-{batchsize}\"\n        logger.info(fname)\n\n        if name in transformer_datasets:\n            explode_columns = [c for c in list_columns if c != \"test_error_rate\"]\n            dataset = _dataset.drop(columns=[\"test_error_rate\"])\n        else:\n            explode_columns = list_columns\n            dataset = _dataset\n\n        if name == \"uniref50\":\n            # For some reason the epochs of this datasets are basically [0, 0, 0, 1]\n            # We just turn this into an incremental thing\n            epochs = dataset[\"epoch\"]\n            assert epochs is not None\n            dataset[\"epoch\"] = dataset[\"epoch\"].apply(  # type: ignore\n                uniref50_epoch_convert,\n            )\n\n        # Make sure train_cost rows are all of equal length\n        dataset[\"train_cost\"] = [\n            np.nan if r in (None, np.nan) else list(safe_accumulate(r, fill=np.nan))\n            for r in dataset[\"train_cost\"]  # type: ignore\n        ]\n\n        # Explode out the lists in the entires of the datamframe to be a single long\n        # dataframe with each element of that list on its own row\n        dataset = dataset.explode(explode_columns, ignore_index=True)\n        logger.info(f\"{len(dataset)} rows\")\n        assert isinstance(dataset, pd.DataFrame)\n\n        # Remove any rows that have a nan in the exploded columns\n        nan_rows = dataset[\"train_cost\"].isna()\n        logger.info(f\" - len(nan_rows) {sum(nan_rows)}\")\n\n        logger.debug(f\"Removing rows with nan in {explode_columns}\")\n        dataset = dataset[~nan_rows]  # type: ignore\n        assert isinstance(dataset, pd.DataFrame)\n\n        logger.info(f\"{len(dataset)} rows (after nan removal)\")\n\n        if fname == \"lm1b-transformer-2048\":\n            # Some train costs go obsenly high for no reason, we drop these rows\n            dataset = dataset[dataset[\"train_cost\"] &lt; 10_000]  # type: ignore\n\n        elif fname == \"uniref50-transformer-128\":\n            # Some train costs go obsenly high for no reason, we drop these rows\n            # Almost all are below 400 but we add a buffer\n            dataset = dataset[dataset[\"train_cost\"] &lt; 4_000]  # type: ignore\n\n        elif fname == \"imagenet-resnet-512\":\n            # We drop all configs that exceed the 0.95 quantile in their max train_cost\n            # as we consider this to be a diverging config. The surrogate will smooth\n            # out these configs as it is not aware of divergence\n            # NOTE: q95 was experimentally determined so as to not remove too many\n            # configs but remove configs which would create massive gaps in \"train_cost\"\n            # which would cause optimization of the surrogate to focus too much on\n            # minimizing it's loss for outliers\n            hp_names = [\"lr_decay_factor\", \"lr_initial\", \"lr_power\", \"opt_momentum\"]\n            maxes = [\n                v[\"train_cost\"].max()  # type: ignore\n                for _, v in dataset.groupby(hp_names)\n            ]\n            q95 = np.quantile(maxes, 0.95)\n            configs_who_dont_exceed_q95 = (\n                v\n                for _, v in dataset.groupby(hp_names)\n                if v[\"train_cost\"].max() &lt; q95  # type: ignore\n            )\n            dataset = pd.concat(configs_who_dont_exceed_q95, axis=0)\n\n        elif fname == \"cifar100-wide_resnet-2048\":\n            # We drop all configs that exceed the 0.95 quantile in their max train_cost\n            # as we consider this to be a diverging config. The surrogate will smooth\n            # out these configs as it is not aware of divergence\n            # NOTE: q93 was experimentally determined so as to not remove too many\n            # configs but remove configs which would create massive gaps in\n            # \"train_cost\" which would cause optimization of the surrogate to\n            # focus too much on minimizing it's loss for outliers\n            hp_names = [\"lr_decay_factor\", \"lr_initial\", \"lr_power\", \"opt_momentum\"]\n            maxes = [\n                v[\"train_cost\"].max()  # type: ignore\n                for _, v in dataset.groupby(hp_names)\n            ]\n            q93 = np.quantile(maxes, 0.93)\n            configs_who_dont_exceed_q93 = (\n                v\n                for _, v in dataset.groupby(hp_names)\n                if v[\"train_cost\"].max() &lt; q93  # type: ignore\n            )\n            dataset = pd.concat(configs_who_dont_exceed_q93, axis=0)\n\n        # We want to write the full mixed {phase,matched} for surrogate training while\n        # only keeping matched phase 1 data for tabular.\n        # We also no longer need to keep dataset, model and batchsize for individual\n        # datasets.\n        # We can also drop \"activate_fn\" for all but 4 datasets\n        has_activation_fn = [\n            \"fashion_mnist-max_pooling_cnn-256\",\n            \"fashion_mnist-max_pooling_cnn-2048\",\n            \"mnist-max_pooling_cnn-256\",\n            \"mnist-max_pooling_cnn-2048\",\n        ]\n        drop_columns = [\"dataset\", \"model\", \"batch_size\"]\n        if fname not in has_activation_fn:\n            drop_columns += [\"activation_fn\"]\n\n        dataset = dataset.drop(columns=drop_columns)  # type: ignore\n\n        # Select only the tabular part (matched and phase1)\n        \"\"\"\n        tabular_path = datadir / f\"{fname}_tabular.csv\"\n        tabular_mask = dataset[\"matched\"] &amp; (dataset[\"phase\"] == 1)\n        df_tabular = dataset[tabular_mask]\n        df_tabular = df_tabular.drop(columns=[\"matched\", \"phase\"])\n\n        print(f\"Writing tabular data to {tabular_path}\")\n        df_tabular.to_csv(tabular_path, index=False)\n        \"\"\"\n\n        # There are some entries which seem to appear twice. This is due to the same\n        # config in {phase0,phase1} x {matched, unmatched}\n        # To prevent issues, we simply drop duplicates\n        hps = [\"lr_decay_factor\", \"lr_initial\", \"lr_power\", \"opt_momentum\"]\n        hps = [*hps, \"activation_fn\"] if fname in has_activation_fn else list(hps)\n\n        dataset = dataset.drop_duplicates([*hps, \"epoch\"], keep=\"last\")  # type: ignore\n\n        # The rest can be used for surrogate training\n        surrogate_path = datadir / f\"{fname}_surrogate.csv\"\n        df_surrogate = dataset.drop(columns=[\"matched\", \"phase\"])\n        df_surrogate.to_csv(surrogate_path, index=False)\n</code></pre>"},{"location":"api/mfpbench/pd1/surrogate/train_xgboost/","title":"train_xgboost","text":""},{"location":"api/mfpbench/pd1/surrogate/train_xgboost/#mfpbench.pd1.surrogate.train_xgboost.train_xgboost","title":"<code>def train_xgboost(config, budget, X, y, seed=None)</code>","text":"<p>Train an XGBoost model on the given data.</p> PARAMETER  DESCRIPTION <code>config</code> <p>The configuration to use for the XGBoost model</p> <p> TYPE: <code>Configuration</code> </p> <code>budget</code> <p>The number of estimators to use for the XGBoost model</p> <p> TYPE: <code>int</code> </p> <code>X</code> <p>The data to train on</p> <p> TYPE: <code>DataFrame</code> </p> <code>y</code> <p>The target to train on</p> <p> TYPE: <code>Series</code> </p> <code>seed</code> <p>The seed to use for the XGBoost model</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>XGBRegressor</code> <p>The trained XGBoost model</p> Source code in <code>src/mfpbench/pd1/surrogate/train_xgboost.py</code> <pre><code>def train_xgboost(\n    config: Configuration,\n    budget: int,\n    X: pd.DataFrame,\n    y: pd.Series,\n    seed: int | None = None,\n) -&gt; XGBRegressor:\n    \"\"\"Train an XGBoost model on the given data.\n\n    Args:\n        config: The configuration to use for the XGBoost model\n        budget: The number of estimators to use for the XGBoost model\n        X: The data to train on\n        y: The target to train on\n        seed: The seed to use for the XGBoost model\n\n    Returns:\n        The trained XGBoost model\n    \"\"\"\n    from xgboost import XGBRegressor\n\n    if y.name == \"train_cost\":\n        model = XGBRegressor(\n            **config,\n            seed=seed,\n            n_estimators=budget,\n            monotone_constraints={\"epoch\": 1},\n        )\n    else:\n        model = XGBRegressor(**config, seed=seed, n_estimators=budget)\n\n    model.fit(X, y)\n\n    return model  # type: ignore\n</code></pre>"},{"location":"api/mfpbench/pd1/surrogate/training/","title":"training","text":""},{"location":"api/mfpbench/pd1/surrogate/training/#mfpbench.pd1.surrogate.training.dehb_target_function","title":"<code>def dehb_target_function(config, budget, X, y, seed=None, default_budget=MAX_ESTIMATORS, cv=5, scoring=('r2'))</code>","text":"<p>Target function to run while training an xgboost model.</p> PARAMETER  DESCRIPTION <code>config</code> <p>The configuration to use for the XGBoost model</p> <p> TYPE: <code>Configuration</code> </p> <code>budget</code> <p>The number of estimators to use for the XGBoost model</p> <p> TYPE: <code>int | float | None</code> </p> <code>X</code> <p>The data to train on</p> <p> TYPE: <code>DataFrame</code> </p> <code>y</code> <p>The target to train on</p> <p> TYPE: <code>Series</code> </p> <code>seed</code> <p>The seed to use for the XGBoost model</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>default_budget</code> <p>The default budget to use if budget is None</p> <p> TYPE: <code>int</code> DEFAULT: <code>MAX_ESTIMATORS</code> </p> <code>cv</code> <p>The number of folds to use for cross validation</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>scoring</code> <p>The scoring metrics to use for cross validation</p> <p> TYPE: <code>tuple[str]</code> DEFAULT: <code>('r2')</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The result of the target function</p> Source code in <code>src/mfpbench/pd1/surrogate/training.py</code> <pre><code>def dehb_target_function(\n    config: Configuration,\n    budget: int | float | None,\n    X: pd.DataFrame,\n    y: pd.Series,\n    seed: int | None = None,\n    default_budget: int = MAX_ESTIMATORS,\n    cv: int = 5,\n    scoring: tuple[str] = (\"r2\",),\n) -&gt; dict[str, Any]:\n    \"\"\"Target function to run while training an xgboost model.\n\n    Args:\n        config: The configuration to use for the XGBoost model\n        budget: The number of estimators to use for the XGBoost model\n        X: The data to train on\n        y: The target to train on\n        seed: The seed to use for the XGBoost model\n        default_budget: The default budget to use if budget is None\n        cv: The number of folds to use for cross validation\n        scoring: The scoring metrics to use for cross validation\n\n    Returns:\n        The result of the target function\n    \"\"\"\n    start = time.time()\n\n    # Not sure if this is really needed but it's in example code for dehb\n    budget = default_budget if budget is None else int(budget)\n\n    if y.name == \"train_cost\":\n        model = XGBRegressor(\n            **config,\n            seed=seed,\n            n_estimators=budget,\n            monotone_constraints={\"epoch\": 1},\n            n_jobs=1,\n        )\n    else:\n        model = XGBRegressor(**config, seed=seed, n_estimators=budget, n_jobs=1)\n\n    scores = cross_validate(\n        estimator=model,\n        X=X,\n        y=y,\n        cv=KFold(shuffle=True, random_state=seed, n_splits=cv),\n        scoring=scoring,\n        return_train_score=True,\n    )\n\n    primary_eval_metric = scoring[0]\n    primary = np.mean(scores[f\"test_{primary_eval_metric}\"])\n\n    cost = time.time() - start\n    for k, v in scores.items():\n        scores[k] = list(v) if isinstance(v, np.ndarray) else v\n\n    return {\n        \"fitness\": -primary,  # DEHB minimized\n        \"cost\": cost,\n        \"info\": {\n            \"score\": primary,\n            \"cv_scores\": scores,\n            \"budget\": budget,\n            \"config\": dict(config),\n        },\n    }\n</code></pre>"},{"location":"api/mfpbench/pd1/surrogate/training/#mfpbench.pd1.surrogate.training.find_xgboost_surrogate","title":"<code>def find_xgboost_surrogate(X, y, *, cv=5, seed=None, opt_time=30.0, output_path=None, n_workers=1)</code>","text":"<p>Find the best XGBoost surrogate for the given data.</p> PARAMETER  DESCRIPTION <code>X</code> <p>The data to train on</p> <p> TYPE: <code>DataFrame</code> </p> <code>y</code> <p>The target to train on</p> <p> TYPE: <code>Series</code> </p> <code>cv</code> <p>The number of folds to use for cross validation</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>seed</code> <p>The seed to use for the XGBoost model</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>opt_time</code> <p>The time to spend optimizing the surrogate</p> <p> TYPE: <code>float</code> DEFAULT: <code>30.0</code> </p> <code>output_path</code> <p>The path to save the results to</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The number of workers to use for DEHB</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>XGBRegressor</code> <p>The trained XGBoost model</p> Source code in <code>src/mfpbench/pd1/surrogate/training.py</code> <pre><code>def find_xgboost_surrogate(\n    X: pd.DataFrame,\n    y: pd.Series,\n    *,\n    cv: int = 5,\n    seed: int | None = None,\n    opt_time: float = 30.0,\n    output_path: Path | None = None,\n    n_workers: int = 1,\n) -&gt; XGBRegressor:\n    \"\"\"Find the best XGBoost surrogate for the given data.\n\n    Args:\n        X: The data to train on\n        y: The target to train on\n        cv: The number of folds to use for cross validation\n        seed: The seed to use for the XGBoost model\n        opt_time: The time to spend optimizing the surrogate\n        output_path: The path to save the results to\n        n_workers: The number of workers to use for DEHB\n\n    Returns:\n        The trained XGBoost model\n    \"\"\"\n    cs = space(seed=seed)\n    if output_path is None:\n        timestamp = datetime.isoformat(datetime.now())\n        output_path = Path(f\"surrogate-opt-{y.name}-{timestamp}\")\n\n    if not output_path.exists():\n        output_path.mkdir(exist_ok=True)\n\n    dehb_path = output_path / \"dehb\"\n    if not dehb_path.exists():\n        dehb_path.mkdir(exist_ok=True)\n\n    dehb = DEHB(\n        f=dehb_target_function,\n        cs=cs,\n        dimensions=len(cs.get_hyperparameters()),\n        min_budget=MIN_ESTIMATORS,\n        max_budget=MAX_ESTIMATORS,\n        n_workers=n_workers,\n        output_path=str(dehb_path),\n    )\n\n    _, _, hist = dehb.run(\n        total_cost=opt_time,\n        verbose=True,\n        save_intermediate=False,\n        # kwargs\n        X=X,\n        y=y,\n        cv=cv,\n    )\n\n    # Now we find the one with the highest test_score and use that for\n    # training our final model\n    infos = [info for *_, info in hist]\n    best = max(infos, key=lambda i: i[\"score\"])\n\n    # Write out the info\n    info_path = output_path / \"info.json\"\n    best_path = output_path / \"best.json\"\n    with info_path.open(\"w\") as f:\n        json.dump(infos, f)\n\n    with best_path.open(\"w\") as f:\n        json.dump(best, f)\n\n    best_config = best[\"config\"]\n    best_budget = best[\"budget\"]\n\n    # Train\n    model = XGBRegressor(**best_config, seed=seed, n_estimators=best_budget)\n    model.fit(X, y)\n    return model  # type: ignore\n</code></pre>"},{"location":"api/mfpbench/pd1/surrogate/xgboost_space/","title":"xgboost_space","text":""},{"location":"api/mfpbench/pd1/surrogate/xgboost_space/#mfpbench.pd1.surrogate.xgboost_space.space","title":"<code>def space(seed)</code>","text":"<p>Space for the xgboost surrogate.</p> Source code in <code>src/mfpbench/pd1/surrogate/xgboost_space.py</code> <pre><code>def space(seed: int | None) -&gt; ConfigurationSpace:\n    \"\"\"Space for the xgboost surrogate.\"\"\"\n    cs = ConfigurationSpace(seed=seed)\n\n    cs.add_hyperparameters(\n        [\n            UniformFloatHyperparameter(\n                \"learning_rate\",\n                lower=0.001,\n                upper=1.0,\n                default_value=0.3,\n                log=True,\n            ),  # learning rate\n            UniformIntegerHyperparameter(\n                \"max_depth\",\n                lower=3,\n                upper=20,\n                default_value=10,\n                log=True,\n            ),\n            UniformFloatHyperparameter(\n                \"colsample_bytree\",\n                lower=0.3,\n                upper=1.0,\n                default_value=1.0,\n                log=True,\n            ),\n            UniformFloatHyperparameter(\n                \"reg_lambda\",\n                lower=1e-3,\n                upper=10.0,\n                default_value=1,\n                log=True,\n            ),\n            UniformFloatHyperparameter(\n                \"subsample\",\n                lower=0.4,\n                upper=1.0,\n                default_value=1.0,\n                log=False,\n            ),\n            UniformFloatHyperparameter(\n                \"alpha\",\n                lower=1e-3,\n                upper=10,\n                default_value=1,\n                log=True,\n            ),\n            UniformIntegerHyperparameter(\n                \"min_child_weight\",\n                lower=1,\n                upper=300,\n                default_value=100,\n            ),\n        ],\n    )\n    return cs\n</code></pre>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/","title":"benchmark","text":"<p>The hartmann benchmarks.</p> <p>The presets of terrible, bad, moderate and good are empirically obtained hyperparameters for the hartmann function</p> <p>The function flattens with increasing fidelity bias. Along with increasing noise, that obviously makes one config harder to distinguish from another. Moreover, this works with any number of fidelitiy levels.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmann3Config","title":"<code>class MFHartmann3Config</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Config</code></p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmann3Config.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/synthetic/hartmann/benchmark.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert 0.0 &lt;= self.X_0 &lt;= 1.0\n    assert 0.0 &lt;= self.X_1 &lt;= 1.0\n    assert 0.0 &lt;= self.X_2 &lt;= 1.0\n</code></pre>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmann6Config","title":"<code>class MFHartmann6Config</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Config</code></p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmann6Config.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/synthetic/hartmann/benchmark.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert 0.0 &lt;= self.X_0 &lt;= 1.0\n    assert 0.0 &lt;= self.X_1 &lt;= 1.0\n    assert 0.0 &lt;= self.X_2 &lt;= 1.0\n    assert 0.0 &lt;= self.X_3 &lt;= 1.0\n    assert 0.0 &lt;= self.X_4 &lt;= 1.0\n    assert 0.0 &lt;= self.X_5 &lt;= 1.0\n</code></pre>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannResult","title":"<code>class MFHartmannResult</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Result[C, int]</code></p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannResult.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannResult.error","title":"<code>error: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannResult.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>Just returns the score.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannResult.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>Just returns the error.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannResult.val_score","title":"<code>val_score: float</code>   <code>prop</code>","text":"<p>Just returns the score.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannResult.val_error","title":"<code>val_error: float</code>   <code>prop</code>","text":"<p>Just returns the error.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannResult.cost","title":"<code>cost: float</code>   <code>prop</code>","text":"<p>Just retuns the fidelity.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannBenchmark","title":"<code>class MFHartmannBenchmark(*, seed=None, bias=None, noise=None, prior=None, perturb_prior=None)</code>","text":"<p>         Bases: <code>Benchmark</code>, <code>Generic[G, C]</code></p> PARAMETER  DESCRIPTION <code>seed</code> <p>The seed to use.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>bias</code> <p>How much bias to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>How much noise to introduce</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark.</p> <ul> <li>if <code>Path</code> - path to a file</li> <li>if <code>Mapping</code> - Use directly</li> <li>if <code>None</code> - There is no prior</li> </ul> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If not None, will perturb the prior by this amount. For numericals, while for categoricals, this is interpreted as the probability of swapping the value for a random one.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/synthetic/hartmann/benchmark.py</code> <pre><code>def __init__(\n    self,\n    *,\n    seed: int | None = None,\n    bias: float | None = None,\n    noise: float | None = None,\n    prior: str | Path | C | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n):\n    \"\"\"Initialize the benchmark.\n\n    Args:\n        seed: The seed to use.\n        bias: How much bias to introduce\n        noise: How much noise to introduce\n        prior: The prior to use for the benchmark.\n\n            * if `Path` - path to a file\n            * if `Mapping` - Use directly\n            * if `None` - There is no prior\n\n        perturb_prior: If not None, will perturb the prior by this amount.\n            For numericals, while for categoricals, this is interpreted as\n            the probability of swapping the value for a random one.\n    \"\"\"\n    cls = self.__class__\n    self.bias = bias if bias is not None else cls.mfh_bias_noise[0]\n    self.noise = noise if noise is not None else cls.mfh_bias_noise[1]\n    self.mfh = cls.Generator(\n        n_fidelities=cls.fidelity_range[1],\n        fidelity_noise=self.noise,\n        fidelity_bias=self.bias,\n        seed=seed,\n    )\n\n    name = (\n        f\"mfh{cls.mfh_dims}_{cls.mfh_suffix}\"\n        if cls.mfh_suffix != \"\"\n        else f\"mfh{cls.mfh_dims}\"\n    )\n    space = ConfigurationSpace(name=name, seed=seed)\n    space.add_hyperparameters(\n        [\n            UniformFloatHyperparameter(f\"X_{i}\", lower=0.0, upper=1.0)\n            for i in range(cls.mfh_dims)\n        ],\n    )\n    super().__init__(\n        name=name,\n        space=space,\n        seed=seed,\n        prior=prior,\n        perturb_prior=perturb_prior,\n    )\n</code></pre>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannBenchmark.mfh_dims","title":"<code>mfh_dims: int</code>   <code>classvar</code>","text":"<p>How many dimensions there are to the Hartmann function.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannBenchmark.mfh_suffix","title":"<code>mfh_suffix: str</code>   <code>classvar</code>","text":"<p>Suffix for the benchmark name</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannBenchmark.Config","title":"<code>Config: type[C]</code>   <code>attr</code>","text":"<p>The Config type for this mfhartmann benchmark.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannBenchmark.Generator","title":"<code>Generator: type[G]</code>   <code>attr</code>","text":"<p>The underlying mfhartmann function generator.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannBenchmark.mfh_bias_noise","title":"<code>mfh_bias_noise: tuple[float, float]</code>   <code>classvar</code>","text":"<p>The default bias and noise for mfhartmann benchmarks.</p>"},{"location":"api/mfpbench/synthetic/hartmann/benchmark/#mfpbench.synthetic.hartmann.benchmark.MFHartmannBenchmark.optimum","title":"<code>optimum: C</code>   <code>prop</code>","text":"<p>The optimum of the benchmark.</p>"},{"location":"api/mfpbench/synthetic/hartmann/generators/","title":"generators","text":"<p>Extends Hartmann functions to incorporate fidelities.</p>"},{"location":"api/mfpbench/synthetic/hartmann/generators/#mfpbench.synthetic.hartmann.generators.MFHartmannGenerator","title":"<code>class MFHartmannGenerator(n_fidelities, fidelity_bias, fidelity_noise, seed=None)</code>","text":"<p>         Bases: <code>ABC</code></p> <p>A multifidelity version of the Hartmann3 function.</p> <p>Carried a bias term, which flattens the objective, and a noise term. The impact of both terms decrease with increasing fidelity, meaning that <code>num_fidelities</code> is the best fidelity. This fidelity level also constitutes a noiseless, true evaluation of the Hartmann function.</p> PARAMETER  DESCRIPTION <code>n_fidelities</code> <p>The fidelity at which the function is evalated.</p> <p> TYPE: <code>int</code> </p> <code>fidelity_bias</code> <p>Amount of bias, realized as a flattening of the objective.</p> <p> TYPE: <code>float</code> </p> <code>fidelity_noise</code> <p>Amount of noise, decreasing linearly (in st.dev.) with fidelity.</p> <p> TYPE: <code>float</code> </p> <code>seed</code> <p>The seed to use for the noise.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/synthetic/hartmann/generators.py</code> <pre><code>def __init__(\n    self,\n    n_fidelities: int,\n    fidelity_bias: float,\n    fidelity_noise: float,\n    seed: int | None = None,\n):\n    \"\"\"Initialize the generator.\n\n    Args:\n        n_fidelities: The fidelity at which the function is evalated.\n        fidelity_bias: Amount of bias, realized as a flattening of the objective.\n        fidelity_noise: Amount of noise, decreasing linearly (in st.dev.) with\n            fidelity.\n        seed: The seed to use for the noise.\n    \"\"\"\n    self.z_min, self.z_max = (1, n_fidelities)\n    self.seed = seed if seed else self._default_seed\n    self.bias = fidelity_bias\n    self.noise = fidelity_noise\n    self.random_state = np.random.default_rng(seed)\n</code></pre>"},{"location":"api/mfpbench/synthetic/hartmann/generators/#mfpbench.synthetic.hartmann.generators.MFHartmannGenerator.__call__","title":"<code>def __call__(z, Xs)</code>   <code>abstractmethod</code>","text":"<p>Evaluate the function at the given fidelity and points.</p> PARAMETER  DESCRIPTION <code>z</code> <p>The fidelity at which to query.</p> <p> TYPE: <code>int</code> </p> <code>Xs</code> <p>The Xs as input to the function, in the correct order</p> <p> TYPE: <code>tuple[float, ...]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Value at that position</p> Source code in <code>src/mfpbench/synthetic/hartmann/generators.py</code> <pre><code>@abstractmethod\ndef __call__(self, z: int, Xs: tuple[float, ...]) -&gt; float:\n    \"\"\"Evaluate the function at the given fidelity and points.\n\n    Args:\n        z: The fidelity at which to query.\n        Xs: The Xs as input to the function, in the correct order\n\n    Returns:\n        Value at that position\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/mfpbench/synthetic/hartmann/generators/#mfpbench.synthetic.hartmann.generators.MFHartmann3","title":"<code>class MFHartmann3</code>","text":"<p>         Bases: <code>MFHartmannGenerator</code></p>"},{"location":"api/mfpbench/synthetic/hartmann/generators/#mfpbench.synthetic.hartmann.generators.MFHartmann3.__call__","title":"<code>def __call__(z, Xs)</code>","text":"<p>Evaluate the function at the given fidelity and points.</p> PARAMETER  DESCRIPTION <code>z</code> <p>The fidelity.</p> <p> TYPE: <code>int</code> </p> <code>Xs</code> <p>Parameters of the function.</p> <p> TYPE: <code>tuple[float, ...]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The function value</p> Source code in <code>src/mfpbench/synthetic/hartmann/generators.py</code> <pre><code>def __call__(self, z: int, Xs: tuple[float, ...]) -&gt; float:\n    \"\"\"Evaluate the function at the given fidelity and points.\n\n    Args:\n        z: The fidelity.\n        Xs: Parameters of the function.\n\n    Returns:\n        The function value\n    \"\"\"\n    assert len(Xs) == self.dims\n    X_0, X_1, X_2 = Xs\n\n    log_z = np.log(z)\n    log_lb, log_ub = np.log(self.z_min), np.log(self.z_max)\n    log_z_scaled = (log_z - log_lb) / (log_ub - log_lb)\n\n    # Highest fidelity (1) accounts for the regular Hartmann\n    X = np.array([X_0, X_1, X_2]).reshape(1, -1)\n    alpha = np.array([1.0, 1.2, 3.0, 3.2])\n\n    alpha_prime = alpha - self.bias * np.power(1 - log_z_scaled, 1)\n    A: np.ndarray = np.array(\n        [[3.0, 10, 30], [0.1, 10, 35], [3.0, 10, 30], [0.1, 10, 35]],\n        dtype=float,\n    )\n    P: np.ndarray = np.array(\n        [\n            [3689, 1170, 2673],\n            [4699, 4387, 7470],\n            [1091, 8732, 5547],\n            [381, 5743, 8828],\n        ],\n        dtype=float,\n    )\n\n    inner_sum = np.sum(A * (X[:, np.newaxis, :] - 0.0001 * P) ** 2, axis=-1)\n    H = -(np.sum(alpha_prime * np.exp(-inner_sum), axis=-1))\n\n    # TODO: Didn't seem used\n    # H_true = -(np.sum(alpha * np.exp(-inner_sum), axis=-1))\n\n    # and add some noise\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")  # Seed below will overflow\n        rng = np.random.default_rng(seed=abs(self.seed * z * hash(Xs)))\n\n    noise = np.abs(rng.normal(size=H.size)) * self.noise * (1 - log_z_scaled)\n\n    return float((H + noise)[0])\n</code></pre>"},{"location":"api/mfpbench/synthetic/hartmann/generators/#mfpbench.synthetic.hartmann.generators.MFHartmann6","title":"<code>class MFHartmann6</code>","text":"<p>         Bases: <code>MFHartmannGenerator</code></p>"},{"location":"api/mfpbench/synthetic/hartmann/generators/#mfpbench.synthetic.hartmann.generators.MFHartmann6.__call__","title":"<code>def __call__(z, Xs)</code>","text":"<p>Evaluate the function at the given fidelity and points.</p> PARAMETER  DESCRIPTION <code>z</code> <p>The fidelity it's evaluated at.</p> <p> TYPE: <code>int</code> </p> <code>Xs</code> <p>Parameters of the function</p> <p> TYPE: <code>tuple[float, ...]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The function value</p> Source code in <code>src/mfpbench/synthetic/hartmann/generators.py</code> <pre><code>def __call__(self, z: int, Xs: tuple[float, ...]) -&gt; float:\n    \"\"\"Evaluate the function at the given fidelity and points.\n\n    Args:\n        z: The fidelity it's evaluated at.\n        Xs: Parameters of the function\n\n    Returns:\n        The function value\n    \"\"\"\n    assert len(Xs) == self.dims\n    X_0, X_1, X_2, X_3, X_4, X_5 = Xs\n\n    # Change by Carl - z now comes in normalized\n    log_z = np.log(z)\n    log_lb, log_ub = np.log(self.z_min), np.log(self.z_max)\n    log_z_scaled = (log_z - log_lb) / (log_ub - log_lb)\n\n    # Highest fidelity (1) accounts for the regular Hartmann\n    X = np.array([X_0, X_1, X_2, X_3, X_4, X_5]).reshape(1, -1)\n    alpha = np.array([1.0, 1.2, 3.0, 3.2])\n    alpha_prime = alpha - self.bias * np.power(1 - log_z_scaled, 1)\n    A: np.ndarray = np.array(\n        [\n            [10, 3, 17, 3.5, 1.7, 8],\n            [0.05, 10, 17, 0.1, 8, 14],\n            [3, 3.5, 1.7, 10, 17, 8],\n            [17, 8, 0.05, 10, 0.1, 14],\n        ],\n        dtype=float,\n    )\n    P: np.ndarray = np.array(\n        [\n            [1312, 1696, 5569, 124, 8283, 5886],\n            [2329, 4135, 8307, 3736, 1004, 9991],\n            [2348, 1451, 3522, 2883, 3047, 6650],\n            [4047, 8828, 8732, 5743, 1091, 381],\n        ],\n        dtype=float,\n    )\n\n    inner_sum = np.sum(A * (X[:, np.newaxis, :] - 0.0001 * P) ** 2, axis=-1)\n    H = -(np.sum(alpha_prime * np.exp(-inner_sum), axis=-1))\n\n    # TODO: Doesn't seem to be used?\n    # H_true = -(np.sum(alpha * np.exp(-inner_sum), axis=-1))\n\n    # and add some noise\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")  # Seed below will overflow\n        rng = np.random.default_rng(seed=abs(self.seed * z * hash(Xs)))\n\n    noise = np.abs(rng.normal(size=H.size)) * self.noise * (1 - log_z_scaled)\n    return float((H + noise)[0])\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmark/","title":"benchmark","text":""},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark","title":"<code>class YAHPOBenchmark(task_id, *, datadir=None, seed=None, prior=None, perturb_prior=None, session=None)</code>","text":"<p>         Bases: <code>Benchmark[C, R, F]</code></p> PARAMETER  DESCRIPTION <code>task_id</code> <p>The task id to choose.</p> <p> TYPE: <code>str</code> </p> <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>datadir</code> <p>The path to where mfpbench stores it data. If left to <code>None</code>, will use the <code>_default_download_dir = ./data/yahpo-gym-data</code>.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed for the benchmark instance</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prior</code> <p>The prior to use for the benchmark. If None, no prior is used. If a str, will check the local location first for a prior specific for this benchmark, otherwise assumes it to be a Path. If a Path, will load the prior from the path. If a Mapping, will be used directly.</p> <p> TYPE: <code>str | Path | C | Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>perturb_prior</code> <p>If given, will perturb the prior by this amount. Only used if <code>prior=</code> is given as a config.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>session</code> <p>The onnxruntime session to use. If None, will create a new one.</p> <p>Not for faint hearted</p> <p>This is only a backdoor for onnx compatibility issues with YahpoGym. You are advised not to use this unless you know what you are doing.</p> <p> TYPE: <code>InferenceSession | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/mfpbench/yahpo/benchmark.py</code> <pre><code>def __init__(  # noqa: C901, PLR0912\n    self,\n    task_id: str,\n    *,\n    datadir: str | Path | None = None,\n    seed: int | None = None,\n    prior: str | Path | C | Mapping[str, Any] | None = None,\n    perturb_prior: float | None = None,\n    session: onnxruntime.InferenceSession | None = None,\n):\n    \"\"\"Initialize a Yahpo Benchmark.\n\n    Args:\n        task_id: The task id to choose.\n        seed: The seed to use\n        datadir: The path to where mfpbench stores it data. If left to `None`,\n            will use the `_default_download_dir = ./data/yahpo-gym-data`.\n        seed: The seed for the benchmark instance\n        prior: The prior to use for the benchmark. If None, no prior is used.\n            If a str, will check the local location first for a prior\n            specific for this benchmark, otherwise assumes it to be a Path.\n            If a Path, will load the prior from the path.\n            If a Mapping, will be used directly.\n        perturb_prior: If given, will perturb the prior by this amount. Only used if\n            `prior=` is given as a config.\n        session: The onnxruntime session to use. If None, will create a new one.\n\n            !!! warning \"Not for faint hearted\"\n\n                This is only a backdoor for onnx compatibility issues with YahpoGym.\n                You are advised not to use this unless you know what you are doing.\n    \"\"\"\n    # Validation\n    cls = self.__class__\n\n    # These errors are maintainers errors, not user errors\n    if cls.yahpo_forced_remove_hps is not None and cls.has_conditionals:\n        raise NotImplementedError(\n            \"Error setting up a YAHPO Benchmark with conditionals\",\n            \" and forced hps\",\n        )\n\n    if cls.yahpo_task_id_name is not None and cls.has_conditionals:\n        raise NotImplementedError(\n            f\"{self.name} has conditionals, can't remove task_id from space\",\n        )\n\n    instances = cls.yahpo_instances\n    if task_id is None and instances is not None:\n        raise ValueError(f\"{cls} requires a task in {instances}\")\n    if task_id is not None and instances is None:\n        raise ValueError(f\"{cls} has no instances, you passed {task_id}\")\n    if task_id is not None and instances and task_id not in instances:\n        raise ValueError(f\"{cls} requires a task in {instances}\")\n\n    if datadir is None:\n        datadir = YAHPOSource.default_location()\n    elif isinstance(datadir, str):\n        datadir = Path(datadir)\n\n    datadir = Path(datadir) if isinstance(datadir, str) else datadir\n    if not datadir.exists():\n        raise FileNotFoundError(\n            f\"Can't find folder at {datadir}, have you run\\n\"\n            f\"`python -m mfpbench download --status --data-dir {datadir.parent}`\",\n        )\n    _ensure_yahpo_config_set(datadir)\n\n    import yahpo_gym\n\n    if session is None:\n        dummy_bench = yahpo_gym.BenchmarkSet(\n            cls.yahpo_base_benchmark_name,\n            instance=task_id,\n            multithread=False,\n            # HACK: Used to fix onnxruntime session issue with 1.16.0 where\n            # `providers` is required. By setting these options, we prevent\n            # the benchmark from automatically creating a session.\n            # We will manually do so and set it later.\n            active_session=False,\n            session=None,\n        )\n        session = _yahpo_create_session(benchmark=dummy_bench)\n\n    bench = yahpo_gym.BenchmarkSet(\n        cls.yahpo_base_benchmark_name,\n        instance=task_id,\n        multithread=False,\n        session=session,\n    )\n\n    name = f\"{cls.yahpo_base_benchmark_name}-{task_id}\"\n\n    # These can have one or two fidelities\n    # NOTE: seed is allowed to be int | None\n    space = bench.get_opt_space(\n        drop_fidelity_params=True,\n        seed=seed,  # type: ignore\n    )\n\n    if cls.yahpo_task_id_name is not None:\n        space = remove_hyperparameter(cls.yahpo_task_id_name, space)\n\n    if cls.yahpo_forced_remove_hps is not None:\n        names = space.get_hyperparameter_names()\n        for key in cls.yahpo_forced_remove_hps:\n            if key in names:\n                space = remove_hyperparameter(key, space)\n\n    self._bench = bench\n    self.datadir = datadir\n    self.task_id = task_id\n    super().__init__(\n        name=name,\n        seed=seed,\n        space=space,\n        prior=prior,\n        perturb_prior=perturb_prior,\n    )\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark.yahpo_base_benchmark_name","title":"<code>yahpo_base_benchmark_name: str</code>   <code>classvar</code>","text":"<p>Base name of the yahpo benchmark.</p>"},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark.yahpo_instances","title":"<code>yahpo_instances: tuple[str, ...] | None</code>   <code>attr</code>","text":"<p>The instances available for this benchmark, if Any.</p>"},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark.yahpo_task_id_name","title":"<code>yahpo_task_id_name: str | None</code>   <code>classvar</code>","text":"<p>Name of hp used to indicate task.</p>"},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark.yahpo_forced_remove_hps","title":"<code>yahpo_forced_remove_hps: Mapping[str, int | float | str] | None</code>   <code>attr</code>","text":"<p>Any hyperparameters that should be forcefully deleted from the space but have default values filled in</p>"},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark.yahpo_replacements_hps","title":"<code>yahpo_replacements_hps: Sequence[tuple[str, str]] | None</code>   <code>attr</code>","text":"<p>Any replacements that need to be done in hyperparameters [(dataclass_version, dict_version)]</p>"},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark.datadir","title":"<code>datadir: Path</code>   <code>attr</code>","text":"<p>The path to where the data is stored.</p>"},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark.task_id","title":"<code>task_id: str</code>   <code>attr</code>","text":"<p>The task id for this benchmark.</p>"},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark.bench","title":"<code>bench: yahpo_gym.BenchmarkSet</code>   <code>prop</code>","text":"<p>The underlying yahpo gym benchmark.</p>"},{"location":"api/mfpbench/yahpo/benchmark/#mfpbench.yahpo.benchmark.YAHPOBenchmark.load","title":"<code>def load()</code>","text":"<p>Load the benchmark into memory.</p> Source code in <code>src/mfpbench/yahpo/benchmark.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the benchmark into memory.\"\"\"\n    _ = self.bench\n</code></pre>"},{"location":"api/mfpbench/yahpo/config/","title":"config","text":""},{"location":"api/mfpbench/yahpo/result/","title":"result","text":""},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/","title":"lcbench","text":""},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchConfig","title":"<code>class LCBenchConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>YAHPOConfig</code></p> <p>A LCBench Config.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchConfig--note","title":"Note:","text":"<p>For <code>momentum</code>, the paper seems to suggest it's (0.1, 0.9) but the configspace says (0.1, 0.99), going with the code version.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this is a correct config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/lcbench.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate this is a correct config.\"\"\"\n    assert 16 &lt;= self.batch_size &lt;= 512\n    assert 1e-04 &lt;= self.learning_rate &lt;= 0.1\n    assert 0.1 &lt;= self.momentum &lt;= 0.99\n    assert 1e-05 &lt;= self.weight_decay &lt;= 0.1\n    assert 1 &lt;= self.num_layers &lt;= 5\n    assert 64 &lt;= self.max_units &lt;= 1024\n    assert 0.0 &lt;= self.max_dropout &lt;= 1.0\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchResult","title":"<code>class LCBenchResult</code>   <code>dataclass</code>","text":"<p>         Bases: <code>YAHPOResult[LCBenchConfig, int]</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchResult.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchResult.error","title":"<code>error: float</code>   <code>prop</code>","text":"<p>The error of interest.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchResult.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchResult.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchResult.val_score","title":"<code>val_score: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchResult.val_error","title":"<code>val_error: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchResult.cost","title":"<code>cost: float</code>   <code>prop</code>","text":"<p>Time taken in seconds to train the config (assumed to be seconds).</p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchBenchmark","title":"<code>class LCBenchBenchmark</code>","text":"<p>         Bases: <code>YAHPOBenchmark</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/lcbench/#mfpbench.yahpo.benchmarks.lcbench.LCBenchBenchmark.yahpo_instances","title":"<code>yahpo_instances</code>   <code>classvar</code> <code>attr</code>","text":"<pre><code>('3945', '7593', '34539', '126025', '126026', '126029', '146212', '167104', '167149', '167152', '167161', '167168', '167181', '167184', '167185', '167190', '167200', '167201', '168329', '168330', '168331', '168335', '168868', '168908', '168910', '189354', '189862', '189865', '189866', '189873', '189905', '189906', '189908', '189909')\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/","title":"nb301","text":""},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Config","title":"<code>class NB301Config</code>   <code>dataclass</code>","text":"<p>         Bases: <code>YAHPOConfig</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Config.validate","title":"<code>def validate()</code>","text":"<p>Validate this is a correct config.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Config.validate--note","title":"Note:","text":"<p>We don't check conditionals validity</p> Source code in <code>src/mfpbench/yahpo/benchmarks/nb301.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this is a correct config.\n\n    Note:\n    ----\n    We don't check conditionals validity\n    \"\"\"\n    nodes = list(range(13 + 1))\n    cells = [\"normal\", \"reduce\"]\n    for i, cell in product(nodes, cells):\n        attr_name = f\"edge_{cell}_{i}\"\n        attr = getattr(self, attr_name)\n        assert attr is None or attr in Choices, attr_name\n\n    choices_3 = [\"0_1\", \"0_2\", \"1_2\"]\n    choices_4 = [\"0_1\", \"0_2\", \"0_3\", \"1_2\", \"1_3\", \"2_3\"]\n    choices_5 = [\n        \"0_1\",\n        \"0_2\",\n        \"0_3\",\n        \"0_4\",\n        \"1_2\",\n        \"1_3\",\n        \"1_4\",\n        \"2_3\",\n        \"2_4\",\n        \"3_4\",\n    ]\n\n    nodes = list(range(3, 5 + 1))\n    for i, choices in [(3, choices_3), (4, choices_4), (5, choices_5)]:\n        normal_node = f\"inputs_node_normal_{i}\"\n        assert getattr(self, normal_node) in choices\n\n        reduce_node = f\"inputs_node_reduce_{i}\"\n        assert getattr(self, reduce_node) in choices\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Config.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create from a dict or mapping object.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/nb301.py</code> <pre><code>@classmethod\ndef from_dict(cls: type[Self], d: Mapping[str, Any]) -&gt; Self:\n    \"\"\"Create from a dict or mapping object.\"\"\"\n    # We just flatten things because it's way too big of a name\n    config = {k.replace(_hp_name_extension, \"\"): v for k, v in d.items()}\n    return cls(**config)\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Config.dict","title":"<code>def dict()</code>","text":"<p>Converts the config to a raw dictionary.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/nb301.py</code> <pre><code>def dict(self) -&gt; dict[str, Any]:\n    \"\"\"Converts the config to a raw dictionary.\"\"\"\n    return {\n        _hp_name_extension + k: v for k, v in asdict(self).items() if v is not None\n    }\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Result","title":"<code>class NB301Result</code>   <code>dataclass</code>","text":"<p>         Bases: <code>YAHPOResult[NB301Config, int]</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Result.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Result.error","title":"<code>error: float</code>   <code>prop</code>","text":"<p>The error of interest.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Result.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Result.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Result.val_score","title":"<code>val_score: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Result.val_error","title":"<code>val_error: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/nb301/#mfpbench.yahpo.benchmarks.nb301.NB301Result.cost","title":"<code>cost: float</code>   <code>prop</code>","text":"<p>Time taken in seconds to train the config.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/","title":"iaml","text":""},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLConfig","title":"<code>class IAMLConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>YAHPOConfig</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLConfig.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create from a dict or mapping object.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/iaml/iaml.py</code> <pre><code>@classmethod\ndef from_dict(cls: type[C], d: Mapping[str, Any]) -&gt; C:\n    \"\"\"Create from a dict or mapping object.\"\"\"\n    # We may have keys that are conditional and hence we need to flatten them\n    config = {k.replace(\".\", \"__\"): v for k, v in d.items()}\n    return cls(**config)\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLConfig.dict","title":"<code>def dict()</code>","text":"<p>Converts the config to a raw dictionary.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/iaml/iaml.py</code> <pre><code>def dict(self) -&gt; dict[str, Any]:\n    \"\"\"Converts the config to a raw dictionary.\"\"\"\n    d = asdict(self)\n    return {k.replace(\"__\", \".\"): v for k, v in d.items() if v is not None}\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLResult","title":"<code>class IAMLResult</code>   <code>dataclass</code>","text":"<p>         Bases: <code>YAHPOResult[C, float]</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLResult.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLResult.error","title":"<code>error: float</code>   <code>prop</code>","text":"<p>The error of interest.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLResult.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLResult.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>The error on the test set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLResult.val_score","title":"<code>val_score: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLResult.val_error","title":"<code>val_error: float</code>   <code>prop</code>","text":"<p>The error on the validation set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml/#mfpbench.yahpo.benchmarks.iaml.iaml.IAMLResult.cost","title":"<code>cost: float</code>   <code>prop</code>","text":"<p>The time taken in seconds to train the config.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_glmnet/","title":"iaml_glmnet","text":""},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_glmnet/#mfpbench.yahpo.benchmarks.iaml.iaml_glmnet.IAMLglmnetConfig","title":"<code>class IAMLglmnetConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>IAMLConfig</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_glmnet/#mfpbench.yahpo.benchmarks.iaml.iaml_glmnet.IAMLglmnetConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/iaml/iaml_glmnet.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert 0.0 &lt;= self.alpha &lt;= 1.0\n    assert 0.00010000000000000009 &lt;= self.s &lt;= 999.9999999999998\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_ranger/","title":"iaml_ranger","text":""},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_ranger/#mfpbench.yahpo.benchmarks.iaml.iaml_ranger.IAMLrangerConfig","title":"<code>class IAMLrangerConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>IAMLConfig</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_ranger/#mfpbench.yahpo.benchmarks.iaml.iaml_ranger.IAMLrangerConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/iaml/iaml_ranger.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert 1 &lt;= self.min__node__size &lt;= 100\n    assert 0 &lt;= self.mtry__power &lt;= 1\n    assert 1 &lt;= self.num__trees &lt;= 2000\n    assert self.respect__unordered__factors in [\n        \"ignore\",\n        \"order\",\n        \"partition\",\n    ]\n    assert 0.1 &lt;= self.sample__fraction &lt;= 1.0\n    assert self.splitrule in [\"gini\", \"extratrees\"]\n\n    if self.num__random__splits is not None:\n        assert self.splitrule == \"extratrees\"\n        assert 1 &lt;= self.num__random__splits &lt;= 100\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_rpart/","title":"iaml_rpart","text":""},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_rpart/#mfpbench.yahpo.benchmarks.iaml.iaml_rpart.IAMLrpartConfig","title":"<code>class IAMLrpartConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>IAMLConfig</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_rpart/#mfpbench.yahpo.benchmarks.iaml.iaml_rpart.IAMLrpartConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/iaml/iaml_rpart.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert 0.00010000000000000009 &lt;= self.cp &lt;= 1.0\n    assert 1 &lt;= self.maxdepth &lt;= 30\n    assert 1 &lt;= self.minbucket &lt;= 100\n    assert 1 &lt;= self.minsplit &lt;= 100\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_super/","title":"iaml_super","text":""},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_super/#mfpbench.yahpo.benchmarks.iaml.iaml_super.IAMLSuperConfig","title":"<code>class IAMLSuperConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>IAMLConfig</code></p> <p>Config has conditionals and as such, we use None to indicate not set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_super/#mfpbench.yahpo.benchmarks.iaml.iaml_super.IAMLSuperConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/iaml/iaml_super.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:  # noqa: C901, PLR0915, PLR0912\n    \"\"\"Validate this config.\"\"\"\n    assert self.learner_id in [\"glmnet\", \"ranger\", \"rpart\", \"xgboost\"]\n\n    # We do some conditional checking here\n    learner = self.learner_id\n\n    # We filter out all attributes except for those that must always be contained\n    # or are the selected learner, ...\n    attrs = [\n        attr\n        for attr in dir(self)\n        if not attr.startswith(\"__\")\n        or not attr.startswith(learner)\n        or attr in [\"learner_id\"]\n    ]\n\n    # ... the remaining must always have None set then\n    for attr in attrs:\n        assert attr is None\n\n    if learner == \"glmnet\":\n        assert self.glmnet__alpha is not None\n        assert self.glmnet__s is not None\n        assert 0.0 &lt;= self.glmnet__alpha &lt;= 1.0\n        assert 0.00010000000000000009 &lt;= self.glmnet__s &lt;= 999.9999999999998\n\n    elif learner == \"rpart\":\n        assert self.rpart__cp is not None\n        assert self.rpart__maxdepth is not None\n        assert self.rpart__minbucket is not None\n        assert self.rpart__minsplit is not None\n        assert 0.00010000000000000009 &lt;= self.rpart__cp &lt;= 1.0\n        assert 1 &lt;= self.rpart__maxdepth &lt;= 30\n        assert 1 &lt;= self.rpart__minbucket &lt;= 100\n        assert 1 &lt;= self.rpart__minsplit &lt;= 100\n\n    elif learner == \"ranger\":\n        assert self.ranger__min__node__size is not None\n        assert self.ranger__mtry__power is not None\n        assert self.ranger__num__trees is not None\n        assert self.ranger__respect__unordered__factors is not None\n        assert self.ranger__sample__fraction is not None\n        assert 1 &lt;= self.ranger__min__node__size &lt;= 100\n        assert 0 &lt;= self.ranger__mtry__power &lt;= 1\n        assert 1 &lt;= self.ranger__num__trees &lt;= 2000\n        assert self.ranger__respect__unordered__factors in [\n            \"ignore\",\n            \"order\",\n            \"partition\",\n        ]\n        assert 0.1 &lt;= self.ranger__sample__fraction &lt;= 1.0\n        assert self.ranger__splitrule in [\"gini\", \"extratrees\"]\n\n        if self.ranger__num__random__splits is not None:\n            assert self.ranger__splitrule == \"extratrees\"\n            assert 1 &lt;= self.ranger__num__random__splits &lt;= 100\n\n    elif learner == \"xgboost\":\n        assert self.xgboost__alpha is not None\n        assert self.xgboost__lambda is not None\n        assert self.xgboost__nrounds is not None\n        assert self.xgboost__subsample is not None\n        assert self.xgboost__booster in [\"gblinear\", \"gbtree\", \"dart\"]\n        assert 0.00010000000000000009 &lt;= self.xgboost__alpha &lt;= 999.9999999999998\n        assert 0.00010000000000000009 &lt;= self.xgboost__lambda &lt;= 999.9999999999998\n        assert 7 &lt;= self.xgboost__nrounds &lt;= 2981\n        assert 0.1 &lt;= self.xgboost__subsample &lt;= 1.0\n\n        if self.xgboost__colsample_bylevel is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert 0.01 &lt;= self.xgboost__colsample_bylevel &lt;= 1.0\n\n        if self.xgboost__colsample_bytree is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert 0.01 &lt;= self.xgboost__colsample_bytree &lt;= 1.0\n\n        if self.xgboost__eta is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert 0.00010000000000000009 &lt;= self.xgboost__eta &lt;= 1.0\n\n        if self.xgboost__gamma is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert (\n                0.00010000000000000009 &lt;= self.xgboost__gamma &lt;= 6.999999999999999\n            )\n\n        if self.xgboost__max_depth is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert 1 &lt;= self.xgboost__max_depth &lt;= 15\n\n        if self.xgboost__min_child_weight is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert (\n                2.718281828459045\n                &lt;= self.xgboost__min_child_weight\n                &lt;= 149.99999999999997\n            )\n\n        if self.xgboost__rate_drop is not None:\n            assert self.xgboost__booster in [\"dart\"]\n            assert 0.0 &lt;= self.xgboost__rate_drop &lt;= 1.0\n\n        if self.xgboost__skip_drop is not None:\n            assert self.xgboost__booster in [\"dart\"]\n            assert 0.0 &lt;= self.xgboost__skip_drop &lt;= 1.0\n\n    else:\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_xgboost/","title":"iaml_xgboost","text":""},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_xgboost/#mfpbench.yahpo.benchmarks.iaml.iaml_xgboost.IAMLxgboostConfig","title":"<code>class IAMLxgboostConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>IAMLConfig</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/iaml/iaml_xgboost/#mfpbench.yahpo.benchmarks.iaml.iaml_xgboost.IAMLxgboostConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/iaml/iaml_xgboost.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert self.booster in [\"gblinear\", \"gbtree\", \"dart\"]\n    assert 0.00010000000000000009 &lt;= self.alpha &lt;= 999.9999999999998\n    assert 0.00010000000000000009 &lt;= self._lambda &lt;= 999.9999999999998\n    assert 7 &lt;= self.nrounds &lt;= 2981\n    assert 0.1 &lt;= self.subsample &lt;= 1.0\n\n    if self.colsample_bylevel is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 0.01 &lt;= self.colsample_bylevel &lt;= 1.0\n\n    if self.colsample_bytree is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 0.01 &lt;= self.colsample_bytree &lt;= 1.0\n\n    if self.eta is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 0.00010000000000000009 &lt;= self.eta &lt;= 1.0\n\n    if self.gamma is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 0.00010000000000000009 &lt;= self.gamma &lt;= 6.999999999999999\n\n    if self.max_depth is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 1 &lt;= self.max_depth &lt;= 15\n\n    if self.min_child_weight is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 2.718281828459045 &lt;= self.min_child_weight &lt;= 149.99999999999997\n\n    if self.rate_drop is not None:\n        assert self.booster in [\"dart\"]\n        assert 0.0 &lt;= self.rate_drop &lt;= 1.0\n\n    if self.skip_drop is not None:\n        assert self.booster in [\"dart\"]\n        assert 0.0 &lt;= self.skip_drop &lt;= 1.0\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/","title":"rbv2","text":""},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Config","title":"<code>class RBV2Config</code>   <code>dataclass</code>","text":"<p>         Bases: <code>YAHPOConfig</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Config.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create from a dict or mapping object.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/rbv2/rbv2.py</code> <pre><code>@classmethod\ndef from_dict(cls: type[C], d: Mapping[str, Any]) -&gt; C:\n    \"\"\"Create from a dict or mapping object.\"\"\"\n    # We may have keys that are conditional and hence we need to flatten them\n    config = {k.replace(\".\", \"__\"): v for k, v in d.items()}\n    return cls(**config)\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Config.dict","title":"<code>def dict()</code>","text":"<p>Converts the config to a raw dictionary.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/rbv2/rbv2.py</code> <pre><code>def dict(self) -&gt; dict[str, Any]:\n    \"\"\"Converts the config to a raw dictionary.\"\"\"\n    d = asdict(self)\n    return {k.replace(\"__\", \".\"): v for k, v in d.items() if v is not None}\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Result","title":"<code>class RBV2Result</code>   <code>dataclass</code>","text":"<p>         Bases: <code>YAHPOResult[C, float]</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Result.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>The score of interest.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Result.error","title":"<code>error: float</code>   <code>prop</code>","text":"<p>The error of interest.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Result.test_score","title":"<code>test_score: float</code>   <code>prop</code>","text":"<p>The score on the test set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Result.test_error","title":"<code>test_error: float</code>   <code>prop</code>","text":"<p>The error on the test set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Result.val_score","title":"<code>val_score: float</code>   <code>prop</code>","text":"<p>The score on the validation set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Result.val_error","title":"<code>val_error: float</code>   <code>prop</code>","text":"<p>The error on the validation set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2/#mfpbench.yahpo.benchmarks.rbv2.rbv2.RBV2Result.cost","title":"<code>cost: float</code>   <code>prop</code>","text":"<p>The time taken in seconds to train the config.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_aknn/","title":"rbv2_aknn","text":""},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_aknn/#mfpbench.yahpo.benchmarks.rbv2.rbv2_aknn.RBV2aknnConfig","title":"<code>class RBV2aknnConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RBV2Config</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_aknn/#mfpbench.yahpo.benchmarks.rbv2.rbv2_aknn.RBV2aknnConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/rbv2/rbv2_aknn.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert self.num__impute__selected__cpo in [\n        \"impute.mean\",\n        \"impute.median\",\n        \"impute.hist\",\n    ]\n    assert 18 &lt;= self.M &lt;= 50\n    assert self.distance in [\"l2\", \"cosine\", \"ip\"]\n    assert 7 &lt;= self.ef &lt;= 403\n    assert 7 &lt;= self.ef_construction &lt;= 1097\n    assert 1 &lt;= self.k &lt;= 50\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_glmnet/","title":"rbv2_glmnet","text":""},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_glmnet/#mfpbench.yahpo.benchmarks.rbv2.rbv2_glmnet.RBV2glmnetConfig","title":"<code>class RBV2glmnetConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RBV2Config</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_glmnet/#mfpbench.yahpo.benchmarks.rbv2.rbv2_glmnet.RBV2glmnetConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/rbv2/rbv2_glmnet.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert self.num__impute__selected__cpo in [\n        \"impute.mean\",\n        \"impute.median\",\n        \"impute.hist\",\n    ]\n    assert 0.0 &lt;= self.alpha &lt;= 1.0\n    assert 0.0009118819655545162 &lt;= self.s &lt;= 1096.6331584284585\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_ranger/","title":"rbv2_ranger","text":""},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_ranger/#mfpbench.yahpo.benchmarks.rbv2.rbv2_ranger.RBV2rangerConfig","title":"<code>class RBV2rangerConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RBV2Config</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_ranger/#mfpbench.yahpo.benchmarks.rbv2.rbv2_ranger.RBV2rangerConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/rbv2/rbv2_ranger.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert self.num__impute__selected__cpo in [\n        \"impute.mean\",\n        \"impute.median\",\n        \"impute.hist\",\n    ]\n    assert 1 &lt;= self.min__node__size &lt;= 100\n    assert 0 &lt;= self.mtry__power &lt;= 1\n    assert 1 &lt;= self.num__trees &lt;= 2000\n    assert self.respect__unordered__factors in [\n        \"ignore\",\n        \"order\",\n        \"partition\",\n    ]\n    assert 0.1 &lt;= self.sample__fraction &lt;= 1.0\n    assert self.splitrule in [\"gini\", \"extratrees\"]\n\n    if self.num__random__splits is not None:\n        assert self.splitrule == \"extratrees\"\n        assert 1 &lt;= self.num__random__splits &lt;= 100\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_rpart/","title":"rbv2_rpart","text":""},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_rpart/#mfpbench.yahpo.benchmarks.rbv2.rbv2_rpart.RBV2rpartConfig","title":"<code>class RBV2rpartConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RBV2Config</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_rpart/#mfpbench.yahpo.benchmarks.rbv2.rbv2_rpart.RBV2rpartConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/rbv2/rbv2_rpart.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert self.num__impute__selected__cpo in [\n        \"impute.mean\",\n        \"impute.median\",\n        \"impute.hist\",\n    ]\n    assert 0.0009118819655545162 &lt;= self.cp &lt;= 1.0\n    assert 1 &lt;= self.maxdepth &lt;= 30\n    assert 1 &lt;= self.minbucket &lt;= 100\n    assert 1 &lt;= self.minsplit &lt;= 100\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_super/","title":"rbv2_super","text":""},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_super/#mfpbench.yahpo.benchmarks.rbv2.rbv2_super.RBV2SuperConfig","title":"<code>class RBV2SuperConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RBV2Config</code></p> <p>Config has conditionals and as such, we use None to indicate not set.</p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_super/#mfpbench.yahpo.benchmarks.rbv2.rbv2_super.RBV2SuperConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/rbv2/rbv2_super.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:  # noqa: C901, PLR0915, PLR0912\n    \"\"\"Validate this config.\"\"\"\n    assert self.learner_id in [\n        \"aknn\",\n        \"glmnet\",\n        \"ranger\",\n        \"rpart\",\n        \"svm\",\n        \"xgboost\",\n    ]\n\n    assert self.num__impute__selected__cpo in [\n        \"impute.mean\",\n        \"impute.median\",\n        \"impute.hist\",\n    ]\n\n    # We do some conditional checking here\n    learner = self.learner_id\n\n    # We filter out all attributes except for those that must always be contained\n    # or are the selected learner, ...\n    attrs = [\n        attr\n        for attr in dir(self)\n        if not attr.startswith(\"__\")\n        or not attr.startswith(learner)\n        or attr in [\"learner_id\", \"num__impute__selected__cpo\"]\n    ]\n\n    # ... the remaining must always have None set then\n    for attr in attrs:\n        assert attr is None\n\n    if learner == \"aknn\":\n        assert self.aknn__M is not None\n        assert self.aknn__ef is not None\n        assert self.aknn__ef_construction is not None\n        assert self.aknn__k is not None\n        assert 18 &lt;= self.aknn__M &lt;= 50\n        assert self.aknn__distance in [\"l2\", \"cosine\", \"ip\"]\n        assert 7 &lt;= self.aknn__ef &lt;= 403\n        assert 7 &lt;= self.aknn__ef_construction &lt;= 1097\n        assert 1 &lt;= self.aknn__k &lt;= 50\n\n    elif learner == \"glmnet\":\n        assert self.glmnet__alpha is not None\n        assert self.glmnet__s is not None\n        assert 0.0 &lt;= self.glmnet__alpha &lt;= 1.0\n        assert 0.0009118819655545162 &lt;= self.glmnet__s &lt;= 1096.6331584284585\n\n    elif learner == \"rpart\":\n        assert self.rpart__cp is not None\n        assert self.rpart__maxdepth is not None\n        assert self.rpart__minbucket is not None\n        assert self.rpart__minsplit is not None\n        assert 0.0009118819655545162 &lt;= self.rpart__cp &lt;= 1.0\n        assert 1 &lt;= self.rpart__maxdepth &lt;= 30\n        assert 1 &lt;= self.rpart__minbucket &lt;= 100\n        assert 1 &lt;= self.rpart__minsplit &lt;= 100\n\n    elif learner == \"ranger\":\n        assert self.ranger__min__node__size is not None\n        assert self.ranger__mtry__power is not None\n        assert self.ranger__num__trees is not None\n        assert self.ranger__respect__unordered__factors is not None\n        assert self.ranger__sample__fraction is not None\n        assert 1 &lt;= self.ranger__min__node__size &lt;= 100\n        assert 0 &lt;= self.ranger__mtry__power &lt;= 1\n        assert 1 &lt;= self.ranger__num__trees &lt;= 2000\n        assert self.ranger__respect__unordered__factors in [\n            \"ignore\",\n            \"order\",\n            \"partition\",\n        ]\n        assert 0.1 &lt;= self.ranger__sample__fraction &lt;= 1.0\n        assert self.ranger__splitrule in [\"gini\", \"extratrees\"]\n\n        if self.ranger__num__random__splits is not None:\n            assert self.ranger__splitrule == \"extratrees\"\n            assert 1 &lt;= self.ranger__num__random__splits &lt;= 100\n\n    elif learner == \"svm\":\n        assert self.svm__cost is not None\n        assert self.svm__gamma is not None\n        assert self.svm__kernel is not None\n        assert self.svm__tolerance is not None\n\n        assert 4.5399929762484854e-05 &lt;= self.svm__cost &lt;= 22026.465794806718\n        assert 4.5399929762484854e-05 &lt;= self.svm__gamma &lt;= 22026.465794806718\n        assert self.svm__kernel in [\"linear\", \"polynomial\", \"radial\"]\n        assert 4.5399929762484854e-05 &lt;= self.svm__tolerance &lt;= 2.0\n\n        if self.svm__degree is not None:\n            assert 2 &lt;= self.svm__degree &lt;= 5\n            assert self.svm__kernel == \"polynomial\"\n\n        if self.svm__gamma is not None:\n            assert 4.5399929762484854e-05 &lt;= self.svm__gamma &lt;= 22026.465794806718\n            assert self.svm__kernel == \"radial\"\n\n    elif learner == \"xgboost\":\n        assert self.xgboost__alpha is not None\n        assert self.xgboost__booster is not None\n        assert self.xgboost__lambda is not None\n        assert self.xgboost__nrounds is not None\n        assert self.xgboost__subsample is not None\n\n        assert self.xgboost__booster in [\"gblinear\", \"gbtree\", \"dart\"]\n        assert 0.0009118819655545162 &lt;= self.xgboost__alpha &lt;= 1096.6331584284585\n        assert 0.0009118819655545162 &lt;= self.xgboost__lambda &lt;= 1096.6331584284585\n        assert 7 &lt;= self.xgboost__nrounds &lt;= 2981\n        assert 0.1 &lt;= self.xgboost__subsample &lt;= 1.0\n\n        if self.xgboost__colsample_bylevel is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert 0.01 &lt;= self.xgboost__colsample_bylevel &lt;= 1.0\n\n        if self.xgboost__colsample_bytree is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert 0.01 &lt;= self.xgboost__colsample_bytree &lt;= 1.0\n\n        if self.xgboost__eta is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert 0.0009118819655545162 &lt;= self.xgboost__eta &lt;= 1.0\n\n        if self.xgboost__gamma is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert 4.5399929762484854e-05 &lt;= self.xgboost__gamma &lt;= 7.38905609893065\n\n        if self.xgboost__max_depth is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert 1 &lt;= self.xgboost__max_depth &lt;= 15\n\n        if self.xgboost__min_child_weight is not None:\n            assert self.xgboost__booster in [\"dart\", \"gbtree\"]\n            assert (\n                2.718281828459045\n                &lt;= self.xgboost__min_child_weight\n                &lt;= 148.4131591025766\n            )\n\n        if self.xgboost__rate_drop is not None:\n            assert self.xgboost__booster in [\"dart\"]\n            assert 0.0 &lt;= self.xgboost__rate_drop &lt;= 1.0\n\n        if self.xgboost__skip_drop is not None:\n            assert self.xgboost__booster in [\"dart\"]\n            assert 0.0 &lt;= self.xgboost__skip_drop &lt;= 1.0\n\n    else:\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_svm/","title":"rbv2_svm","text":""},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_svm/#mfpbench.yahpo.benchmarks.rbv2.rbv2_svm.RBV2svmConfig","title":"<code>class RBV2svmConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RBV2Config</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_svm/#mfpbench.yahpo.benchmarks.rbv2.rbv2_svm.RBV2svmConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/rbv2/rbv2_svm.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert self.num__impute__selected__cpo in [\n        \"impute.mean\",\n        \"impute.median\",\n        \"impute.hist\",\n    ]\n\n    assert 4.5399929762484854e-05 &lt;= self.cost &lt;= 22026.465794806718\n    assert 4.5399929762484854e-05 &lt;= self.gamma &lt;= 22026.465794806718\n    assert self.kernel in [\"linear\", \"polynomial\", \"radial\"]\n    assert 4.5399929762484854e-05 &lt;= self.tolerance &lt;= 2.0\n\n    if self.degree is not None:\n        assert 2 &lt;= self.degree &lt;= 5\n        assert self.kernel == \"polynomial\"\n\n    if self.gamma is not None:\n        assert 4.5399929762484854e-05 &lt;= self.gamma &lt;= 22026.465794806718\n        assert self.kernel == \"radial\"\n</code></pre>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_xgboost/","title":"rbv2_xgboost","text":""},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_xgboost/#mfpbench.yahpo.benchmarks.rbv2.rbv2_xgboost.RBV2xgboostConfig","title":"<code>class RBV2xgboostConfig</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RBV2Config</code></p>"},{"location":"api/mfpbench/yahpo/benchmarks/rbv2/rbv2_xgboost/#mfpbench.yahpo.benchmarks.rbv2.rbv2_xgboost.RBV2xgboostConfig.validate","title":"<code>def validate()</code>","text":"<p>Validate this config.</p> Source code in <code>src/mfpbench/yahpo/benchmarks/rbv2/rbv2_xgboost.py</code> <pre><code>@no_type_check\ndef validate(self) -&gt; None:\n    \"\"\"Validate this config.\"\"\"\n    assert self.booster in [\"gblinear\", \"gbtree\", \"dart\"]\n    assert 0.0009118819655545162 &lt;= self.alpha &lt;= 1096.6331584284585\n    assert 0.0009118819655545162 &lt;= self._lambda &lt;= 1096.6331584284585\n    assert 7 &lt;= self.nrounds &lt;= 2981\n    assert 0.1 &lt;= self.subsample &lt;= 1.0\n\n    if self.colsample_bylevel is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 0.01 &lt;= self.colsample_bylevel &lt;= 1.0\n\n    if self.colsample_bytree is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 0.01 &lt;= self.colsample_bytree &lt;= 1.0\n\n    if self.eta is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 0.0009118819655545162 &lt;= self.eta &lt;= 1.0\n\n    if self.gamma is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 4.5399929762484854e-05 &lt;= self.gamma &lt;= 7.38905609893065\n\n    if self.max_depth is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 1 &lt;= self.max_depth &lt;= 15\n\n    if self.min_child_weight is not None:\n        assert self.booster in [\"dart\", \"gbtree\"]\n        assert 2.718281828459045 &lt;= self.min_child_weight &lt;= 148.4131591025766\n\n    if self.rate_drop is not None:\n        assert self.booster in [\"dart\"]\n        assert 0.0 &lt;= self.rate_drop &lt;= 1.0\n\n    if self.skip_drop is not None:\n        assert self.booster in [\"dart\"]\n        assert 0.0 &lt;= self.skip_drop &lt;= 1.0\n\n    assert self.num__impute__selected__cpo in [\n        \"impute.mean\",\n        \"impute.median\",\n        \"impute.hist\",\n    ]\n</code></pre>"},{"location":"examples/integrating_objective_function_benchmark/","title":"Integrating your own functional benchmark","text":"<p>TODO</p>"},{"location":"examples/integrating_tabular_benchmark/","title":"Integrating a Tabular Benchmark","text":"<p>TODO</p>"},{"location":"examples/jahs-bench/","title":"JAHS Bench","text":"<p>TODO</p>"},{"location":"examples/lcbench-tabular/","title":"LCBench (Tabular)","text":"<p>TODO</p>"},{"location":"examples/mfh/","title":"MFHartmann","text":"<p>TODO</p>"},{"location":"examples/pd1/","title":"PD1","text":"<p>TODO</p>"},{"location":"examples/yahpo-gym/","title":"YAHPO-Gym","text":"<p>TODO</p>"}]}